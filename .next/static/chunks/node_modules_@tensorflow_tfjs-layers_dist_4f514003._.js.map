{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"file":"errors.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/errors.js/__/__/__/__/__/tfjs-layers/src/errors.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Explicit error types.\n *\n * See the following link for more information about why the code includes\n * calls to setPrototypeOf:\n *\n * https://github.com/Microsoft/TypeScript-wiki/blob/master/Breaking-Changes.md#extending-built-ins-like-error-array-and-map-may-no-longer-work\n */\n// tslint:enable\n\n/**\n * Equivalent of Python's AttributeError.\n */\nexport class AttributeError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, AttributeError.prototype);\n  }\n}\n\n/**\n * Equivalent of Python's RuntimeError.\n */\nexport class RuntimeError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, RuntimeError.prototype);\n  }\n}\n\n/**\n * Equivalent of Python's ValueError.\n */\nexport class ValueError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, ValueError.prototype);\n  }\n}\n\n/**\n * Equivalent of Python's NotImplementedError.\n */\nexport class NotImplementedError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, NotImplementedError.prototype);\n  }\n}\n\n/**\n * Equivalent of Python's AssertionError.\n */\nexport class AssertionError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, AssertionError.prototype);\n  }\n}\n\n/**\n * Equivalent of Python's IndexError.\n */\nexport class IndexError extends Error {\n  constructor(message?: string) {\n    super(message);\n    // Set the prototype explicitly.\n    Object.setPrototypeOf(this, IndexError.prototype);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH;;;;;;;GAOG,CACH,gBAAgB;AAEhB;;GAEG;;;;;;;;AACG,MAAO,cAAe,SAAQ,KAAK;IACvC,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,cAAc,CAAC,SAAS,CAAC,CAAC;IACxD,CAAC;CACF;AAKK,MAAO,YAAa,SAAQ,KAAK;IACrC,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,YAAY,CAAC,SAAS,CAAC,CAAC;IACtD,CAAC;CACF;AAKK,MAAO,UAAW,SAAQ,KAAK;IACnC,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,UAAU,CAAC,SAAS,CAAC,CAAC;IACpD,CAAC;CACF;AAKK,MAAO,mBAAoB,SAAQ,KAAK;IAC5C,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,mBAAmB,CAAC,SAAS,CAAC,CAAC;IAC7D,CAAC;CACF;AAKK,MAAO,cAAe,SAAQ,KAAK;IACvC,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,cAAc,CAAC,SAAS,CAAC,CAAC;IACxD,CAAC;CACF;AAKK,MAAO,UAAW,SAAQ,KAAK;IACnC,YAAY,OAAgB,CAAA;QAC1B,KAAK,CAAC,OAAO,CAAC,CAAC;QACf,gCAAgC;QAChC,MAAM,CAAC,cAAc,CAAC,IAAI,EAAE,UAAU,CAAC,SAAS,CAAC,CAAC;IACpD,CAAC;CACF","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 81, "column": 0}, "map": {"version":3,"file":"common.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/keras_format/common.js/__/__/__/__/__/__/tfjs-layers/src/keras_format/common.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n// TODO(huan): add layer-specific input shape types (see: https://github.com/tensorflow/tfjs-layers/pull/492)\n/** @docalias (null | number)[] */\nexport type Shape = Array<null | number>;\n\n// The tfjs-core version of DataType must stay synced with this.\nexport type DataType = 'float32'|'int32'|'bool'|'complex64'|'string';\n\n// TODO(soergel): Move the CamelCase versions back out of keras_format\n// e.g. to src/common.ts.  Maybe even duplicate *all* of these to be pedantic?\n/** @docinline */\nexport type DataFormat = 'channelsFirst'|'channelsLast';\nexport const VALID_DATA_FORMAT_VALUES = ['channelsFirst', 'channelsLast'];\n\nexport type InterpolationFormat = 'nearest'|'bilinear';\nexport const VALID_INTERPOLATION_FORMAT_VALUES = ['nearest', 'bilinear'];\n// These constants have a snake vs. camel distinction.\nexport type DataFormatSerialization = 'channels_first'|'channels_last';\n\n/** @docinline */\nexport type PaddingMode = 'valid'|'same'|'causal';\nexport const VALID_PADDING_MODE_VALUES = ['valid', 'same', 'causal'];\n\n/** @docinline */\nexport type PoolMode = 'max'|'avg';\nexport const VALID_POOL_MODE_VALUES = ['max', 'avg'];\n\n/** @docinline */\nexport type BidirectionalMergeMode = 'sum'|'mul'|'concat'|'ave';\nexport const VALID_BIDIRECTIONAL_MERGE_MODES = ['sum', 'mul', 'concat', 'ave'];\n\n/** @docinline */\nexport type SampleWeightMode = 'temporal';\nexport const VALID_SAMPLE_WEIGHT_MODES = ['temporal'];\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;;;;AAaI,MAAM,wBAAwB,GAAG;IAAC,eAAe;IAAE,cAAc;CAAC,CAAC;AAGnE,MAAM,iCAAiC,GAAG;IAAC,SAAS;IAAE,UAAU;CAAC,CAAC;AAMlE,MAAM,yBAAyB,GAAG;IAAC,OAAO;IAAE,MAAM;IAAE,QAAQ;CAAC,CAAC;AAI9D,MAAM,sBAAsB,GAAG;IAAC,KAAK;IAAE,KAAK;CAAC,CAAC;AAI9C,MAAM,+BAA+B,GAAG;IAAC,KAAK;IAAE,KAAK;IAAE,QAAQ;IAAE,KAAK;CAAC,CAAC;AAIxE,MAAM,yBAAyB,GAAG;IAAC,UAAU;CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 129, "column": 0}, "map": {"version":3,"file":"initializer_config.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/keras_format/initializer_config.js/__/__/__/__/__/__/tfjs-layers/src/keras_format/initializer_config.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {BaseSerialization} from './types';\n\n// TODO(soergel): Move the CamelCase versions back out of keras_format\n// e.g. to src/common.ts.  Maybe even duplicate *all* of these to be pedantic?\n/** @docinline */\nexport type FanMode = 'fanIn'|'fanOut'|'fanAvg';\nexport const VALID_FAN_MODE_VALUES = ['fanIn', 'fanOut', 'fanAvg'];\n\n// These constants have a snake vs. camel distinction.\nexport type FanModeSerialization = 'fan_in'|'fan_out'|'fan_avg';\n\n/** @docinline */\nexport type Distribution = 'normal'|'uniform'|'truncatedNormal';\nexport const VALID_DISTRIBUTION_VALUES =\n    ['normal', 'uniform', 'truncatedNormal'];\n// These constants have a snake vs. camel distinction.\nexport type DistributionSerialization = 'normal'|'uniform'|'truncated_normal';\n\nexport type ZerosSerialization = BaseSerialization<'Zeros', {}>;\n\nexport type OnesSerialization = BaseSerialization<'Ones', {}>;\n\nexport type ConstantConfig = {\n  value: number;\n};\n\nexport type ConstantSerialization =\n    BaseSerialization<'Constant', ConstantConfig>;\n\nexport type RandomNormalConfig = {\n  mean?: number;\n  stddev?: number;\n  seed?: number;\n};\n\nexport type RandomNormalSerialization =\n    BaseSerialization<'RandomNormal', RandomNormalConfig>;\n\nexport type RandomUniformConfig = {\n  minval?: number;\n  maxval?: number;\n  seed?: number;\n};\n\nexport type RandomUniformSerialization =\n    BaseSerialization<'RandomUniform', RandomUniformConfig>;\n\nexport type TruncatedNormalConfig = {\n  mean?: number;\n  stddev?: number;\n  seed?: number;\n};\n\nexport type TruncatedNormalSerialization =\n    BaseSerialization<'TruncatedNormal', TruncatedNormalConfig>;\n\nexport type VarianceScalingConfig = {\n  scale?: number;\n\n  mode?: FanModeSerialization;\n  distribution?: DistributionSerialization;\n  seed?: number;\n};\n\nexport type VarianceScalingSerialization =\n    BaseSerialization<'VarianceScaling', VarianceScalingConfig>;\n\nexport type OrthogonalConfig = {\n  seed?: number;\n  gain?: number;\n};\n\nexport type OrthogonalSerialization =\n    BaseSerialization<'Orthogonal', OrthogonalConfig>;\n\nexport type IdentityConfig = {\n  gain?: number;\n};\n\nexport type IdentitySerialization =\n    BaseSerialization<'Identity', IdentityConfig>;\n\n// Update initializerClassNames below in concert with this.\nexport type InitializerSerialization = ZerosSerialization|OnesSerialization|\n    ConstantSerialization|RandomUniformSerialization|RandomNormalSerialization|\n    TruncatedNormalSerialization|IdentitySerialization|\n    VarianceScalingSerialization|OrthogonalSerialization;\n\nexport type InitializerClassName = InitializerSerialization['class_name'];\n\n// We can't easily extract a string[] from the string union type, but we can\n// recapitulate the list, enforcing at compile time that the values are valid\n// and that we have the right number of them.\n\n/**\n * A string array of valid Initializer class names.\n *\n * This is guaranteed to match the `InitializerClassName` union type.\n */\nexport const initializerClassNames: InitializerClassName[] = [\n  'Zeros', 'Ones', 'Constant', 'RandomNormal', 'RandomUniform',\n  'TruncatedNormal', 'VarianceScaling', 'Orthogonal', 'Identity'\n];\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;AAQI,MAAM,qBAAqB,GAAG;IAAC,OAAO;IAAE,QAAQ;IAAE,QAAQ;CAAC,CAAC;AAO5D,MAAM,yBAAyB,GAClC;IAAC,QAAQ;IAAE,SAAS;IAAE,iBAAiB;CAAC,CAAC;AAqFtC,MAAM,qBAAqB,GAA2B;IAC3D,OAAO;IAAE,MAAM;IAAE,UAAU;IAAE,cAAc;IAAE,eAAe;IAC5D,iBAAiB;IAAE,iBAAiB;IAAE,YAAY;IAAE,UAAU;CAC/D,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 169, "column": 0}, "map": {"version":3,"file":"common.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/common.js/__/__/__/__/__/tfjs-layers/src/common.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Common functions for TensorFlow.js Layers.\n */\nimport {VALID_DATA_FORMAT_VALUES, VALID_INTERPOLATION_FORMAT_VALUES, VALID_PADDING_MODE_VALUES, VALID_POOL_MODE_VALUES} from './keras_format/common';\nimport {checkStringTypeUnionValue} from './utils/generic_utils';\n\n// A map from the requested scoped name of a Tensor to the number of Tensors\n// wanting that name so far.  This allows enforcing name uniqueness by appending\n// an incrementing index, e.g. scope/name, scope/name_1, scope/name_2, etc.\nconst nameMap: Map<string, number> = new Map<string, number>();\n\nexport function checkDataFormat(value?: string): void {\n  checkStringTypeUnionValue(VALID_DATA_FORMAT_VALUES, 'DataFormat', value);\n}\n\nexport function checkInterpolationFormat(value?: string): void {\n  checkStringTypeUnionValue(\n      VALID_INTERPOLATION_FORMAT_VALUES, 'InterpolationFormat', value);\n}\n\nexport function checkPaddingMode(value?: string): void {\n  checkStringTypeUnionValue(VALID_PADDING_MODE_VALUES, 'PaddingMode', value);\n}\n\nexport function checkPoolMode(value?: string): void {\n  checkStringTypeUnionValue(VALID_POOL_MODE_VALUES, 'PoolMode', value);\n}\n\nconst _nameScopeStack: string[] = [];\nconst _nameScopeDivider = '/';\n\n/**\n * Enter namescope, which can be nested.\n */\nexport function nameScope<T>(name: string, fn: () => T): T {\n  _nameScopeStack.push(name);\n  try {\n    const val: T = fn();\n    _nameScopeStack.pop();\n    return val;\n  } catch (e) {\n    _nameScopeStack.pop();\n    throw e;\n  }\n}\n\n/**\n * Get the current namescope as a flat, concatenated string.\n */\nfunction currentNameScopePrefix(): string {\n  if (_nameScopeStack.length === 0) {\n    return '';\n  } else {\n    return _nameScopeStack.join(_nameScopeDivider) + _nameScopeDivider;\n  }\n}\n\n/**\n * Get the name a Tensor (or Variable) would have if not uniqueified.\n * @param tensorName\n * @return Scoped name string.\n */\nexport function getScopedTensorName(tensorName: string): string {\n  if (!isValidTensorName(tensorName)) {\n    throw new Error('Not a valid tensor name: \\'' + tensorName + '\\'');\n  }\n  return currentNameScopePrefix() + tensorName;\n}\n\n/**\n * Get unique names for Tensors and Variables.\n * @param scopedName The fully-qualified name of the Tensor, i.e. as produced by\n *  `getScopedTensorName()`.\n * @return A unique version of the given fully scoped name.\n *   If this is the first time that the scoped name is seen in this session,\n *   then the given `scopedName` is returned unaltered.  If the same name is\n *   seen again (producing a collision), an incrementing suffix is added to the\n *   end of the name, so it takes the form 'scope/name_1', 'scope/name_2', etc.\n */\nexport function getUniqueTensorName(scopedName: string): string {\n  if (!isValidTensorName(scopedName)) {\n    throw new Error('Not a valid tensor name: \\'' + scopedName + '\\'');\n  }\n  if (!nameMap.has(scopedName)) {\n    nameMap.set(scopedName, 0);\n  }\n  const index = nameMap.get(scopedName);\n  nameMap.set(scopedName, nameMap.get(scopedName) + 1);\n\n  if (index > 0) {\n    const result = `${scopedName}_${index}`;\n    // Mark the composed name as used in case someone wants\n    // to call getUniqueTensorName(\"name_1\").\n    nameMap.set(result, 1);\n    return result;\n  } else {\n    return scopedName;\n  }\n}\n\nconst tensorNameRegex = new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\\._\\/]*$/);\n\n/**\n * Determine whether a string is a valid tensor name.\n * @param name\n * @returns A Boolean indicating whether `name` is a valid tensor name.\n */\nexport function isValidTensorName(name: string): boolean {\n  return !!name.match(tensorNameRegex);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;;;;;;;;AACH,OAAO,EAAC,wBAAwB,EAAE,iCAAiC,EAAE,yBAAyB,EAAE,sBAAsB,EAAC,MAAM,uBAAuB,CAAC;AACrJ,OAAO,EAAC,yBAAyB,EAAC,MAAM,uBAAuB,CAAC;;;AAEhE,4EAA4E;AAC5E,gFAAgF;AAChF,2EAA2E;AAC3E,MAAM,OAAO,GAAwB,IAAI,GAAG,EAAkB,CAAC;AAEzD,SAAU,eAAe,CAAC,KAAc;IAC5C,oNAAA,AAAyB,sLAAC,2BAAwB,EAAE,YAAY,EAAE,KAAK,CAAC,CAAC;AAC3E,CAAC;AAEK,SAAU,wBAAwB,CAAC,KAAc;4LACrD,4BAAA,AAAyB,sLACrB,oCAAiC,EAAE,qBAAqB,EAAE,KAAK,CAAC,CAAC;AACvE,CAAC;AAEK,SAAU,gBAAgB,CAAC,KAAc;4LAC7C,4BAAA,AAAyB,sLAAC,4BAAyB,EAAE,aAAa,EAAE,KAAK,CAAC,CAAC;AAC7E,CAAC;AAEK,SAAU,aAAa,CAAC,KAAc;4LAC1C,4BAAA,AAAyB,qLAAC,0BAAsB,EAAE,UAAU,EAAE,KAAK,CAAC,CAAC;AACvE,CAAC;AAED,MAAM,eAAe,GAAa,EAAE,CAAC;AACrC,MAAM,iBAAiB,GAAG,GAAG,CAAC;AAKxB,SAAU,SAAS,CAAI,IAAY,EAAE,EAAW;IACpD,eAAe,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IAC3B,IAAI;QACF,MAAM,GAAG,GAAM,EAAE,EAAE,CAAC;QACpB,eAAe,CAAC,GAAG,EAAE,CAAC;QACtB,OAAO,GAAG,CAAC;KACZ,CAAC,OAAO,CAAC,EAAE;QACV,eAAe,CAAC,GAAG,EAAE,CAAC;QACtB,MAAM,CAAC,CAAC;KACT;AACH,CAAC;AAED;;GAEG,CACH,SAAS,sBAAsB;IAC7B,IAAI,eAAe,CAAC,MAAM,KAAK,CAAC,EAAE;QAChC,OAAO,EAAE,CAAC;KACX,MAAM;QACL,OAAO,eAAe,CAAC,IAAI,CAAC,iBAAiB,CAAC,GAAG,iBAAiB,CAAC;KACpE;AACH,CAAC;AAOK,SAAU,mBAAmB,CAAC,UAAkB;IACpD,IAAI,CAAC,iBAAiB,CAAC,UAAU,CAAC,EAAE;QAClC,MAAM,IAAI,KAAK,CAAC,6BAA6B,GAAG,UAAU,GAAG,IAAI,CAAC,CAAC;KACpE;IACD,OAAO,sBAAsB,EAAE,GAAG,UAAU,CAAC;AAC/C,CAAC;AAYK,SAAU,mBAAmB,CAAC,UAAkB;IACpD,IAAI,CAAC,iBAAiB,CAAC,UAAU,CAAC,EAAE;QAClC,MAAM,IAAI,KAAK,CAAC,6BAA6B,GAAG,UAAU,GAAG,IAAI,CAAC,CAAC;KACpE;IACD,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,UAAU,CAAC,EAAE;QAC5B,OAAO,CAAC,GAAG,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC;KAC5B;IACD,MAAM,KAAK,GAAG,OAAO,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC;IACtC,OAAO,CAAC,GAAG,CAAC,UAAU,EAAE,OAAO,CAAC,GAAG,CAAC,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC;IAErD,IAAI,KAAK,GAAG,CAAC,EAAE;QACb,MAAM,MAAM,GAAG,GAAG,UAAU,CAAA,CAAA,EAAI,KAAK,EAAE,CAAC;QACxC,uDAAuD;QACvD,yCAAyC;QACzC,OAAO,CAAC,GAAG,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;QACvB,OAAO,MAAM,CAAC;KACf,MAAM;QACL,OAAO,UAAU,CAAC;KACnB;AACH,CAAC;AAED,MAAM,eAAe,GAAG,IAAI,MAAM,CAAC,iCAAiC,CAAC,CAAC;AAOhE,SAAU,iBAAiB,CAAC,IAAY;IAC5C,OAAO,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC,CAAC;AACvC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 266, "column": 0}, "map": {"version":3,"file":"initializers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/initializers.js/__/__/__/__/__/tfjs-layers/src/initializers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {DataType, eye, linalg, mul, ones, randomUniform, scalar, serialization, Tensor, tidy, truncatedNormal, util, zeros} from '@tensorflow/tfjs-core';\n\nimport * as K from './backend/tfjs_backend';\nimport {checkDataFormat} from './common';\nimport {NotImplementedError, ValueError} from './errors';\nimport {DataFormat, Shape} from './keras_format/common';\nimport {Distribution, FanMode, VALID_DISTRIBUTION_VALUES, VALID_FAN_MODE_VALUES} from './keras_format/initializer_config';\nimport {checkStringTypeUnionValue, deserializeKerasObject, serializeKerasObject} from './utils/generic_utils';\nimport {arrayProd} from './utils/math_utils';\n\nexport function checkFanMode(value?: string): void {\n  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, 'FanMode', value);\n}\n\nexport function checkDistribution(value?: string): void {\n  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, 'Distribution', value);\n}\n\n/**\n * Initializer base class.\n *\n * @doc {\n *   heading: 'Initializers', subheading: 'Classes', namespace: 'initializers'}\n */\nexport abstract class Initializer extends serialization.Serializable {\n  public fromConfigUsesCustomObjects(): boolean {\n    return false;\n  }\n  /**\n   * Generate an initial value.\n   * @param shape\n   * @param dtype\n   * @return The init value.\n   */\n  abstract apply(shape: Shape, dtype?: DataType): Tensor;\n\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\nexport class Zeros extends Initializer {\n  /** @nocollapse */\n  static className = 'Zeros';\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return zeros(shape, dtype);\n  }\n}\nserialization.registerClass(Zeros);\n\nexport class Ones extends Initializer {\n  /** @nocollapse */\n  static className = 'Ones';\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return ones(shape, dtype);\n  }\n}\nserialization.registerClass(Ones);\n\nexport interface ConstantArgs {\n  /** The value for each element in the variable. */\n  value: number;\n}\n\nexport class Constant extends Initializer {\n  /** @nocollapse */\n  static className = 'Constant';\n  private value: number;\n  constructor(args: ConstantArgs) {\n    super();\n    if (typeof args !== 'object') {\n      throw new ValueError(\n          `Expected argument of type ConstantConfig but got ${args}`);\n    }\n    if (args.value === undefined) {\n      throw new ValueError(`config must have value set but got ${args}`);\n    }\n    this.value = args.value;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return tidy(() => mul(scalar(this.value), ones(shape, dtype)));\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {\n      value: this.value,\n    };\n  }\n}\nserialization.registerClass(Constant);\n\nexport interface RandomUniformArgs {\n  /** Lower bound of the range of random values to generate. */\n  minval?: number;\n  /** Upper bound of the range of random values to generate. */\n  maxval?: number;\n  /** Used to seed the random generator. */\n  seed?: number;\n}\n\nexport class RandomUniform extends Initializer {\n  /** @nocollapse */\n  static className = 'RandomUniform';\n  readonly DEFAULT_MINVAL = -0.05;\n  readonly DEFAULT_MAXVAL = 0.05;\n  private minval: number;\n  private maxval: number;\n  private seed: number;\n\n  constructor(args: RandomUniformArgs) {\n    super();\n    this.minval = args.minval || this.DEFAULT_MINVAL;\n    this.maxval = args.maxval || this.DEFAULT_MAXVAL;\n    this.seed = args.seed;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return randomUniform(shape, this.minval, this.maxval, dtype, this.seed);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {minval: this.minval, maxval: this.maxval, seed: this.seed};\n  }\n}\nserialization.registerClass(RandomUniform);\n\nexport interface RandomNormalArgs {\n  /** Mean of the random values to generate. */\n  mean?: number;\n  /** Standard deviation of the random values to generate. */\n  stddev?: number;\n  /** Used to seed the random generator. */\n  seed?: number;\n}\n\nexport class RandomNormal extends Initializer {\n  /** @nocollapse */\n  static className = 'RandomNormal';\n  readonly DEFAULT_MEAN = 0.;\n  readonly DEFAULT_STDDEV = 0.05;\n  private mean: number;\n  private stddev: number;\n  private seed: number;\n\n  constructor(args: RandomNormalArgs) {\n    super();\n    this.mean = args.mean || this.DEFAULT_MEAN;\n    this.stddev = args.stddev || this.DEFAULT_STDDEV;\n    this.seed = args.seed;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    dtype = dtype || 'float32';\n    if (dtype !== 'float32' && dtype !== 'int32') {\n      throw new NotImplementedError(\n          `randomNormal does not support dType ${dtype}.`);\n    }\n\n    return K.randomNormal(shape, this.mean, this.stddev, dtype, this.seed);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {mean: this.mean, stddev: this.stddev, seed: this.seed};\n  }\n}\nserialization.registerClass(RandomNormal);\n\nexport interface TruncatedNormalArgs {\n  /** Mean of the random values to generate. */\n  mean?: number;\n  /** Standard deviation of the random values to generate. */\n  stddev?: number;\n  /** Used to seed the random generator. */\n  seed?: number;\n}\n\nexport class TruncatedNormal extends Initializer {\n  /** @nocollapse */\n  static className = 'TruncatedNormal';\n\n  readonly DEFAULT_MEAN = 0.;\n  readonly DEFAULT_STDDEV = 0.05;\n  private mean: number;\n  private stddev: number;\n  private seed: number;\n\n  constructor(args: TruncatedNormalArgs) {\n    super();\n    this.mean = args.mean || this.DEFAULT_MEAN;\n    this.stddev = args.stddev || this.DEFAULT_STDDEV;\n    this.seed = args.seed;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    dtype = dtype || 'float32';\n    if (dtype !== 'float32' && dtype !== 'int32') {\n      throw new NotImplementedError(\n          `truncatedNormal does not support dType ${dtype}.`);\n    }\n    return truncatedNormal(shape, this.mean, this.stddev, dtype, this.seed);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {mean: this.mean, stddev: this.stddev, seed: this.seed};\n  }\n}\nserialization.registerClass(TruncatedNormal);\n\nexport interface IdentityArgs {\n  /**\n   * Multiplicative factor to apply to the identity matrix.\n   */\n  gain?: number;\n}\n\nexport class Identity extends Initializer {\n  /** @nocollapse */\n  static className = 'Identity';\n  private gain: number;\n  constructor(args: IdentityArgs) {\n    super();\n    this.gain = args.gain != null ? args.gain : 1.0;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return tidy(() => {\n      if (shape.length !== 2 || shape[0] !== shape[1]) {\n        throw new ValueError(\n            'Identity matrix initializer can only be used for' +\n            ' 2D square matrices.');\n      } else {\n        return mul(this.gain, eye(shape[0]));\n      }\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {gain: this.gain};\n  }\n}\nserialization.registerClass(Identity);\n\n/**\n * Computes the number of input and output units for a weight shape.\n * @param shape Shape of weight.\n * @param dataFormat data format to use for convolution kernels.\n *   Note that all kernels in Keras are standardized on the\n *   CHANNEL_LAST ordering (even when inputs are set to CHANNEL_FIRST).\n * @return An length-2 array: fanIn, fanOut.\n */\nfunction computeFans(\n    shape: Shape, dataFormat: DataFormat = 'channelsLast'): number[] {\n  let fanIn: number;\n  let fanOut: number;\n  checkDataFormat(dataFormat);\n  if (shape.length === 2) {\n    fanIn = shape[0];\n    fanOut = shape[1];\n  } else if ([3, 4, 5].indexOf(shape.length) !== -1) {\n    if (dataFormat === 'channelsFirst') {\n      const receptiveFieldSize = arrayProd(shape, 2);\n      fanIn = shape[1] * receptiveFieldSize;\n      fanOut = shape[0] * receptiveFieldSize;\n    } else if (dataFormat === 'channelsLast') {\n      const receptiveFieldSize = arrayProd(shape, 0, shape.length - 2);\n      fanIn = shape[shape.length - 2] * receptiveFieldSize;\n      fanOut = shape[shape.length - 1] * receptiveFieldSize;\n    }\n  } else {\n    const shapeProd = arrayProd(shape);\n    fanIn = Math.sqrt(shapeProd);\n    fanOut = Math.sqrt(shapeProd);\n  }\n\n  return [fanIn, fanOut];\n}\n\nexport interface VarianceScalingArgs {\n  /** Scaling factor (positive float). */\n  scale?: number;\n\n  /** Fanning mode for inputs and outputs. */\n  mode?: FanMode;\n\n  /** Probabilistic distribution of the values. */\n  distribution?: Distribution;\n\n  /** Random number generator seed. */\n  seed?: number;\n}\n\nexport class VarianceScaling extends Initializer {\n  /** @nocollapse */\n  static className = 'VarianceScaling';\n  private scale: number;\n  private mode: FanMode;\n  private distribution: Distribution;\n  private seed: number;\n\n  /**\n   * Constructor of VarianceScaling.\n   * @throws ValueError for invalid value in scale.\n   */\n  constructor(args: VarianceScalingArgs) {\n    super();\n    if (args.scale < 0.0) {\n      throw new ValueError(\n          `scale must be a positive float. Got: ${args.scale}`);\n    }\n    this.scale = args.scale == null ? 1.0 : args.scale;\n    this.mode = args.mode == null ? 'fanIn' : args.mode;\n    checkFanMode(this.mode);\n    this.distribution =\n        args.distribution == null ? 'normal' : args.distribution;\n    checkDistribution(this.distribution);\n    this.seed = args.seed;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    const fans = computeFans(shape);\n    const fanIn = fans[0];\n    const fanOut = fans[1];\n    let scale = this.scale;\n    if (this.mode === 'fanIn') {\n      scale /= Math.max(1, fanIn);\n    } else if (this.mode === 'fanOut') {\n      scale /= Math.max(1, fanOut);\n    } else {\n      scale /= Math.max(1, (fanIn + fanOut) / 2);\n    }\n\n    if (this.distribution === 'normal') {\n      const stddev = Math.sqrt(scale);\n      dtype = dtype || 'float32';\n      if (dtype !== 'float32' && dtype !== 'int32') {\n        throw new NotImplementedError(\n            `${this.getClassName()} does not support dType ${dtype}.`);\n      }\n      return truncatedNormal(shape, 0, stddev, dtype, this.seed);\n    } else {\n      const limit = Math.sqrt(3 * scale);\n      return randomUniform(shape, -limit, limit, dtype, this.seed);\n    }\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {\n      scale: this.scale,\n      mode: this.mode,\n      distribution: this.distribution,\n      seed: this.seed\n    };\n  }\n}\nserialization.registerClass(VarianceScaling);\n\nexport interface SeedOnlyInitializerArgs {\n  /** Random number generator seed. */\n  seed?: number;\n}\n\nexport class GlorotUniform extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'GlorotUniform';\n\n  /**\n   * Constructor of GlorotUniform\n   * @param scale\n   * @param mode\n   * @param distribution\n   * @param seed\n   */\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 1.0,\n      mode: 'fanAvg',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, GlorotUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(GlorotUniform);\n\nexport class GlorotNormal extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'GlorotNormal';\n\n  /**\n   * Constructor of GlorotNormal.\n   * @param scale\n   * @param mode\n   * @param distribution\n   * @param seed\n   */\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 1.0,\n      mode: 'fanAvg',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, GlorotNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(GlorotNormal);\n\nexport class HeNormal extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'HeNormal';\n\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 2.0,\n      mode: 'fanIn',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, HeNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(HeNormal);\n\nexport class HeUniform extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'HeUniform';\n\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 2.0,\n      mode: 'fanIn',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, HeUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(HeUniform);\n\nexport class LeCunNormal extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'LeCunNormal';\n\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 1.0,\n      mode: 'fanIn',\n      distribution: 'normal',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, LeCunNormal is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(LeCunNormal);\n\nexport class LeCunUniform extends VarianceScaling {\n  /** @nocollapse */\n  static override className = 'LeCunUniform';\n\n  constructor(args?: SeedOnlyInitializerArgs) {\n    super({\n      scale: 1.0,\n      mode: 'fanIn',\n      distribution: 'uniform',\n      seed: args == null ? null : args.seed\n    });\n  }\n\n  override getClassName(): string {\n    // In Python Keras, LeCunUniform is not a class, but a helper method\n    // that creates a VarianceScaling object. Use 'VarianceScaling' as\n    // class name to be compatible with that.\n    return VarianceScaling.className;\n  }\n}\nserialization.registerClass(LeCunUniform);\n\nexport interface OrthogonalArgs extends SeedOnlyInitializerArgs {\n  /**\n   * Multiplicative factor to apply to the orthogonal matrix. Defaults to 1.\n   */\n  gain?: number;\n}\n\nexport class Orthogonal extends Initializer {\n  /** @nocollapse */\n  static className = 'Orthogonal';\n  readonly DEFAULT_GAIN = 1;\n  readonly ELEMENTS_WARN_SLOW = 2000;\n  protected readonly gain: number;\n  protected readonly seed: number;\n\n  constructor(args?: OrthogonalArgs) {\n    super();\n    this.gain = args.gain == null ? this.DEFAULT_GAIN : args.gain;\n    this.seed = args.seed;\n  }\n\n  apply(shape: Shape, dtype?: DataType): Tensor {\n    return tidy(() => {\n      if (shape.length < 2) {\n        throw new NotImplementedError('Shape must be at least 2D.');\n      }\n      if (dtype !== 'int32' && dtype !== 'float32' && dtype !== undefined) {\n        throw new TypeError(`Unsupported data type ${dtype}.`);\n      }\n      dtype = dtype as 'int32' | 'float32' | undefined;\n\n      // flatten the input shape with the last dimension remaining its\n      // original shape so it works for conv2d\n      const numRows = util.sizeFromShape(shape.slice(0, -1));\n      const numCols = shape[shape.length - 1];\n      const numElements = numRows * numCols;\n      if (numElements > this.ELEMENTS_WARN_SLOW) {\n        console.warn(\n            `Orthogonal initializer is being called on a matrix with more ` +\n            `than ${this.ELEMENTS_WARN_SLOW} (${numElements}) elements: ` +\n            `Slowness may result.`);\n      }\n      const flatShape =\n          [Math.max(numCols, numRows), Math.min(numCols, numRows)];\n\n      // Generate a random matrix\n      const randNormalMat = K.randomNormal(flatShape, 0, 1, dtype, this.seed);\n\n      // Compute QR factorization\n      const qr = linalg.qr(randNormalMat, false);\n      let qMat = qr[0];\n      const rMat = qr[1];\n\n      // Make Q uniform\n      const diag = rMat.flatten().stridedSlice(\n          [0], [Math.min(numCols, numRows) * Math.min(numCols, numRows)],\n          [Math.min(numCols, numRows) + 1]);\n      qMat = mul(qMat, diag.sign());\n      if (numRows < numCols) {\n        qMat = qMat.transpose();\n      }\n\n      return mul(scalar(this.gain), qMat.reshape(shape));\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {\n      gain: this.gain,\n      seed: this.seed,\n    };\n  }\n}\nserialization.registerClass(Orthogonal);\n\n/** @docinline */\nexport type InitializerIdentifier =\n    'constant'|'glorotNormal'|'glorotUniform'|'heNormal'|'heUniform'|'identity'|\n    'leCunNormal'|'leCunUniform'|'ones'|'orthogonal'|'randomNormal'|\n    'randomUniform'|'truncatedNormal'|'varianceScaling'|'zeros'|string;\n\n// Maps the JavaScript-like identifier keys to the corresponding registry\n// symbols.\nexport const INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP:\n    {[identifier in InitializerIdentifier]: string} = {\n      'constant': 'Constant',\n      'glorotNormal': 'GlorotNormal',\n      'glorotUniform': 'GlorotUniform',\n      'heNormal': 'HeNormal',\n      'heUniform': 'HeUniform',\n      'identity': 'Identity',\n      'leCunNormal': 'LeCunNormal',\n      'leCunUniform': 'LeCunUniform',\n      'ones': 'Ones',\n      'orthogonal': 'Orthogonal',\n      'randomNormal': 'RandomNormal',\n      'randomUniform': 'RandomUniform',\n      'truncatedNormal': 'TruncatedNormal',\n      'varianceScaling': 'VarianceScaling',\n      'zeros': 'Zeros'\n    };\n\nfunction deserializeInitializer(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Initializer {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'initializer');\n}\n\nexport function serializeInitializer(initializer: Initializer):\n    serialization.ConfigDictValue {\n  return serializeKerasObject(initializer);\n}\n\nexport function getInitializer(identifier: InitializerIdentifier|Initializer|\n                               serialization.ConfigDict): Initializer {\n  if (typeof identifier === 'string') {\n    const className = identifier in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ?\n        INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] :\n        identifier;\n    /* We have four 'helper' classes for common initializers that\n    all get serialized as 'VarianceScaling' and shouldn't go through\n    the deserializeInitializer pathway. */\n    if (className === 'GlorotNormal') {\n      return new GlorotNormal();\n    } else if (className === 'GlorotUniform') {\n      return new GlorotUniform();\n    } else if (className === 'HeNormal') {\n      return new HeNormal();\n    } else if (className === 'HeUniform') {\n      return new HeUniform();\n    } else if (className === 'LeCunNormal') {\n      return new LeCunNormal();\n    } else if (className === 'LeCunUniform') {\n      return new LeCunUniform();\n    } else {\n      const config: serialization.ConfigDict = {};\n      config['className'] = className;\n      config['config'] = {};\n      return deserializeInitializer(config);\n    }\n  } else if (identifier instanceof Initializer) {\n    return identifier;\n  } else {\n    return deserializeInitializer(identifier);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;;;;;;;;;;;;;AAEH,OAAO,EAAW,GAAG,EAAE,MAAM,EAAE,GAAG,EAAE,IAAI,EAAE,aAAa,EAAE,MAAM,EAAE,aAAa,EAAU,IAAI,EAAE,eAAe,EAAE,IAAI,EAAE,KAAK,EAAC,MAAM,uBAAuB,CAAC;;;;;;;;AAEzJ,OAAO,KAAK,CAAC,MAAM,wBAAwB,CAAC;AAC5C,OAAO,EAAC,eAAe,EAAC,MAAM,UAAU,CAAC;AACzC,OAAO,EAAC,mBAAmB,EAAE,UAAU,EAAC,MAAM,UAAU,CAAC;AAEzD,OAAO,EAAwB,yBAAyB,EAAE,qBAAqB,EAAC,MAAM,mCAAmC,CAAC;AAC1H,OAAO,EAAC,yBAAyB,EAAE,sBAAsB,EAAE,oBAAoB,EAAC,MAAM,uBAAuB,CAAC;AAC9G,OAAO,EAAC,SAAS,EAAC,MAAM,oBAAoB,CAAC;;;;;;;;AAEvC,SAAU,YAAY,CAAC,KAAc;4LACzC,4BAAA,AAAyB,kMAAC,wBAAqB,EAAE,SAAS,EAAE,KAAK,CAAC,CAAC;AACrE,CAAC;AAEK,SAAU,iBAAiB,CAAC,KAAc;KAC9C,mNAAA,AAAyB,kMAAC,4BAAyB,EAAE,cAAc,EAAE,KAAK,CAAC,CAAC;AAC9E,CAAC;AAQK,MAAgB,WAAY,2NAAQ,gBAAa,CAAC,YAAY;IAC3D,2BAA2B,GAAA;QAChC,OAAO,KAAK,CAAC;IACf,CAAC;IASD,SAAS,GAAA;QACP,OAAO,CAAA,CAAE,CAAC;IACZ,CAAC;CACF;AAED,MAAa,KAAM,SAAQ,WAAW;IAIpC,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,mLAAO,QAAA,AAAK,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;IAC7B,CAAC;;AALD,gBAAA,EAAkB,CACX,MAAA,SAAS,GAAG,OAAO,CAAC;;kNAM7B,gBAAa,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC;AAEnC,MAAa,IAAK,SAAQ,WAAW;IAInC,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,OAAO,kLAAA,AAAI,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;IAC5B,CAAC;;AALD,gBAAA,EAAkB,CACX,KAAA,SAAS,GAAG,MAAM,CAAC;;kNAM5B,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAOlC,MAAa,QAAS,SAAQ,WAAW;IAIvC,YAAY,IAAkB,CAAA;QAC5B,KAAK,EAAE,CAAC;QACR,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;YAC5B,MAAM,wKAAI,aAAU,CAChB,CAAA,iDAAA,EAAoD,IAAI,EAAE,CAAC,CAAC;SACjE;QACD,IAAI,IAAI,CAAC,KAAK,KAAK,SAAS,EAAE;YAC5B,MAAM,wKAAI,aAAU,CAAC,CAAA,mCAAA,EAAsC,IAAI,EAAE,CAAC,CAAC;SACpE;QACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;IAC1B,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,6KAAO,QAAA,AAAI,EAAC,GAAG,EAAE,yKAAC,MAAA,AAAG,+KAAC,SAAA,AAAM,EAAC,IAAI,CAAC,KAAK,CAAC,EAAE,kLAAA,AAAI,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC;IACjE,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,KAAK,EAAE,IAAI,CAAC,KAAK;SAClB,CAAC;IACJ,CAAC;;AAvBD,gBAAA,EAAkB,CACX,SAAA,SAAS,GAAG,UAAU,CAAC;;kNAwBhC,gBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAWtC,MAAa,aAAc,SAAQ,WAAW;IAS5C,YAAY,IAAuB,CAAA;QACjC,KAAK,EAAE,CAAC;QAPD,IAAA,CAAA,cAAc,GAAG,CAAC,IAAI,CAAC;QACvB,IAAA,CAAA,cAAc,GAAG,IAAI,CAAC;QAO7B,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,cAAc,CAAC;QACjD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,cAAc,CAAC;QACjD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;IACxB,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,4LAAO,gBAAA,AAAa,EAAC,KAAK,EAAE,IAAI,CAAC,MAAM,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IAC1E,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,MAAM,EAAE,IAAI,CAAC,MAAM;YAAE,MAAM,EAAE,IAAI,CAAC,MAAM;YAAE,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IACrE,CAAC;;AArBD,gBAAA,EAAkB,CACX,cAAA,SAAS,GAAG,eAAe,AAAlB,CAAmB;;AAsBrC,kOAAa,CAAC,aAAa,CAAC,aAAa,CAAC,CAAC;AAW3C,MAAa,YAAa,SAAQ,WAAW;IAS3C,YAAY,IAAsB,CAAA;QAChC,KAAK,EAAE,CAAC;QAPD,IAAA,CAAA,YAAY,GAAG,EAAE,CAAC;QAClB,IAAA,CAAA,cAAc,GAAG,IAAI,CAAC;QAO7B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,YAAY,CAAC;QAC3C,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,cAAc,CAAC;QACjD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;IACxB,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,KAAK,GAAG,KAAK,IAAI,SAAS,CAAC;QAC3B,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,OAAO,EAAE;YAC5C,MAAM,wKAAI,sBAAmB,CACzB,CAAA,oCAAA,EAAuC,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;SACtD;QAED,OAAO,CAAC,CAAC,sMAAA,AAAY,EAAC,KAAK,EAAE,IAAI,CAAC,IAAI,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IACzE,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;YAAE,MAAM,EAAE,IAAI,CAAC,MAAM;YAAE,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IACjE,CAAC;;AA3BD,gBAAA,EAAkB,CACX,aAAA,SAAS,GAAG,cAAc,AAAjB,CAAkB;;kNA4BpC,gBAAa,CAAC,aAAa,CAAC,YAAY,CAAC,CAAC;AAW1C,MAAa,eAAgB,SAAQ,WAAW;IAU9C,YAAY,IAAyB,CAAA;QACnC,KAAK,EAAE,CAAC;QAPD,IAAA,CAAA,YAAY,GAAG,EAAE,CAAC;QAClB,IAAA,CAAA,cAAc,GAAG,IAAI,CAAC;QAO7B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,YAAY,CAAC;QAC3C,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,IAAI,CAAC,cAAc,CAAC;QACjD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;IACxB,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,KAAK,GAAG,KAAK,IAAI,SAAS,CAAC;QAC3B,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,OAAO,EAAE;YAC5C,MAAM,wKAAI,sBAAmB,CACzB,CAAA,uCAAA,EAA0C,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;SACzD;QACD,8LAAO,kBAAA,AAAe,EAAC,KAAK,EAAE,IAAI,CAAC,IAAI,EAAE,IAAI,CAAC,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IAC1E,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;YAAE,MAAM,EAAE,IAAI,CAAC,MAAM;YAAE,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IACjE,CAAC;;AA3BD,gBAAA,EAAkB,CACX,gBAAA,SAAS,GAAG,iBAAiB,AAApB,CAAqB;;iNA4BvC,iBAAa,CAAC,aAAa,CAAC,eAAe,CAAC,CAAC;AAS7C,MAAa,QAAS,SAAQ,WAAW;IAIvC,YAAY,IAAkB,CAAA;QAC5B,KAAK,EAAE,CAAC;QACR,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC;IAClD,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;YACf,IAAI,KAAK,CAAC,MAAM,KAAK,CAAC,IAAI,KAAK,CAAC,CAAC,CAAC,KAAK,KAAK,CAAC,CAAC,CAAC,EAAE;gBAC/C,MAAM,wKAAI,aAAU,CAChB,kDAAkD,GAClD,sBAAsB,CAAC,CAAC;aAC7B,MAAM;gBACL,iLAAO,MAAA,AAAG,EAAC,IAAI,CAAC,IAAI,4KAAE,MAAA,AAAG,EAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;aACtC;QACH,CAAC,CAAC,CAAC;IACL,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IAC3B,CAAC;;AAtBD,gBAAA,EAAkB,CACX,SAAA,SAAS,GAAG,UAAU,CAAC;;iNAuBhC,iBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;;;;;;GAOG,CACH,SAAS,WAAW,CAChB,KAAY,EAAE,aAAyB,cAAc;IACvD,IAAI,KAAa,CAAC;IAClB,IAAI,MAAc,CAAC;4KACnB,kBAAA,AAAe,EAAC,UAAU,CAAC,CAAC;IAC5B,IAAI,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;QACtB,KAAK,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC;QACjB,MAAM,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC;KACnB,MAAM,IAAI;QAAC,CAAC;QAAE,CAAC;QAAE,CAAC;KAAC,CAAC,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;QACjD,IAAI,UAAU,KAAK,eAAe,EAAE;YAClC,MAAM,kBAAkB,wLAAG,YAAA,AAAS,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC;YAC/C,KAAK,GAAG,KAAK,CAAC,CAAC,CAAC,GAAG,kBAAkB,CAAC;YACtC,MAAM,GAAG,KAAK,CAAC,CAAC,CAAC,GAAG,kBAAkB,CAAC;SACxC,MAAM,IAAI,UAAU,KAAK,cAAc,EAAE;YACxC,MAAM,kBAAkB,wLAAG,YAAA,AAAS,EAAC,KAAK,EAAE,CAAC,EAAE,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACjE,KAAK,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,GAAG,kBAAkB,CAAC;YACrD,MAAM,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,GAAG,kBAAkB,CAAC;SACvD;KACF,MAAM;QACL,MAAM,SAAS,OAAG,6LAAA,AAAS,EAAC,KAAK,CAAC,CAAC;QACnC,KAAK,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;QAC7B,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC;KAC/B;IAED,OAAO;QAAC,KAAK;QAAE,MAAM;KAAC,CAAC;AACzB,CAAC;AAgBD,MAAa,eAAgB,SAAQ,WAAW;IAQ9C;;;OAGG,CACH,YAAY,IAAyB,CAAA;QACnC,KAAK,EAAE,CAAC;QACR,IAAI,IAAI,CAAC,KAAK,GAAG,GAAG,EAAE;YACpB,MAAM,wKAAI,aAAU,CAChB,CAAA,qCAAA,EAAwC,IAAI,CAAC,KAAK,EAAE,CAAC,CAAC;SAC3D;QACD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;QACnD,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QACpD,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACxB,IAAI,CAAC,YAAY,GACb,IAAI,CAAC,YAAY,IAAI,IAAI,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC;QAC7D,iBAAiB,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;QACrC,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;IACxB,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,MAAM,IAAI,GAAG,WAAW,CAAC,KAAK,CAAC,CAAC;QAChC,MAAM,KAAK,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACtB,MAAM,MAAM,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACvB,IAAI,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC;QACvB,IAAI,IAAI,CAAC,IAAI,KAAK,OAAO,EAAE;YACzB,KAAK,IAAI,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC;SAC7B,MAAM,IAAI,IAAI,CAAC,IAAI,KAAK,QAAQ,EAAE;YACjC,KAAK,IAAI,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC;SAC9B,MAAM;YACL,KAAK,IAAI,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,KAAK,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;SAC5C;QAED,IAAI,IAAI,CAAC,YAAY,KAAK,QAAQ,EAAE;YAClC,MAAM,MAAM,GAAG,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAChC,KAAK,GAAG,KAAK,IAAI,SAAS,CAAC;YAC3B,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,OAAO,EAAE;gBAC5C,MAAM,wKAAI,sBAAmB,CACzB,GAAG,IAAI,CAAC,YAAY,EAAE,CAAA,wBAAA,EAA2B,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;aAChE;YACD,8LAAO,kBAAA,AAAe,EAAC,KAAK,EAAE,CAAC,EAAE,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;SAC5D,MAAM;YACL,MAAM,KAAK,GAAG,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC;YACnC,4LAAO,gBAAA,AAAa,EAAC,KAAK,EAAE,CAAC,KAAK,EAAE,KAAK,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;SAC9D;IACH,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,YAAY,EAAE,IAAI,CAAC,YAAY;YAC/B,IAAI,EAAE,IAAI,CAAC,IAAI;SAChB,CAAC;IACJ,CAAC;;AA5DD,gBAAA,EAAkB,CACX,gBAAA,SAAS,GAAG,iBAAiB,CAAC;;kNA6DvC,gBAAa,CAAC,aAAa,CAAC,eAAe,CAAC,CAAC;AAO7C,MAAa,aAAc,SAAQ,eAAe;IAIhD;;;;;;OAMG,CACH,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,QAAQ;YACd,YAAY,EAAE,SAAS;YACvB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,qEAAqE;QACrE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAxBD,gBAAA,EAAkB,CACF,cAAA,SAAS,GAAG,eAAe,CAAC;;kNAyB9C,gBAAa,CAAC,aAAa,CAAC,aAAa,CAAC,CAAC;AAE3C,MAAa,YAAa,SAAQ,eAAe;IAI/C;;;;;;OAMG,CACH,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,QAAQ;YACd,YAAY,EAAE,QAAQ;YACtB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,oEAAoE;QACpE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAxBD,gBAAA,EAAkB,CACF,aAAA,SAAS,GAAG,cAAc,CAAC;;kNAyB7C,gBAAa,CAAC,aAAa,CAAC,YAAY,CAAC,CAAC;AAE1C,MAAa,QAAS,SAAQ,eAAe;IAI3C,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,OAAO;YACb,YAAY,EAAE,QAAQ;YACtB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,gEAAgE;QAChE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAjBD,gBAAA,EAAkB,CACF,SAAA,SAAS,GAAG,UAAU,CAAC;;kNAkBzC,gBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC,MAAa,SAAU,SAAQ,eAAe;IAI5C,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,OAAO;YACb,YAAY,EAAE,SAAS;YACvB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,iEAAiE;QACjE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAjBD,gBAAA,EAAkB,CACF,UAAA,SAAS,GAAG,WAAW,CAAC;;kNAkB1C,gBAAa,CAAC,aAAa,CAAC,SAAS,CAAC,CAAC;AAEvC,MAAa,WAAY,SAAQ,eAAe;IAI9C,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,OAAO;YACb,YAAY,EAAE,QAAQ;YACtB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,mEAAmE;QACnE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAjBD,gBAAA,EAAkB,CACF,YAAA,SAAS,GAAG,aAAa,CAAC;;kNAkB5C,gBAAa,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;AAEzC,MAAa,YAAa,SAAQ,eAAe;IAI/C,YAAY,IAA8B,CAAA;QACxC,KAAK,CAAC;YACJ,KAAK,EAAE,GAAG;YACV,IAAI,EAAE,OAAO;YACb,YAAY,EAAE,SAAS;YACvB,IAAI,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI;SACtC,CAAC,CAAC;IACL,CAAC;IAEQ,YAAY,GAAA;QACnB,oEAAoE;QACpE,kEAAkE;QAClE,yCAAyC;QACzC,OAAO,eAAe,CAAC,SAAS,CAAC;IACnC,CAAC;;AAjBD,gBAAA,EAAkB,CACF,aAAA,SAAS,GAAG,cAAc,CAAC;;kNAkB7C,gBAAa,CAAC,aAAa,CAAC,YAAY,CAAC,CAAC;AAS1C,MAAa,UAAW,SAAQ,WAAW;IAQzC,YAAY,IAAqB,CAAA;QAC/B,KAAK,EAAE,CAAC;QAND,IAAA,CAAA,YAAY,GAAG,CAAC,CAAC;QACjB,IAAA,CAAA,kBAAkB,GAAG,IAAI,CAAC;QAMjC,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC;QAC9D,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC;IACxB,CAAC;IAED,KAAK,CAAC,KAAY,EAAE,KAAgB,EAAA;QAClC,WAAO,0KAAA,AAAI,EAAC,GAAG,EAAE;YACf,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE;gBACpB,MAAM,wKAAI,sBAAmB,CAAC,4BAA4B,CAAC,CAAC;aAC7D;YACD,IAAI,KAAK,KAAK,OAAO,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,SAAS,EAAE;gBACnE,MAAM,IAAI,SAAS,CAAC,CAAA,sBAAA,EAAyB,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;aACxD;YACD,KAAK,GAAG,KAAwC,CAAC;YAEjD,gEAAgE;YAChE,wCAAwC;YACxC,MAAM,OAAO,mMAAG,OAAI,CAAC,aAAa,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;YACvD,MAAM,OAAO,GAAG,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;YACxC,MAAM,WAAW,GAAG,OAAO,GAAG,OAAO,CAAC;YACtC,IAAI,WAAW,GAAG,IAAI,CAAC,kBAAkB,EAAE;gBACzC,OAAO,CAAC,IAAI,CACR,CAAA,6DAAA,CAA+D,GAC/D,CAAA,KAAA,EAAQ,IAAI,CAAC,kBAAkB,CAAA,EAAA,EAAK,WAAW,CAAA,YAAA,CAAc,GAC7D,CAAA,oBAAA,CAAsB,CAAC,CAAC;aAC7B;YACD,MAAM,SAAS,GACX;gBAAC,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,OAAO,CAAC;gBAAE,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,OAAO,CAAC;aAAC,CAAC;YAE7D,2BAA2B;YAC3B,MAAM,aAAa,4LAAG,CAAC,CAAC,aAAA,AAAY,EAAC,SAAS,EAAE,CAAC,EAAE,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;YAExE,2BAA2B;YAC3B,MAAM,EAAE,yLAAG,SAAM,CAAC,EAAE,CAAC,aAAa,EAAE,KAAK,CAAC,CAAC;YAC3C,IAAI,IAAI,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC;YACjB,MAAM,IAAI,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC;YAEnB,iBAAiB;YACjB,MAAM,IAAI,GAAG,IAAI,CAAC,OAAO,EAAE,CAAC,YAAY,CACpC;gBAAC,CAAC;aAAC,EAAE;gBAAC,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,OAAO,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,OAAO,CAAC;aAAC,EAC9D;gBAAC,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,OAAO,CAAC,GAAG,CAAC;aAAC,CAAC,CAAC;YACtC,IAAI,6KAAG,MAAA,AAAG,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAE,CAAC,CAAC;YAC9B,IAAI,OAAO,GAAG,OAAO,EAAE;gBACrB,IAAI,GAAG,IAAI,CAAC,SAAS,EAAE,CAAC;aACzB;YAED,OAAO,gLAAA,AAAG,+KAAC,SAAA,AAAM,EAAC,IAAI,CAAC,IAAI,CAAC,EAAE,IAAI,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC;QACrD,CAAC,CAAC,CAAC;IACL,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,IAAI,EAAE,IAAI,CAAC,IAAI;SAChB,CAAC;IACJ,CAAC;;AA/DD,gBAAA,EAAkB,CACX,WAAA,SAAS,GAAG,YAAY,AAAf,CAAgB;;iNAgElC,iBAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;AAUjC,MAAM,0CAA0C,GACD;IAChD,UAAU,EAAE,UAAU;IACtB,cAAc,EAAE,cAAc;IAC9B,eAAe,EAAE,eAAe;IAChC,UAAU,EAAE,UAAU;IACtB,WAAW,EAAE,WAAW;IACxB,UAAU,EAAE,UAAU;IACtB,aAAa,EAAE,aAAa;IAC5B,cAAc,EAAE,cAAc;IAC9B,MAAM,EAAE,MAAM;IACd,YAAY,EAAE,YAAY;IAC1B,cAAc,EAAE,cAAc;IAC9B,eAAe,EAAE,eAAe;IAChC,iBAAiB,EAAE,iBAAiB;IACpC,iBAAiB,EAAE,iBAAiB;IACpC,OAAO,EAAE,OAAO;CACjB,CAAC;AAEN,SAAS,sBAAsB,CAC3B,MAAgC,EAChC,gBAA0C,CAAA,CAAE;IAC9C,+LAAO,yBAAA,AAAsB,EACzB,MAAM,oNAAE,gBAAa,CAAC,gBAAgB,CAAC,MAAM,EAAE,CAAC,YAAY,EAC5D,aAAa,EAAE,aAAa,CAAC,CAAC;AACpC,CAAC;AAEK,SAAU,oBAAoB,CAAC,WAAwB;IAE3D,+LAAO,uBAAA,AAAoB,EAAC,WAAW,CAAC,CAAC;AAC3C,CAAC;AAEK,SAAU,cAAc,CAAC,UACwB;IACrD,IAAI,OAAO,UAAU,KAAK,QAAQ,EAAE;QAClC,MAAM,SAAS,GAAG,UAAU,IAAI,0CAA0C,CAAC,CAAC,CACxE,0CAA0C,CAAC,UAAU,CAAC,CAAC,CAAC,CACxD,UAAU,CAAC;QACf;;8CAEsC,CACtC,IAAI,SAAS,KAAK,cAAc,EAAE;YAChC,OAAO,IAAI,YAAY,EAAE,CAAC;SAC3B,MAAM,IAAI,SAAS,KAAK,eAAe,EAAE;YACxC,OAAO,IAAI,aAAa,EAAE,CAAC;SAC5B,MAAM,IAAI,SAAS,KAAK,UAAU,EAAE;YACnC,OAAO,IAAI,QAAQ,EAAE,CAAC;SACvB,MAAM,IAAI,SAAS,KAAK,WAAW,EAAE;YACpC,OAAO,IAAI,SAAS,EAAE,CAAC;SACxB,MAAM,IAAI,SAAS,KAAK,aAAa,EAAE;YACtC,OAAO,IAAI,WAAW,EAAE,CAAC;SAC1B,MAAM,IAAI,SAAS,KAAK,cAAc,EAAE;YACvC,OAAO,IAAI,YAAY,EAAE,CAAC;SAC3B,MAAM;YACL,MAAM,MAAM,GAA6B,CAAA,CAAE,CAAC;YAC5C,MAAM,CAAC,WAAW,CAAC,GAAG,SAAS,CAAC;YAChC,MAAM,CAAC,QAAQ,CAAC,GAAG,CAAA,CAAE,CAAC;YACtB,OAAO,sBAAsB,CAAC,MAAM,CAAC,CAAC;SACvC;KACF,MAAM,IAAI,UAAU,YAAY,WAAW,EAAE;QAC5C,OAAO,UAAU,CAAC;KACnB,MAAM;QACL,OAAO,sBAAsB,CAAC,UAAU,CAAC,CAAC;KAC3C;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 809, "column": 0}, "map": {"version":3,"file":"variables.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/variables.js/__/__/__/__/__/tfjs-layers/src/variables.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {DataType, Tensor, variableGrads} from '@tensorflow/tfjs-core';\n\nimport {getNextUniqueTensorId} from './backend/state';\nimport {getScopedTensorName, getUniqueTensorName} from './common';\nimport {Constraint} from './constraints';\nimport {NotImplementedError} from './errors';\nimport {Shape} from './keras_format/common';\nimport {HasShape} from './types';\n\nconst DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n\n/**\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\n * dtype and shape, but its value is mutable.  The value is itself represented\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\n * the `write()` method.\n */\nexport class LayerVariable {\n  readonly dtype: DataType;\n  readonly shape: Shape;\n\n  readonly id: number;\n  // The fully scoped name of this Variable, including a unique suffix if needed\n  readonly name: string;\n  // The originally requested fully scoped name of this Variable, not including\n  // any unique suffix.  This may be needed when restoring weights because this\n  // original name is used as a key.\n  readonly originalName: string;\n  private trainable_: boolean;\n\n  protected readonly val: tfc.Variable;\n  readonly constraint: Constraint;\n  /**\n   * Construct Variable from a `tf.Tensor`.\n   *\n   * If not explicitly named, the Variable will be given a name with the\n   * prefix 'Variable'. Variable names are unique. In the case of name\n   * collision, suffixies '_<num>' will be added to the name.\n   *\n   * @param val Initial value of the Variable.\n   * @param name Name of the variable. If `null` or `undefined` is provided, it\n   *   will default a name with the prefix 'Variable'.\n   * @param constraint Optional, projection function to be applied to the\n   * variable after optimize updates\n   * @throws ValueError if `name` is `null` or `undefined`.\n   */\n  constructor(\n      val: Tensor, dtype: DataType = 'float32',\n      name = DEFAULT_VARIABLE_NAME_PREFIX, trainable = true,\n      constraint: Constraint = null) {\n    this.dtype = dtype == null ? 'float32' : dtype;\n    this.shape = val.shape;\n    this.id = getNextUniqueTensorId();\n\n    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n\n    this.trainable_ = trainable;\n    this.constraint = constraint;\n\n    this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n  }\n\n  /**\n   * Get a snapshot of the Variable's value.\n   *\n   * The returned value is a snapshot of the Variable's value at the time of\n   * the invocation. Future mutations in the value of the tensor will only\n   * be reflected by future calls to this method.\n   */\n  read(): Tensor {\n    this.assertNotDisposed();\n    return this.val;\n  }\n\n  /**\n   * Update the value of the Variable.\n   *\n   * @param newVal: The new value to update to. Must be consistent with the\n   *   dtype and shape of the Variable.\n   * @return This Variable.\n   */\n  write(newVal: Tensor) {\n    // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n    this.assertNotDisposed();\n    checkShapesMatch(this.val, newVal);\n    // Skip updating if this is the exact same tensor.\n    if (this.val.id !== newVal.id) {\n      this.val.assign(newVal);\n      if (this.constraint != null) {\n        this.val.assign(this.constraint.apply(this.val));\n      }\n    }\n    return this;\n  }\n\n  /**\n   * Dispose this LayersVariable instance from memory.\n   */\n  dispose(): void {\n    this.assertNotDisposed();\n    this.val.dispose();\n  }\n\n  protected assertNotDisposed(): void {\n    if (this.val.isDisposed) {\n      throw new Error(`LayersVariable ${this.name} is already disposed.`);\n    }\n  }\n\n  get trainable(): boolean {\n    return this.trainable_;\n  }\n\n  set trainable(trainable: boolean) {\n    this.trainable_ = trainable;\n    this.val.trainable = trainable;\n  }\n}\n\nfunction checkShapesMatch(x: HasShape, y: HasShape): void {\n  if (x.shape.toString() !== y.shape.toString()) {\n    throw new Error(\n        'Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' +\n        JSON.stringify(y.shape));\n  }\n}\n\n/**\n * Create a Variable.\n * @param x The initial value of the `Variable`.\n * @param dtype optional, the type of the variable.\n * @param name optional, the name of the variable, default provided by\n * Variable.\n * @param constraint optional, a constraint to be applied after every update.\n * @return The newly instantiated `Variable`.\n */\nexport function variable(\n    x: Tensor, dtype?: DataType, name?: string,\n    constraint?: Constraint): LayerVariable {\n  return new LayerVariable(x, dtype, name, true, constraint);\n}\n\n/**\n * Instantiates an all-zeros Variable and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-zero Variable.\n */\nexport function zerosVariable(\n    shape: Shape, dtype?: DataType, name?: string): LayerVariable {\n  // TODO(cais): Implement logic for dtype.\n  return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n\n/**\n * Instantiates an all-zeros tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function zerosLike(\n    x: Tensor, dtype?: DataType, name?: string): LayerVariable {\n  return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n\n/**\n * Instantiates an all-ones tensor and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-ones Variable.\n */\nexport function onesVariable(\n    shape: Shape, dtype?: DataType, name?: string): LayerVariable {\n  // TODO(cais): Implement logic for dtype.\n  const allocated = tfc.ones(shape);\n  return new LayerVariable(allocated, dtype, name);\n}\n\n/**\n * Instantiates an all-ones tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function onesLike(\n    x: Tensor, dtype?: DataType, name?: string): LayerVariable {\n  const allocated = tfc.onesLike(x);\n  return new LayerVariable(allocated, dtype, name);\n}\n\n/**\n * Instantiate an identity matrix and returns it, as a Variable\n *\n * @param size Number of rows/columns.\n * @param dtype Data type of returned Variable.\n * @param name Name of returned Variable.\n * @return A Variable, an identity matrix.\n */\nexport function eyeVariable(\n    size: number, dtype?: DataType, name?: string): LayerVariable {\n  return new LayerVariable(tfc.eye(size), dtype, name);\n}\n\n/**\n * Get a Variable with uniform distribution of values.\n * @param shape Shape of the tensor.\n * @param minval Lower bound of the uniform distribution.\n * @param maxval Upper bound of the uniform distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The uniform-random Variable.\n */\nexport function randomUniformVariable(\n    shape: Shape, minval: number, maxval: number, dtype?: DataType,\n    seed?: number, name = 'randomUniform'): LayerVariable {\n  return new LayerVariable(\n      tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n\n/**\n * Get a Variable with truncated-normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function truncatedNormalVariable(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: DataType, seed?: number,\n    name = 'truncatedNormal'): LayerVariable {\n  // TODO(cais): Implement logic for dtype and seed once they are supported\n  // by deeplearn.js.\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\n        `randomNormal does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(\n      tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Get a Variable with normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function randomNormalVariable(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: DataType, seed?: number,\n    name = 'randomNormal'): LayerVariable {\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\n        `randomNormalVariable does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(\n      tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n\n/**\n * Update the value of a Variable.\n * @param x The Variable to be updated.\n * @param xNew The new value to update to.\n * @return The Variable updated.\n */\nexport function update(x: LayerVariable, xNew: Tensor): LayerVariable {\n  return x.write(xNew);\n}\n\n/**\n * Update the value of a Variable by adding an increment.\n * @param x The Variable to be updated.\n * @param increment The incrment to add to `x`.\n * @return The Variable updated.\n */\nexport function updateAdd(x: LayerVariable, increment: Tensor): LayerVariable {\n  return x.write(tfc.add(x.read(), increment));\n}\n\n/**\n * Update the value of a Variable by subtracting a decrement.\n * @param x The Variable to be updated.\n * @param decrement The decrement to subtract from `x`.\n * @return The Variable updated.\n */\nexport function updateSub(x: LayerVariable, decrement: Tensor): LayerVariable {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n\n/**\n * Get the values of an array of Variables.\n *\n * @param tensors An `Array` of `Variable`s to get the values of.\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\n */\nexport function batchGetValue(xs: LayerVariable[]): Tensor[] {\n  return xs.map(x => x.read());\n}\n\n/**\n * Update the value of multiple Variables at once.\n *\n * @param variablesAndValues An `Array`, each element is of type\n *   [Variable, Tensor]. The first item is the\n *   `Variable` of which the value is to be updated. The second item\n *   carries the new value.\n */\nexport function batchSetValue(\n    variablesAndValues: Array<[LayerVariable, Tensor]>): void {\n  variablesAndValues.forEach(variableAndValue => {\n    const variable: LayerVariable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n\n/**\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\n * @param lossFn A function which returns a Scalar to be used as the function\n *   value (i.e., numerator) for differentiation.\n * @param variables List of variables to be used as the independent variables\n *   (i.e., denominator) for differentiation.\n * @returns An Array of gradients tensors.\n */\nexport function gradients(\n    lossFn: () => tfc.Scalar, variables: LayerVariable[]): Tensor[] {\n  // TODO(cais): The return type signature can be simplified if deeplearn makes\n  //   the corresponding type public.\n  const variableList =\n      variables.map(variable => variable.read() as tfc.Variable);\n  const valudAndGrads = variableGrads(lossFn, variableList);\n  return variables.map(variable => valudAndGrads.grads[variable.name]);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;;;;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;;;;;;AAC7C,OAAO,EAAmB,aAAa,EAAC,MAAM,uBAAuB,CAAC;AAEtE,OAAO,EAAC,qBAAqB,EAAC,MAAM,iBAAiB,CAAC;AACtD,OAAO,EAAC,mBAAmB,EAAE,mBAAmB,EAAC,MAAM,UAAU,CAAC;AAElE,OAAO,EAAC,mBAAmB,EAAC,MAAM,UAAU,CAAC;;;;;;AAI7C,MAAM,4BAA4B,GAAG,UAAU,CAAC;AAQ1C,MAAO,aAAa;IAexB;;;;;;;;;;;;;OAaG,CACH,YACI,GAAW,EAAE,QAAkB,SAAS,EACxC,IAAI,GAAG,4BAA4B,EAAE,SAAS,GAAG,IAAI,EACrD,aAAyB,IAAI,CAAA;QAC/B,IAAI,CAAC,KAAK,GAAG,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC;QAC/C,IAAI,CAAC,KAAK,GAAG,GAAG,CAAC,KAAK,CAAC;QACvB,IAAI,CAAC,EAAE,qLAAG,wBAAA,AAAqB,EAAE,CAAC;QAElC,IAAI,GAAG,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,4BAA4B,CAAC,CAAC,CAAC,IAAI,CAAC;QAC1D,IAAI,CAAC,YAAY,OAAG,0LAAA,AAAmB,EAAC,IAAI,CAAC,CAAC;QAC9C,IAAI,CAAC,IAAI,2KAAG,sBAAA,AAAmB,EAAC,IAAI,CAAC,YAAY,CAAC,CAAC;QAEnD,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;QAC5B,IAAI,CAAC,UAAU,GAAG,UAAU,CAAC;QAE7B,IAAI,CAAC,GAAG,kLAAG,GAAG,CAAC,OAAA,AAAQ,EAAC,GAAG,EAAE,IAAI,CAAC,UAAU,EAAE,IAAI,CAAC,IAAI,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;IACvE,CAAC;IAED;;;;;;OAMG,CACH,IAAI,GAAA;QACF,IAAI,CAAC,iBAAiB,EAAE,CAAC;QACzB,OAAO,IAAI,CAAC,GAAG,CAAC;IAClB,CAAC;IAED;;;;;;OAMG,CACH,KAAK,CAAC,MAAc,EAAA;QAClB,yEAAyE;QACzE,IAAI,CAAC,iBAAiB,EAAE,CAAC;QACzB,gBAAgB,CAAC,IAAI,CAAC,GAAG,EAAE,MAAM,CAAC,CAAC;QACnC,kDAAkD;QAClD,IAAI,IAAI,CAAC,GAAG,CAAC,EAAE,KAAK,MAAM,CAAC,EAAE,EAAE;YAC7B,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACxB,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;gBAC3B,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;aAClD;SACF;QACD,OAAO,IAAI,CAAC;IACd,CAAC;IAED;;OAEG,CACH,OAAO,GAAA;QACL,IAAI,CAAC,iBAAiB,EAAE,CAAC;QACzB,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC;IACrB,CAAC;IAES,iBAAiB,GAAA;QACzB,IAAI,IAAI,CAAC,GAAG,CAAC,UAAU,EAAE;YACvB,MAAM,IAAI,KAAK,CAAC,CAAA,eAAA,EAAkB,IAAI,CAAC,IAAI,CAAA,qBAAA,CAAuB,CAAC,CAAC;SACrE;IACH,CAAC;IAED,IAAI,SAAS,GAAA;QACX,OAAO,IAAI,CAAC,UAAU,CAAC;IACzB,CAAC;IAED,IAAI,SAAS,CAAC,SAAkB,EAAA;QAC9B,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC;QAC5B,IAAI,CAAC,GAAG,CAAC,SAAS,GAAG,SAAS,CAAC;IACjC,CAAC;CACF;AAED,SAAS,gBAAgB,CAAC,CAAW,EAAE,CAAW;IAChD,IAAI,CAAC,CAAC,KAAK,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC,KAAK,CAAC,QAAQ,EAAE,EAAE;QAC7C,MAAM,IAAI,KAAK,CACX,kBAAkB,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC,GAAG,OAAO,GACtD,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;KAC9B;AACH,CAAC;AAWK,SAAU,QAAQ,CACpB,CAAS,EAAE,KAAgB,EAAE,IAAa,EAC1C,UAAuB;IACzB,OAAO,IAAI,aAAa,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,IAAI,EAAE,UAAU,CAAC,CAAC;AAC7D,CAAC;AAUK,SAAU,aAAa,CACzB,KAAY,EAAE,KAAgB,EAAE,IAAa;IAC/C,yCAAyC;IACzC,OAAO,IAAI,aAAa,6KAAC,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AAC1D,CAAC;AAUK,SAAU,SAAS,CACrB,CAAS,EAAE,KAAgB,EAAE,IAAa;IAC5C,OAAO,IAAI,aAAa,kLAAC,GAAG,CAAC,QAAA,AAAS,EAAC,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AAC1D,CAAC;AAUK,SAAU,YAAY,CACxB,KAAY,EAAE,KAAgB,EAAE,IAAa;IAC/C,yCAAyC;IACzC,MAAM,SAAS,8KAAG,GAAG,CAAC,GAAA,AAAI,EAAC,KAAK,CAAC,CAAC;IAClC,OAAO,IAAI,aAAa,CAAC,SAAS,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AACnD,CAAC;AAUK,SAAU,QAAQ,CACpB,CAAS,EAAE,KAAgB,EAAE,IAAa;IAC5C,MAAM,SAAS,IAAG,GAAG,CAAC,sLAAA,AAAQ,EAAC,CAAC,CAAC,CAAC;IAClC,OAAO,IAAI,aAAa,CAAC,SAAS,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AACnD,CAAC;AAUK,SAAU,WAAW,CACvB,IAAY,EAAE,KAAgB,EAAE,IAAa;IAC/C,OAAO,IAAI,aAAa,0KAAC,GAAG,CAAC,GAAA,AAAG,EAAC,IAAI,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AACvD,CAAC;AAYK,SAAU,qBAAqB,CACjC,KAAY,EAAE,MAAc,EAAE,MAAc,EAAE,KAAgB,EAC9D,IAAa,EAAE,IAAI,GAAG,eAAe;IACvC,OAAO,IAAI,aAAa,KACpB,GAAG,CAAC,6LAAA,AAAa,EAAC,KAAK,EAAE,MAAM,EAAE,MAAM,EAAE,KAAK,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AACpE,CAAC;AAYK,SAAU,uBAAuB,CACnC,KAAY,EAAE,IAAI,GAAG,GAAG,EAAE,MAAM,GAAG,GAAG,EAAE,KAAgB,EAAE,IAAa,EACvE,IAAI,GAAG,iBAAiB;IAC1B,yEAAyE;IACzE,mBAAmB;IACnB,KAAK,GAAG,KAAK,IAAI,SAAS,CAAC;IAC3B,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,OAAO,EAAE;QAC5C,MAAM,IAAI,0LAAmB,CACzB,CAAA,oCAAA,EAAuC,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;KACtD;IACD,OAAO,IAAI,aAAa,wLACpB,GAAG,CAAC,cAAA,AAAe,EAAC,KAAK,EAAE,IAAI,EAAE,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AAC1E,CAAC;AAWK,SAAU,oBAAoB,CAChC,KAAY,EAAE,IAAI,GAAG,GAAG,EAAE,MAAM,GAAG,GAAG,EAAE,KAAgB,EAAE,IAAa,EACvE,IAAI,GAAG,cAAc;IACvB,KAAK,GAAG,KAAK,IAAI,SAAS,CAAC;IAC3B,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,KAAK,OAAO,EAAE;QAC5C,MAAM,wKAAI,sBAAmB,CACzB,CAAA,4CAAA,EAA+C,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;KAC9D;IACD,OAAO,IAAI,aAAa,qLACpB,GAAG,CAAC,WAAA,AAAY,EAAC,KAAK,EAAE,IAAI,EAAE,MAAM,EAAE,KAAK,EAAE,IAAI,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC;AACvE,CAAC;AAQK,SAAU,MAAM,CAAC,CAAgB,EAAE,IAAY;IACnD,OAAO,CAAC,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAQK,SAAU,SAAS,CAAC,CAAgB,EAAE,SAAiB;IAC3D,OAAO,CAAC,CAAC,KAAK,2KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,CAAC,IAAI,EAAE,EAAE,SAAS,CAAC,CAAC,CAAC;AAC/C,CAAC;AAQK,SAAU,SAAS,CAAC,CAAgB,EAAE,SAAiB;IAC3D,OAAO,CAAC,CAAC,KAAK,CAAC,GAAG,CAAC,4KAAA,AAAG,EAAC,CAAC,CAAC,IAAI,EAAE,EAAE,SAAS,CAAC,CAAC,CAAC;AAC/C,CAAC;AAQK,SAAU,aAAa,CAAC,EAAmB;IAC/C,OAAO,EAAE,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC;AAC/B,CAAC;AAUK,SAAU,aAAa,CACzB,kBAAkD;IACpD,kBAAkB,CAAC,OAAO,EAAC,gBAAgB,CAAC,EAAE;QAC5C,MAAM,QAAQ,GAAkB,gBAAgB,CAAC,CAAC,CAAC,CAAC;QACpD,QAAQ,CAAC,KAAK,CAAC,gBAAgB,CAAC,CAAC,CAAC,CAAC,CAAC;IACtC,CAAC,CAAC,CAAC;AACL,CAAC;AAUK,SAAU,SAAS,CACrB,MAAwB,EAAE,SAA0B;IACtD,6EAA6E;IAC7E,mCAAmC;IACnC,MAAM,YAAY,GACd,SAAS,CAAC,GAAG,EAAC,QAAQ,CAAC,EAAE,AAAC,QAAQ,CAAC,IAAI,EAAkB,CAAC,CAAC;IAC/D,MAAM,aAAa,4KAAG,gBAAA,AAAa,EAAC,MAAM,EAAE,YAAY,CAAC,CAAC;IAC1D,OAAO,SAAS,CAAC,GAAG,EAAC,QAAQ,CAAC,EAAE,AAAC,aAAa,CAAC,KAAK,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;AACvE,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1007, "column": 0}, "map": {"version":3,"file":"flags_layers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/flags_layers.js/__/__/__/__/__/tfjs-layers/src/flags_layers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2022 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {env} from '@tensorflow/tfjs-core';\n\nimport {updateCacheMaxEntries} from './engine/executor';\n\nexport const ENV = env();\n\n/** The max number of entries for the caches of layers' topological sort. */\nENV.registerFlag(\n    'TOPOLOGICAL_SORT_CACHE_MAX_ENTRIES', () => 100, updateCacheMaxEntries);\n"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;;;;AAEH,OAAO,EAAC,GAAG,EAAC,MAAM,uBAAuB,CAAC;AAE1C,OAAO,EAAC,qBAAqB,EAAC,MAAM,mBAAmB,CAAC;;;AAEjD,MAAM,GAAG,8KAAG,MAAA,AAAG,EAAE,CAAC;AAEzB,0EAAA,EAA4E,CAC5E,GAAG,CAAC,YAAY,CACZ,oCAAoC,EAAE,GAAG,CAAG,CAAD,EAAI,kLAAE,wBAAqB,CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1038, "column": 0}, "map": {"version":3,"file":"constraints.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/constraints.js/__/__/__/__/__/tfjs-layers/src/constraints.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/contraints.py */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport {epsilon} from './backend/common';\nimport {deserializeKerasObject, serializeKerasObject} from './utils/generic_utils';\n\n/**\n * Helper function used by many of the Constraints to find the L2Norms.\n */\nfunction calcL2Norms(w: Tensor, axis: number): Tensor {\n  return tidy(() => tfc.sqrt(tfc.sum(tfc.mul(w, w), axis, true)));\n}\n\n/**\n * Base class for functions that impose constraints on weight values\n *\n * @doc {\n *   heading: 'Constraints',\n *   subheading: 'Classes',\n *   namespace: 'constraints'\n * }\n */\nexport abstract class Constraint extends serialization.Serializable {\n  /* Porting note: was __call__, apply chosen to match other similar choices */\n  abstract apply(w: Tensor): Tensor;\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\nexport interface MaxNormArgs {\n  /**\n   * Maximum norm for incoming weights\n   */\n  maxValue?: number;\n  /**\n   * Axis along which to calculate norms.\n   *\n   *  For instance, in a `Dense` layer the weight matrix\n   *  has shape `[inputDim, outputDim]`,\n   *  set `axis` to `0` to constrain each weight vector\n   *  of length `[inputDim,]`.\n   *  In a `Conv2D` layer with `dataFormat=\"channels_last\"`,\n   *  the weight tensor has shape\n   *  `[rows, cols, inputDepth, outputDepth]`,\n   *  set `axis` to `[0, 1, 2]`\n   *  to constrain the weights of each filter tensor of size\n   *  `[rows, cols, inputDepth]`.\n   */\n  axis?: number;\n}\n\nexport class MaxNorm extends Constraint {\n  /** @nocollapse */\n  static readonly className = 'MaxNorm';\n  private maxValue: number;\n  private axis: number;\n  private readonly defaultMaxValue = 2;\n  private readonly defaultAxis = 0;\n\n  constructor(args: MaxNormArgs) {\n    super();\n    this.maxValue =\n        args.maxValue != null ? args.maxValue : this.defaultMaxValue;\n    this.axis = args.axis != null ? args.axis : this.defaultAxis;\n  }\n\n  apply(w: Tensor): Tensor {\n    return tidy(() => {\n      const norms = calcL2Norms(w, this.axis);\n      const desired = tfc.clipByValue(norms, 0, this.maxValue);\n      return tfc.mul(w, tfc.div(desired, tfc.add(epsilon(), norms)));\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {maxValue: this.maxValue, axis: this.axis};\n  }\n}\nserialization.registerClass(MaxNorm);\n\nexport interface UnitNormArgs {\n  /**\n   * Axis along which to calculate norms.\n   *\n   * For instance, in a `Dense` layer the weight matrix\n   * has shape `[inputDim, outputDim]`,\n   * set `axis` to `0` to constrain each weight vector\n   * of length `[inputDim,]`.\n   * In a `Conv2D` layer with `dataFormat=\"channels_last\"`,\n   * the weight tensor has shape\n   * `[rows, cols, inputDepth, outputDepth]`,\n   * set `axis` to `[0, 1, 2]`\n   * to constrain the weights of each filter tensor of size\n   * `[rows, cols, inputDepth]`.\n   */\n  axis?: number;\n}\n\nexport class UnitNorm extends Constraint {\n  /** @nocollapse */\n  static readonly className = 'UnitNorm';\n  private axis: number;\n  private readonly defaultAxis = 0;\n  constructor(args: UnitNormArgs) {\n    super();\n    this.axis = args.axis != null ? args.axis : this.defaultAxis;\n  }\n\n  apply(w: Tensor): Tensor {\n    return tidy(\n        () => tfc.div(w, tfc.add(epsilon(), calcL2Norms(w, this.axis))));\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {axis: this.axis};\n  }\n}\nserialization.registerClass(UnitNorm);\n\nexport class NonNeg extends Constraint {\n  /** @nocollapse */\n  static readonly className = 'NonNeg';\n\n  apply(w: Tensor): Tensor {\n    return tfc.relu(w);\n  }\n}\nserialization.registerClass(NonNeg);\n\nexport interface MinMaxNormArgs {\n  /**\n   * Minimum norm for incoming weights\n   */\n  minValue?: number;\n  /**\n   * Maximum norm for incoming weights\n   */\n  maxValue?: number;\n  /**\n   * Axis along which to calculate norms.\n   * For instance, in a `Dense` layer the weight matrix\n   * has shape `[inputDim, outputDim]`,\n   * set `axis` to `0` to constrain each weight vector\n   * of length `[inputDim,]`.\n   * In a `Conv2D` layer with `dataFormat=\"channels_last\"`,\n   * the weight tensor has shape\n   * `[rows, cols, inputDepth, outputDepth]`,\n   * set `axis` to `[0, 1, 2]`\n   * to constrain the weights of each filter tensor of size\n   * `[rows, cols, inputDepth]`.\n   */\n  axis?: number;\n  /**\n   * Rate for enforcing the constraint: weights will be rescaled to yield:\n   * `(1 - rate) * norm + rate * norm.clip(minValue, maxValue)`.\n   * Effectively, this means that rate=1.0 stands for strict\n   * enforcement of the constraint, while rate<1.0 means that\n   * weights will be rescaled at each step to slowly move\n   * towards a value inside the desired interval.\n   */\n  rate?: number;\n}\n\nexport class MinMaxNorm extends Constraint {\n  /** @nocollapse */\n  static readonly className = 'MinMaxNorm';\n  private minValue: number;\n  private maxValue: number;\n  private rate: number;\n  private axis: number;\n  private readonly defaultMinValue = 0.0;\n  private readonly defaultMaxValue = 1.0;\n  private readonly defaultRate = 1.0;\n  private readonly defaultAxis = 0;\n\n  constructor(args: MinMaxNormArgs) {\n    super();\n    this.minValue =\n        args.minValue != null ? args.minValue : this.defaultMinValue;\n    this.maxValue =\n        args.maxValue != null ? args.maxValue : this.defaultMaxValue;\n    this.rate = args.rate != null ? args.rate : this.defaultRate;\n    this.axis = args.axis != null ? args.axis : this.defaultAxis;\n  }\n\n  apply(w: Tensor): Tensor {\n    return tidy(() => {\n      const norms = calcL2Norms(w, this.axis);\n      const desired = tfc.add(\n          tfc.mul(\n              this.rate, tfc.clipByValue(norms, this.minValue, this.maxValue)),\n          tfc.mul(1.0 - this.rate, norms));\n      return tfc.mul(w, tfc.div(desired, tfc.add(epsilon(), norms)));\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    return {\n      minValue: this.minValue,\n      maxValue: this.maxValue,\n      rate: this.rate,\n      axis: this.axis\n    };\n  }\n}\nserialization.registerClass(MinMaxNorm);\n\n/** @docinline */\nexport type ConstraintIdentifier =\n    'maxNorm'|'minMaxNorm'|'nonNeg'|'unitNorm'|string;\n\n// Maps the JavaScript-like identifier keys to the corresponding registry\n// symbols.\nexport const CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP:\n    {[identifier in ConstraintIdentifier]: string} = {\n      'maxNorm': 'MaxNorm',\n      'minMaxNorm': 'MinMaxNorm',\n      'nonNeg': 'NonNeg',\n      'unitNorm': 'UnitNorm'\n    };\n\nexport function serializeConstraint(constraint: Constraint):\n    serialization.ConfigDictValue {\n  return serializeKerasObject(constraint);\n}\n\nexport function deserializeConstraint(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Constraint {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'constraint');\n}\n\nexport function getConstraint(identifier: ConstraintIdentifier|\n                              serialization.ConfigDict|Constraint): Constraint {\n  if (identifier == null) {\n    return null;\n  }\n  if (typeof identifier === 'string') {\n    const className = identifier in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP ?\n        CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] :\n        identifier;\n    const config = {className, config: {}};\n    return deserializeConstraint(config);\n  } else if (identifier instanceof Constraint) {\n    return identifier;\n  } else {\n    return deserializeConstraint(identifier);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,wCAAA,EAA0C;;;;;;;;;;;;;;;;;AAE1C,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;AAC7C,OAAO,EAAC,aAAa,EAAU,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAClE,OAAO,EAAC,OAAO,EAAC,MAAM,kBAAkB,CAAC;AACzC,OAAO,EAAC,sBAAsB,EAAE,oBAAoB,EAAC,MAAM,uBAAuB,CAAC;;;;;AAEnF;;GAEG,CACH,SAAS,WAAW,CAAC,CAAS,EAAE,IAAY;IAC1C,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE,0KAAC,GAAG,CAAC,GAAA,AAAI,4KAAC,GAAG,CAAC,EAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC;AAClE,CAAC;AAWK,MAAgB,UAAW,2NAAQ,gBAAa,CAAC,YAAY;IAGjE,SAAS,GAAA;QACP,OAAO,CAAA,CAAE,CAAC;IACZ,CAAC;CACF;AAwBD,MAAa,OAAQ,SAAQ,UAAU;IAQrC,YAAY,IAAiB,CAAA;QAC3B,KAAK,EAAE,CAAC;QAJO,IAAA,CAAA,eAAe,GAAG,CAAC,CAAC;QACpB,IAAA,CAAA,WAAW,GAAG,CAAC,CAAC;QAI/B,IAAI,CAAC,QAAQ,GACT,IAAI,CAAC,QAAQ,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC,IAAI,CAAC,eAAe,CAAC;QACjE,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC;IAC/D,CAAC;IAED,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;YACf,MAAM,KAAK,GAAG,WAAW,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;YACxC,MAAM,OAAO,OAAG,GAAG,CAAC,0LAAA,AAAW,EAAC,KAAK,EAAE,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC;YACzD,iLAAO,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,MAAE,GAAG,CAAC,wKAAA,AAAG,EAAC,OAAO,4KAAE,GAAG,CAAC,EAAA,AAAG,qLAAC,UAAA,AAAO,EAAE,GAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACjE,CAAC,CAAC,CAAC;IACL,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,QAAQ,EAAE,IAAI,CAAC,QAAQ;YAAE,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IACpD,CAAC;;AAxBD,gBAAA,EAAkB,CACF,QAAA,SAAS,GAAG,SAAS,AAAZ,CAAa;;iNAyBxC,iBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAoBrC,MAAa,QAAS,SAAQ,UAAU;IAKtC,YAAY,IAAkB,CAAA;QAC5B,KAAK,EAAE,CAAC;QAFO,IAAA,CAAA,WAAW,GAAG,CAAC,CAAC;QAG/B,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC;IAC/D,CAAC;IAED,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EACP,GAAG,EAAE,GAAC,GAAG,CAAC,wKAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,qLAAC,UAAA,AAAO,EAAE,GAAE,WAAW,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;IACvE,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;QAAA,CAAC,CAAC;IAC3B,CAAC;;AAhBD,gBAAA,EAAkB,CACF,SAAA,SAAS,GAAG,UAAU,AAAb,CAAc;;AAiBzC,kOAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC,MAAa,MAAO,SAAQ,UAAU;IAIpC,KAAK,CAAC,CAAS,EAAA;QACb,kLAAO,GAAG,CAAC,GAAA,AAAI,EAAC,CAAC,CAAC,CAAC;IACrB,CAAC;;AALD,gBAAA,EAAkB,CACF,OAAA,SAAS,GAAG,QAAQ,CAAC;;kNAMvC,gBAAa,CAAC,aAAa,CAAC,MAAM,CAAC,CAAC;AAoCpC,MAAa,UAAW,SAAQ,UAAU;IAYxC,YAAY,IAAoB,CAAA;QAC9B,KAAK,EAAE,CAAC;QANO,IAAA,CAAA,eAAe,GAAG,GAAG,CAAC;QACtB,IAAA,CAAA,eAAe,GAAG,GAAG,CAAC;QACtB,IAAA,CAAA,WAAW,GAAG,GAAG,CAAC;QAClB,IAAA,CAAA,WAAW,GAAG,CAAC,CAAC;QAI/B,IAAI,CAAC,QAAQ,GACT,IAAI,CAAC,QAAQ,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC,IAAI,CAAC,eAAe,CAAC;QACjE,IAAI,CAAC,QAAQ,GACT,IAAI,CAAC,QAAQ,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC,CAAC,IAAI,CAAC,eAAe,CAAC;QACjE,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC;QAC7D,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,WAAW,CAAC;IAC/D,CAAC;IAED,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;YACf,MAAM,KAAK,GAAG,WAAW,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;YACxC,MAAM,OAAO,6KAAG,GAAG,CAAC,EAAA,AAAG,4KACnB,GAAG,CAAC,EAAA,AAAG,EACH,IAAI,CAAC,IAAI,EAAE,GAAG,CAAC,8LAAA,AAAW,EAAC,KAAK,EAAE,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,QAAQ,CAAC,CAAC,EACpE,GAAG,CAAC,4KAAA,AAAG,EAAC,GAAG,GAAG,IAAI,CAAC,IAAI,EAAE,KAAK,CAAC,CAAC,CAAC;YACrC,iLAAO,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,GAAE,GAAG,CAAC,2KAAA,AAAG,EAAC,OAAO,4KAAE,GAAG,CAAC,EAAA,AAAG,GAAC,4LAAA,AAAO,EAAE,GAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACjE,CAAC,CAAC,CAAC;IACL,CAAC;IAEQ,SAAS,GAAA;QAChB,OAAO;YACL,QAAQ,EAAE,IAAI,CAAC,QAAQ;YACvB,QAAQ,EAAE,IAAI,CAAC,QAAQ;YACvB,IAAI,EAAE,IAAI,CAAC,IAAI;YACf,IAAI,EAAE,IAAI,CAAC,IAAI;SAChB,CAAC;IACJ,CAAC;;AAvCD,gBAAA,EAAkB,CACF,WAAA,SAAS,GAAG,YAAY,AAAf,CAAgB;;kNAwC3C,gBAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;AAQjC,MAAM,yCAAyC,GACD;IAC/C,SAAS,EAAE,SAAS;IACpB,YAAY,EAAE,YAAY;IAC1B,QAAQ,EAAE,QAAQ;IAClB,UAAU,EAAE,UAAU;CACvB,CAAC;AAEA,SAAU,mBAAmB,CAAC,UAAsB;IAExD,+LAAO,uBAAA,AAAoB,EAAC,UAAU,CAAC,CAAC;AAC1C,CAAC;AAEK,SAAU,qBAAqB,CACjC,MAAgC,EAChC,gBAA0C,CAAA,CAAE;IAC9C,+LAAO,yBAAA,AAAsB,EACzB,MAAM,oNAAE,gBAAa,CAAC,gBAAgB,CAAC,MAAM,EAAE,CAAC,YAAY,EAC5D,aAAa,EAAE,YAAY,CAAC,CAAC;AACnC,CAAC;AAEK,SAAU,aAAa,CAAC,UACmC;IAC/D,IAAI,UAAU,IAAI,IAAI,EAAE;QACtB,OAAO,IAAI,CAAC;KACb;IACD,IAAI,OAAO,UAAU,KAAK,QAAQ,EAAE;QAClC,MAAM,SAAS,GAAG,UAAU,IAAI,yCAAyC,CAAC,CAAC,CACvE,yCAAyC,CAAC,UAAU,CAAC,CAAC,CAAC,CACvD,UAAU,CAAC;QACf,MAAM,MAAM,GAAG;YAAC,SAAS;YAAE,MAAM,EAAE,CAAA,CAAE;QAAA,CAAC,CAAC;QACvC,OAAO,qBAAqB,CAAC,MAAM,CAAC,CAAC;KACtC,MAAM,IAAI,UAAU,YAAY,UAAU,EAAE;QAC3C,OAAO,UAAU,CAAC;KACnB,MAAM;QACL,OAAO,qBAAqB,CAAC,UAAU,CAAC,CAAC;KAC1C;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1200, "column": 0}, "map": {"version":3,"file":"exports_constraints.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_constraints.js/__/__/__/__/__/tfjs-layers/src/exports_constraints.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// tslint:disable-next-line:max-line-length\nimport {Constraint, MaxNorm, MaxNormArgs, MinMaxNorm, MinMaxNormArgs, NonNeg, UnitNorm, UnitNormArgs} from './constraints';\n\n/**\n * MaxNorm weight constraint.\n *\n * Constrains the weights incident to each hidden unit\n * to have a norm less than or equal to a desired value.\n *\n * References\n *       - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n * Srivastava, Hinton, et al.\n * 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n *\n * @doc {heading: 'Constraints',namespace: 'constraints'}\n */\nexport function maxNorm(args: MaxNormArgs): Constraint {\n  return new MaxNorm(args);\n}\n\n/**\n * Constrains the weights incident to each hidden unit to have unit norm.\n *\n * @doc {heading: 'Constraints', namespace: 'constraints'}\n */\nexport function unitNorm(args: UnitNormArgs): Constraint {\n  return new UnitNorm(args);\n}\n\n/**\n * Constrains the weight to be non-negative.\n *\n * @doc {heading: 'Constraints', namespace: 'constraints'}\n */\nexport function nonNeg(): Constraint {\n  return new NonNeg();\n}\n\n/** @doc {heading: 'Constraints', namespace: 'constraints'} */\nexport function minMaxNorm(config: MinMaxNormArgs): Constraint {\n  return new MinMaxNorm(config);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CACH,2CAA2C;;;;;;;AAC3C,OAAO,EAAa,OAAO,EAAe,UAAU,EAAkB,MAAM,EAAE,QAAQ,EAAe,MAAM,eAAe,CAAC;;AAerH,SAAU,OAAO,CAAC,IAAiB;IACvC,OAAO,6KAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAOK,SAAU,QAAQ,CAAC,IAAkB;IACzC,OAAO,6KAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAOK,SAAU,MAAM;IACpB,OAAO,6KAAI,SAAM,EAAE,CAAC;AACtB,CAAC;AAGK,SAAU,UAAU,CAAC,MAAsB;IAC/C,OAAO,6KAAI,aAAU,CAAC,MAAM,CAAC,CAAC;AAChC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1235, "column": 0}, "map": {"version":3,"file":"exports_initializers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_initializers.js/__/__/__/__/__/tfjs-layers/src/exports_initializers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// tslint:disable-next-line:max-line-length\nimport {Constant, ConstantArgs, GlorotNormal, GlorotUniform, HeNormal, HeUniform, Identity, IdentityArgs, Initializer, LeCunNormal, LeCunUniform, Ones, Orthogonal, OrthogonalArgs, RandomNormal, RandomNormalArgs, RandomUniform, RandomUniformArgs, SeedOnlyInitializerArgs, TruncatedNormal, TruncatedNormalArgs, VarianceScaling, VarianceScalingArgs, Zeros} from './initializers';\n\n/**\n * Initializer that generates tensors initialized to 0.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function zeros(): Zeros {\n  return new Zeros();\n}\n\n/**\n * Initializer that generates tensors initialized to 1.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function ones(): Initializer {\n  return new Ones();\n}\n\n/**\n * Initializer that generates values initialized to some constant.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function constant(args: ConstantArgs): Initializer {\n  return new Constant(args);\n}\n\n/**\n * Initializer that generates random values initialized to a uniform\n * distribution.\n *\n * Values will be distributed uniformly between the configured minval and\n * maxval.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function randomUniform(args: RandomUniformArgs): Initializer {\n  return new RandomUniform(args);\n}\n\n/**\n * Initializer that generates random values initialized to a normal\n * distribution.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function randomNormal(args: RandomNormalArgs): Initializer {\n  return new RandomNormal(args);\n}\n\n/**\n * Initializer that generates random values initialized to a truncated normal\n * distribution.\n *\n * These values are similar to values from a `RandomNormal` except that values\n * more than two standard deviations from the mean are discarded and re-drawn.\n * This is the recommended initializer for neural network weights and filters.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function truncatedNormal(args: TruncatedNormalArgs): Initializer {\n  return new TruncatedNormal(args);\n}\n\n/**\n * Initializer that generates the identity matrix.\n * Only use for square 2D matrices.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function identity(args: IdentityArgs): Initializer {\n  return new Identity(args);\n}\n\n/**\n * Initializer capable of adapting its scale to the shape of weights.\n * With distribution=NORMAL, samples are drawn from a truncated normal\n * distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n *   - number of input units in the weight tensor, if mode = FAN_IN.\n *   - number of output units, if mode = FAN_OUT.\n *   - average of the numbers of input and output units, if mode = FAN_AVG.\n * With distribution=UNIFORM,\n * samples are drawn from a uniform distribution\n * within [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\nexport function varianceScaling(config: VarianceScalingArgs): Initializer {\n  return new VarianceScaling(config);\n}\n\n/**\n * Glorot uniform initializer, also called Xavier uniform initializer.\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function glorotUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new GlorotUniform(args);\n}\n\n/**\n * Glorot normal initializer, also called Xavier normal initializer.\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor.\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function glorotNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new GlorotNormal(args);\n}\n\n/**\n * He normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function heNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new HeNormal(args);\n}\n\n/**\n * He uniform initializer.\n *\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / fan_in)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\nexport function heUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new HeUniform(args);\n}\n\n/**\n * LeCun normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(1 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * References:\n *   [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n *   [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function leCunNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new LeCunNormal(args);\n}\n\n/**\n * LeCun uniform initializer.\n *\n * It draws samples from a uniform distribution in the interval\n * `[-limit, limit]` with `limit = sqrt(3 / fanIn)`,\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function leCunUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new LeCunUniform(args);\n}\n\n/**\n * Initializer that generates a random orthogonal matrix.\n *\n * Reference:\n * [Saxe et al., http://arxiv.org/abs/1312.6120](http://arxiv.org/abs/1312.6120)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function orthogonal(args: OrthogonalArgs): Initializer {\n  return new Orthogonal(args);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CACH,2CAA2C;;;;;;;;;;;;;;;;;;AAC3C,OAAO,EAAC,QAAQ,EAAgB,YAAY,EAAE,aAAa,EAAE,QAAQ,EAAE,SAAS,EAAE,QAAQ,EAA6B,WAAW,EAAE,YAAY,EAAE,IAAI,EAAE,UAAU,EAAkB,YAAY,EAAoB,aAAa,EAA8C,eAAe,EAAuB,eAAe,EAAuB,KAAK,EAAC,MAAM,gBAAgB,CAAC;;AAOlX,SAAU,KAAK;IACnB,OAAO,8KAAI,QAAK,EAAE,CAAC;AACrB,CAAC;AAOK,SAAU,IAAI;IAClB,OAAO,8KAAI,OAAI,EAAE,CAAC;AACpB,CAAC;AAOK,SAAU,QAAQ,CAAC,IAAkB;IACzC,OAAO,8KAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAWK,SAAU,aAAa,CAAC,IAAuB;IACnD,OAAO,8KAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAQK,SAAU,YAAY,CAAC,IAAsB;IACjD,OAAO,8KAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAYK,SAAU,eAAe,CAAC,IAAyB;IACvD,OAAO,8KAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAQK,SAAU,QAAQ,CAAC,IAAkB;IACzC,OAAO,8KAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAeK,SAAU,eAAe,CAAC,MAA2B;IACzD,OAAO,8KAAI,kBAAe,CAAC,MAAM,CAAC,CAAC;AACrC,CAAC;AAeK,SAAU,aAAa,CAAC,IAA6B;IACzD,OAAO,8KAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAeK,SAAU,YAAY,CAAC,IAA6B;IACxD,OAAO,8KAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAcK,SAAU,QAAQ,CAAC,IAA6B;IACpD,OAAO,8KAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAcK,SAAU,SAAS,CAAC,IAA6B;IACrD,OAAO,8KAAI,YAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAeK,SAAU,WAAW,CAAC,IAA6B;IACvD,OAAO,8KAAI,cAAW,CAAC,IAAI,CAAC,CAAC;AAC/B,CAAC;AAWK,SAAU,YAAY,CAAC,IAA6B;IACxD,OAAO,8KAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAUK,SAAU,UAAU,CAAC,IAAoB;IAC7C,OAAO,8KAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1314, "column": 0}, "map": {"version":3,"file":"logs.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/logs.js/__/__/__/__/__/tfjs-layers/src/logs.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {dispose, Scalar} from '@tensorflow/tfjs-core';\n\n/**\n * Logs in which values can be either numbers or Tensors (Scalars).\n *\n * Used internally.\n */\nexport type UnresolvedLogs = {\n  [key: string]: number|Scalar;\n};\n\n/**\n * Turn any Scalar values in a Logs object into actual number values.\n *\n * @param logs The `Logs` object to be resolved in place.\n */\nexport async function resolveScalarsInLogs(logs: UnresolvedLogs) {\n  if (logs == null) {\n    return;\n  }\n  const promises: Array<Promise<Float32Array|Int32Array|Uint8Array>> = [];\n  const keys: string[] = [];\n  const scalarsToDispose: Scalar[] = [];\n  for (const key in logs) {\n    const value = logs[key];\n    if (typeof value !== 'number') {\n      const valueScalar = value;\n      promises.push(valueScalar.data());\n      keys.push(key);\n      scalarsToDispose.push(valueScalar);\n    }\n  }\n  if (promises.length > 0) {\n    const values = await Promise.all(promises);\n    for (let i = 0; i < values.length; ++i) {\n      logs[keys[i]] = values[i][0];\n    }\n    // Dispose the original scalar tensors.\n    dispose(scalarsToDispose);\n  }\n}\n\n/**\n * Dispose all Tensors in an UnresolvedLogs object.\n *\n * @param logs An `UnresolvedLogs` object potentially containing `tf.Tensor`s in\n *   places where the values can be `tf.Tensor` or `number`.\n */\nexport function disposeTensorsInLogs(logs: UnresolvedLogs) {\n  if (logs == null) {\n    return;\n  }\n  for (const key in logs) {\n    const value = logs[key];\n    if (typeof value !== 'number') {\n      value.dispose();\n    }\n  }\n}\n\n/**\n * Logs in which values can only be numbers.\n *\n * Used when calling client-provided custom callbacks.\n */\nexport type Logs = {\n  [key: string]: number;\n};\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;AAEH,OAAO,EAAC,OAAO,EAAS,MAAM,uBAAuB,CAAC;;;AAgB/C,KAAK,UAAU,oBAAoB,CAAC,IAAoB;IAC7D,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,OAAO;KACR;IACD,MAAM,QAAQ,GAAuD,EAAE,CAAC;IACxE,MAAM,IAAI,GAAa,EAAE,CAAC;IAC1B,MAAM,gBAAgB,GAAa,EAAE,CAAC;IACtC,IAAK,MAAM,GAAG,IAAI,IAAI,CAAE;QACtB,MAAM,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC;QACxB,IAAI,OAAO,KAAK,KAAK,QAAQ,EAAE;YAC7B,MAAM,WAAW,GAAG,KAAK,CAAC;YAC1B,QAAQ,CAAC,IAAI,CAAC,WAAW,CAAC,IAAI,EAAE,CAAC,CAAC;YAClC,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;YACf,gBAAgB,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;SACpC;KACF;IACD,IAAI,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE;QACvB,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC;QAC3C,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACtC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;SAC9B;QACD,uCAAuC;+KACvC,UAAA,AAAO,EAAC,gBAAgB,CAAC,CAAC;KAC3B;AACH,CAAC;AAQK,SAAU,oBAAoB,CAAC,IAAoB;IACvD,IAAI,IAAI,IAAI,IAAI,EAAE;QAChB,OAAO;KACR;IACD,IAAK,MAAM,GAAG,IAAI,IAAI,CAAE;QACtB,MAAM,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC;QACxB,IAAI,OAAO,KAAK,KAAK,QAAQ,EAAE;YAC7B,KAAK,CAAC,OAAO,EAAE,CAAC;SACjB;KACF;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1371, "column": 0}, "map": {"version":3,"file":"base_callbacks.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/base_callbacks.js/__/__/__/__/__/tfjs-layers/src/base_callbacks.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/callbacks.py */\n\nimport {add, div, keep, mul, nextFrame, Scalar, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Container} from './engine/container';\nimport {ValueError} from './errors';\nimport {Logs, resolveScalarsInLogs, UnresolvedLogs} from './logs';\nimport * as generic_utils from './utils/generic_utils';\n\n/** Verbosity logging level when fitting a model. */\nexport enum ModelLoggingVerbosity {\n  SILENT = 0,\n  VERBOSE = 1\n}\n\n/** How often to yield to the main thread when training (in ms). */\nexport const DEFAULT_YIELD_EVERY_MS = 125;\n\nexport type Params = {\n  [key: string]: number|string|boolean|number[]|string[]|boolean[];\n};\n\nexport type YieldEveryOptions = 'auto'|'batch'|'epoch'|'never'|number;\n\n/**\n * Abstract base class used to build new callbacks.\n *\n * The `logs` dictionary that callback methods take as argument will contain\n * keys for quantities relevant to the current batch or epoch.\n *\n * Currently, the `.fit()` method of the `Sequential` model class\n * will include the following quantities in the `logs` that\n * it passes to its callbacks:\n *\n * onEpochEnd: Logs include `acc` and `loss`, and optionally include `valLoss`\n *   (if validation is enabled in `fit`), and `valAcc` (if validation and\n *   accuracy monitoring are enabled).\n * onBatchBegin: Logs include `size`, the number of samples in the current\n *   batch.\n * onBatchEnd: Logs include `loss`, and optionally `acc` (if accuracy monitoring\n *   is enabled).\n */\nexport abstract class BaseCallback {\n  // TODO(michaelterry): This type is a best guess.\n  validationData: Tensor|Tensor[] = null;\n  /**\n   * Training parameters (eg. verbosity, batch size, number of epochs...).\n   */\n  params: Params;\n\n  setParams(params: Params): void {\n    this.params = params;\n  }\n\n  async onEpochBegin(epoch: number, logs?: UnresolvedLogs) {}\n\n  async onEpochEnd(epoch: number, logs?: UnresolvedLogs) {}\n\n  async onBatchBegin(batch: number, logs?: UnresolvedLogs) {}\n\n  async onBatchEnd(batch: number, logs?: UnresolvedLogs) {}\n\n  async onTrainBegin(logs?: UnresolvedLogs) {}\n\n  async onTrainEnd(logs?: UnresolvedLogs) {}\n\n  // LayersModel needs to call Callback.setModel(), but cannot actually depend\n  // on Callback because that creates a cyclic dependency.  Providing this no-op\n  // method on BaseCallback breaks the cycle: this way LayersModel can depend on\n  // BaseCallback but not on Callback.  The argument is typed as `Container`\n  // (the superclass of LayersModel) to avoid recapitulating the cycle. Callback\n  // overrides this method and enforces that the argument is really a\n  // LayersModel.\n  setModel(model: Container): void {\n    // Do nothing. Use Callback instead of BaseCallback to track the model.\n  }\n}\n\n/**\n * Container abstracting a list of callbacks.\n */\nexport class CallbackList {\n  callbacks: BaseCallback[];\n  queueLength: number;\n\n  // TODO(cais): When the need arises, uncomment the following lines and\n  // implement the queue for time values.\n  // private deltaTBatch: number;\n  // private deltaTsBatchBegin: Array<number>;\n  // private deltaTsBatchEnd: Array<number>;\n\n  /**\n   * Constructor of CallbackList.\n   * @param callbacks Array of `Callback` instances.\n   * @param queueLength Queue length for keeping running statistics over\n   *   callback execution time.\n   */\n  constructor(callbacks?: BaseCallback[], queueLength = 10) {\n    // TODO(cais): Make use of queueLength when implementing the queue for time\n    // values.\n    if (callbacks == null) {\n      callbacks = [];\n    }\n    this.callbacks = callbacks;\n    this.queueLength = queueLength;\n  }\n\n  append(callback: BaseCallback): void {\n    this.callbacks.push(callback);\n  }\n\n  setParams(params: Params): void {\n    for (const callback of this.callbacks) {\n      callback.setParams(params);\n    }\n  }\n\n  setModel(model: Container): void {\n    for (const callback of this.callbacks) {\n      callback.setModel(model);\n    }\n  }\n\n  /**\n   * Called at the start of an epoch.\n   * @param epoch Index of epoch.\n   * @param logs Dictionary of logs.\n   */\n  async onEpochBegin(epoch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onEpochBegin(epoch, logs);\n    }\n  }\n\n  /**\n   * Called at the end of an epoch.\n   * @param epoch Index of epoch.\n   * @param logs Dictionary of logs.\n   */\n  async onEpochEnd(epoch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onEpochEnd(epoch, logs);\n    }\n  }\n\n  /**\n   * Called  right before processing a batch.\n   * @param batch Index of batch within the current epoch.\n   * @param logs Dictionary of logs.\n   */\n  async onBatchBegin(batch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onBatchBegin(batch, logs);\n    }\n  }\n\n  /**\n   * Called at the end of a batch.\n   * @param batch Index of batch within the current epoch.\n   * @param logs Dictionary of logs.\n   */\n  async onBatchEnd(batch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onBatchEnd(batch, logs);\n    }\n  }\n\n  /**\n   * Called at the beginning of training.\n   * @param logs Dictionary of logs.\n   */\n  async onTrainBegin(logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onTrainBegin(logs);\n    }\n  }\n\n  /**\n   * Called at the end of training.\n   * @param logs Dictionary of logs.\n   */\n  async onTrainEnd(logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    for (const callback of this.callbacks) {\n      await callback.onTrainEnd(logs);\n    }\n  }\n}\n\n/**\n * Callback that accumulates epoch averages of metrics.\n *\n * This callback is automatically applied to every LayersModel.\n */\nexport class BaseLogger extends BaseCallback {\n  private seen: number;\n  private totals: UnresolvedLogs;\n\n  constructor() {\n    super();\n  }\n\n  override async onEpochBegin(epoch: number) {\n    this.seen = 0;\n    this.totals = {};\n  }\n\n  override async onBatchEnd(batch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    const batchSize = logs['size'] == null ? 0 : logs['size'] as number;\n    this.seen += batchSize;\n    for (const key in logs) {\n      const value = logs[key];\n      if (typeof value === 'number') {\n        if (!this.totals.hasOwnProperty(key)) {\n          this.totals[key] = 0;\n        }\n        this.totals[key] = this.totals[key] as number + value * batchSize;\n      } else {\n        let oldTotalsToDispose: Scalar;\n        if (key in this.totals) {\n          oldTotalsToDispose = this.totals[key] as Scalar;\n        } else {\n          this.totals[key] = 0;\n        }\n        const total: Scalar =\n            tidy(() => add((this.totals[key]), mul(value, batchSize)));\n        this.totals[key] = total;\n        if (oldTotalsToDispose != null) {\n          oldTotalsToDispose.dispose();\n        }\n      }\n    }\n  }\n\n  override async onEpochEnd(epoch: number, logs?: UnresolvedLogs) {\n    if (logs != null) {\n      for (const key of this.params['metrics'] as string[]) {\n        if (this.totals[key] == null) {\n          continue;\n        }\n        if (typeof this.totals[key] === 'number') {\n          logs[key] = this.totals[key] as number / this.seen;\n        } else {\n          tidy(() => {\n            const log: Scalar = mul(div(1, this.seen), this.totals[key]);\n            logs[key] = log;\n            (this.totals[key] as Tensor).dispose();\n            keep(logs[key] as Scalar);\n          });\n        }\n      }\n    }\n  }\n}\n\n/**\n * Callback that records events into a `History` object. This callback is\n * automatically applied to every TF.js Layers model. The `History` object\n * gets returned by the `fit` method of models.\n */\nexport class History extends BaseCallback {\n  epoch: number[];\n  history: {[key: string]: Array<number|Tensor>};\n\n  override async onTrainBegin(logs?: UnresolvedLogs) {\n    this.epoch = [];\n    this.history = {};\n  }\n\n  override async onEpochEnd(epoch: number, logs?: UnresolvedLogs) {\n    if (logs == null) {\n      logs = {};\n    }\n    this.epoch.push(epoch);\n    for (const key in logs) {\n      if (this.history[key] == null) {\n        this.history[key] = [];\n      }\n      this.history[key].push(logs[key]);\n    }\n  }\n\n  /**\n   * Await the values of all losses and metrics.\n   */\n  async syncData() {\n    const promises: Array<Promise<Float32Array|Int32Array|Uint8Array>> = [];\n    const keys: string[] = [];\n    const indices: number[] = [];\n    for (const key in this.history) {\n      const valueArray = this.history[key];\n      for (let i = 0; i < valueArray.length; ++i) {\n        if (typeof valueArray[i] !== 'number') {\n          const valueScalar = valueArray[i] as Tensor;\n          promises.push(valueScalar.data());\n          keys.push(key);\n          indices.push(i);\n        }\n      }\n    }\n    const values = await Promise.all(promises);\n    for (let n = 0; n < values.length; ++n) {\n      const tensorToDispose = this.history[keys[n]][indices[n]] as Tensor;\n      tensorToDispose.dispose();\n      this.history[keys[n]][indices[n]] = values[n][0];\n    }\n  }\n}\n\nexport interface CustomCallbackArgs {\n  onTrainBegin?: (logs?: Logs) => void | Promise<void>;\n  onTrainEnd?: (logs?: Logs) => void | Promise<void>;\n  onEpochBegin?: (epoch: number, logs?: Logs) => void | Promise<void>;\n  onEpochEnd?: (epoch: number, logs?: Logs) => void | Promise<void>;\n  onBatchBegin?: (batch: number, logs?: Logs) => void | Promise<void>;\n  onBatchEnd?: (batch: number, logs?: Logs) => void | Promise<void>;\n  onYield?: (epoch: number, batch: number, logs: Logs) => void | Promise<void>;\n  // Used for test DI mocking.\n  nowFunc?: Function;\n  nextFrameFunc?: Function;\n}\n\n/**\n * Custom callback for training.\n */\nexport class CustomCallback extends BaseCallback {\n  protected readonly trainBegin: (logs?: Logs) => void | Promise<void>;\n  protected readonly trainEnd: (logs?: Logs) => void | Promise<void>;\n  protected readonly epochBegin:\n      (epoch: number, logs?: Logs) => void | Promise<void>;\n  protected readonly epochEnd:\n      (epoch: number, logs?: Logs) => void | Promise<void>;\n  protected readonly batchBegin:\n      (batch: number, logs?: Logs) => void | Promise<void>;\n  protected readonly batchEnd:\n      (batch: number, logs?: Logs) => void | Promise<void>;\n  protected readonly yield:\n      (epoch: number, batch: number, logs: Logs) => void | Promise<void>;\n\n  private yieldEvery: YieldEveryOptions;\n  private currentEpoch = 0;\n  public nowFunc: Function;\n  public nextFrameFunc: Function;\n\n  constructor(args: CustomCallbackArgs, yieldEvery?: YieldEveryOptions) {\n    super();\n    this.nowFunc = args.nowFunc;\n    this.nextFrameFunc = args.nextFrameFunc || nextFrame;\n    this.yieldEvery = yieldEvery || 'auto';\n    if (this.yieldEvery === 'auto') {\n      this.yieldEvery = DEFAULT_YIELD_EVERY_MS;\n    }\n    if (this.yieldEvery === 'never' && args.onYield != null) {\n      throw new Error(\n          'yieldEvery is `never` but you provided an `onYield` callback. ' +\n          'Either change `yieldEvery` or remove the callback');\n    }\n    if (util.isNumber(this.yieldEvery)) {\n      // Decorate `maybeWait` so it will be called at most once every\n      // `yieldEvery` ms.\n      this.maybeWait = generic_utils.debounce(\n          this.maybeWait.bind(this), this.yieldEvery as number, this.nowFunc);\n    }\n    this.trainBegin = args.onTrainBegin;\n    this.trainEnd = args.onTrainEnd;\n    this.epochBegin = args.onEpochBegin;\n    this.epochEnd = args.onEpochEnd;\n    this.batchBegin = args.onBatchBegin;\n    this.batchEnd = args.onBatchEnd;\n    this.yield = args.onYield;\n  }\n\n  async maybeWait(epoch: number, batch: number, logs: UnresolvedLogs) {\n    const ps: Array<void|Promise<void>> = [];\n    if (this.yield != null) {\n      await resolveScalarsInLogs(logs);\n      ps.push(this.yield(epoch, batch, logs as Logs));\n    }\n    ps.push(this.nextFrameFunc());\n    await Promise.all(ps);\n  }\n\n  override async onEpochBegin(epoch: number, logs?: UnresolvedLogs):\n      Promise<void> {\n    this.currentEpoch = epoch;\n    if (this.epochBegin != null) {\n      await resolveScalarsInLogs(logs);\n      await this.epochBegin(epoch, logs as Logs);\n    }\n  }\n\n  override async onEpochEnd(epoch: number, logs?: UnresolvedLogs):\n      Promise<void> {\n    const ps: Array<void|Promise<void>> = [];\n    if (this.epochEnd != null) {\n      await resolveScalarsInLogs(logs);\n      ps.push(this.epochEnd(epoch, logs as Logs));\n    }\n    if (this.yieldEvery === 'epoch') {\n      ps.push(this.nextFrameFunc());\n    }\n    await Promise.all(ps);\n  }\n\n  override async onBatchBegin(batch: number, logs?: UnresolvedLogs):\n      Promise<void> {\n    if (this.batchBegin != null) {\n      await resolveScalarsInLogs(logs);\n      await this.batchBegin(batch, logs as Logs);\n    }\n  }\n\n  override async onBatchEnd(batch: number, logs?: UnresolvedLogs):\n      Promise<void> {\n    const ps: Array<void|Promise<void>> = [];\n    if (this.batchEnd != null) {\n      await resolveScalarsInLogs(logs);\n      ps.push(this.batchEnd(batch, logs as Logs));\n    }\n    if (this.yieldEvery === 'batch') {\n      ps.push(this.nextFrameFunc());\n    } else if (util.isNumber(this.yieldEvery)) {\n      ps.push(this.maybeWait(this.currentEpoch, batch, logs));\n    }\n    await Promise.all(ps);\n  }\n\n  override async onTrainBegin(logs?: UnresolvedLogs): Promise<void> {\n    if (this.trainBegin != null) {\n      await resolveScalarsInLogs(logs);\n      await this.trainBegin(logs as Logs);\n    }\n  }\n\n  override async onTrainEnd(logs?: UnresolvedLogs): Promise<void> {\n    if (this.trainEnd != null) {\n      await resolveScalarsInLogs(logs);\n      await this.trainEnd(logs as Logs);\n    }\n  }\n}\n\n/**\n * Standardize callbacks or configurations of them to an Array of callbacks.\n */\nexport function standardizeCallbacks(\n    callbacks: BaseCallback|BaseCallback[]|CustomCallbackArgs|\n    CustomCallbackArgs[],\n    yieldEvery: YieldEveryOptions): BaseCallback[] {\n  if (callbacks == null) {\n    callbacks = {} as BaseCallback;\n  }\n  if (callbacks instanceof BaseCallback) {\n    return [callbacks];\n  }\n  if (Array.isArray(callbacks) && callbacks[0] instanceof BaseCallback) {\n    return callbacks as BaseCallback[];\n  }\n  // Convert custom callback configs to custom callback objects.\n  const callbackConfigs =\n      generic_utils.toList<BaseCallback | CustomCallbackArgs>(\n        callbacks) as CustomCallbackArgs[];\n  return callbackConfigs.map(\n      callbackConfig => new CustomCallback(callbackConfig, yieldEvery));\n}\n\nexport declare type BaseCallbackConstructor = {\n  new (): BaseCallback\n};\n\n/**\n * A global registry for callback constructors to be used during\n * LayersModel.fit().\n */\nexport class CallbackConstructorRegistry {\n  private static constructors:\n      {[verbosityLevel: number]: BaseCallbackConstructor[]} = {};\n\n  /**\n   * Blocks public access to constructor.\n   */\n  private constructor() {}\n\n  /**\n   * Register a tf.LayersModel.fit() callback constructor.\n   *\n   * The registered callback constructor will be used to instantiate\n   * callbacks for every tf.LayersModel.fit() call afterwards.\n   *\n   * @param verbosityLevel Level of verbosity at which the `callbackConstructor`\n   *   is to be reigstered.\n   * @param callbackConstructor A no-arg constructor for `tf.Callback`.\n   * @throws Error, if the same callbackConstructor has been registered before,\n   *   either at the same or a different `verbosityLevel`.\n   */\n  static registerCallbackConstructor(\n      verbosityLevel: number, callbackConstructor: BaseCallbackConstructor) {\n    util.assert(\n        verbosityLevel >= 0 && Number.isInteger(verbosityLevel),\n        () => `Verbosity level is expected to be an integer >= 0, ` +\n            `but got ${verbosityLevel}`);\n    CallbackConstructorRegistry.checkForDuplicate(callbackConstructor);\n    if (CallbackConstructorRegistry.constructors[verbosityLevel] == null) {\n      CallbackConstructorRegistry.constructors[verbosityLevel] = [];\n    }\n    CallbackConstructorRegistry.constructors[verbosityLevel].push(\n        callbackConstructor);\n  }\n\n  private static checkForDuplicate(callbackConstructor:\n                                       BaseCallbackConstructor) {\n    for (const levelName in CallbackConstructorRegistry.constructors) {\n      const constructors = CallbackConstructorRegistry.constructors[+levelName];\n      constructors.forEach(ctor => {\n        if (ctor === callbackConstructor) {\n          throw new ValueError('Duplicate callback constructor.');\n        }\n      });\n    }\n  }\n\n  /**\n   * Clear all registered callback constructors.\n   */\n  protected static clear() {\n    CallbackConstructorRegistry.constructors = {};\n  }\n\n  /**\n   * Create callbacks using the registered callback constructors.\n   *\n   * Given `verbosityLevel`, all constructors registered at that level or above\n   * will be called and the instantiated callbacks will be used.\n   *\n   * @param verbosityLevel: Level of verbosity.\n   */\n  static createCallbacks(verbosityLevel: number): BaseCallback[] {\n    const constructors: BaseCallbackConstructor[] = [];\n    for (const levelName in CallbackConstructorRegistry.constructors) {\n      const level = +levelName;\n      if (verbosityLevel >= level) {\n        constructors.push(...CallbackConstructorRegistry.constructors[level]);\n      }\n    }\n    return constructors.map(ctor => new ctor());\n  }\n}\n\nexport function configureCallbacks(\n    callbacks: BaseCallback[], verbose: ModelLoggingVerbosity, epochs: number,\n    initialEpoch: number, numTrainSamples: number, stepsPerEpoch: number,\n    batchSize: number, doValidation: boolean,\n    callbackMetrics: string[]): {callbackList: CallbackList, history: History} {\n  const history = new History();\n  const actualCallbacks: BaseCallback[] = [\n    new BaseLogger(), ...CallbackConstructorRegistry.createCallbacks(verbose)\n  ];\n  if (callbacks != null) {\n    actualCallbacks.push(...callbacks);\n  }\n  actualCallbacks.push(history);\n  const callbackList = new CallbackList(actualCallbacks);\n\n  // TODO(cais): Figure out when this LayersModel instance can have a\n  // dynamically\n  //   set property called 'callback_model' as in PyKeras.\n\n  callbackList.setParams({\n    epochs,\n    initialEpoch,\n    samples: numTrainSamples,\n    steps: stepsPerEpoch,\n    batchSize,\n    verbose,\n    doValidation,\n    metrics: callbackMetrics,\n  });\n  return {callbackList, history};\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,uCAAA,EAAyC;;;;;;;;;;;;;;;;;;AAEzC,OAAO,EAAC,GAAG,EAAE,GAAG,EAAE,IAAI,EAAE,GAAG,EAAE,SAAS,EAAkB,IAAI,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAGjG,OAAO,EAAC,UAAU,EAAC,MAAM,UAAU,CAAC;AACpC,OAAO,EAAO,oBAAoB,EAAiB,MAAM,QAAQ,CAAC;AAClE,OAAO,KAAK,aAAa,MAAM,uBAAuB,CAAC;;;;;AAGvD,IAAY,qBAGX;AAHD,CAAA,SAAY,qBAAqB;IAC/B,qBAAA,CAAA,qBAAA,CAAA,SAAA,GAAA,EAAA,GAAA,QAAU,CAAA;IACV,qBAAA,CAAA,qBAAA,CAAA,UAAA,GAAA,EAAA,GAAA,SAAW,CAAA;AACb,CAAC,EAHW,qBAAqB,IAAA,CAArB,qBAAqB,GAAA,CAAA,CAAA,GAGhC;AAGM,MAAM,sBAAsB,GAAG,GAAG,CAAC;AA0BpC,MAAgB,YAAY;IAAlC,aAAA;QACE,iDAAiD;QACjD,IAAA,CAAA,cAAc,GAAoB,IAAI,CAAC;IAgCzC,CAAC;IA1BC,SAAS,CAAC,MAAc,EAAA;QACtB,IAAI,CAAC,MAAM,GAAG,MAAM,CAAC;IACvB,CAAC;IAED,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA,CAAG,CAAC;IAE3D,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA,CAAG,CAAC;IAEzD,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA,CAAG,CAAC;IAE3D,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA,CAAG,CAAC;IAEzD,KAAK,CAAC,YAAY,CAAC,IAAqB,EAAA,CAAG,CAAC;IAE5C,KAAK,CAAC,UAAU,CAAC,IAAqB,EAAA,CAAG,CAAC;IAE1C,4EAA4E;IAC5E,8EAA8E;IAC9E,8EAA8E;IAC9E,0EAA0E;IAC1E,8EAA8E;IAC9E,mEAAmE;IACnE,eAAe;IACf,QAAQ,CAAC,KAAgB,EAAA;IACvB,uEAAuE;IACzE,CAAC;CACF;AAKK,MAAO,YAAY;IAIvB,sEAAsE;IACtE,uCAAuC;IACvC,+BAA+B;IAC/B,4CAA4C;IAC5C,0CAA0C;IAE1C;;;;;OAKG,CACH,YAAY,SAA0B,EAAE,WAAW,GAAG,EAAE,CAAA;QACtD,2EAA2E;QAC3E,UAAU;QACV,IAAI,SAAS,IAAI,IAAI,EAAE;YACrB,SAAS,GAAG,EAAE,CAAC;SAChB;QACD,IAAI,CAAC,SAAS,GAAG,SAAS,CAAC;QAC3B,IAAI,CAAC,WAAW,GAAG,WAAW,CAAC;IACjC,CAAC;IAED,MAAM,CAAC,QAAsB,EAAA;QAC3B,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;IAChC,CAAC;IAED,SAAS,CAAC,MAAc,EAAA;QACtB,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,QAAQ,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC;SAC5B;IACH,CAAC;IAED,QAAQ,CAAC,KAAgB,EAAA;QACvB,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,QAAQ,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC;SAC1B;IACH,CAAC;IAED;;;;OAIG,CACH,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA;QACrD,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,YAAY,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;SAC1C;IACH,CAAC;IAED;;;;OAIG,CACH,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QACnD,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,UAAU,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;SACxC;IACH,CAAC;IAED;;;;OAIG,CACH,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA;QACrD,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,YAAY,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;SAC1C;IACH,CAAC;IAED;;;;OAIG,CACH,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QACnD,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,UAAU,CAAC,KAAK,EAAE,IAAI,CAAC,CAAC;SACxC;IACH,CAAC;IAED;;;OAGG,CACH,KAAK,CAAC,YAAY,CAAC,IAAqB,EAAA;QACtC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;SACnC;IACH,CAAC;IAED;;;OAGG,CACH,KAAK,CAAC,UAAU,CAAC,IAAqB,EAAA;QACpC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,KAAK,MAAM,QAAQ,IAAI,IAAI,CAAC,SAAS,CAAE;YACrC,MAAM,QAAQ,CAAC,UAAU,CAAC,IAAI,CAAC,CAAC;SACjC;IACH,CAAC;CACF;AAOK,MAAO,UAAW,SAAQ,YAAY;IAI1C,aAAA;QACE,KAAK,EAAE,CAAC;IACV,CAAC;IAEQ,KAAK,CAAC,YAAY,CAAC,KAAa,EAAA;QACvC,IAAI,CAAC,IAAI,GAAG,CAAC,CAAC;QACd,IAAI,CAAC,MAAM,GAAG,CAAA,CAAE,CAAC;IACnB,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QAC5D,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,MAAM,SAAS,GAAG,IAAI,CAAC,MAAM,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,MAAM,CAAW,CAAC;QACpE,IAAI,CAAC,IAAI,IAAI,SAAS,CAAC;QACvB,IAAK,MAAM,GAAG,IAAI,IAAI,CAAE;YACtB,MAAM,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC;YACxB,IAAI,OAAO,KAAK,KAAK,QAAQ,EAAE;gBAC7B,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,cAAc,CAAC,GAAG,CAAC,EAAE;oBACpC,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;iBACtB;gBACD,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,CAAW,GAAG,KAAK,GAAG,SAAS,CAAC;aACnE,MAAM;gBACL,IAAI,kBAA0B,CAAC;gBAC/B,IAAI,GAAG,IAAI,IAAI,CAAC,MAAM,EAAE;oBACtB,kBAAkB,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,CAAW,CAAC;iBACjD,MAAM;oBACL,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;iBACtB;gBACD,MAAM,KAAK,IACP,6KAAA,AAAI,EAAC,GAAG,EAAE,yKAAC,MAAA,AAAG,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,2KAAE,MAAA,AAAG,EAAC,KAAK,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;gBAC/D,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,KAAK,CAAC;gBACzB,IAAI,kBAAkB,IAAI,IAAI,EAAE;oBAC9B,kBAAkB,CAAC,OAAO,EAAE,CAAC;iBAC9B;aACF;SACF;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QAC5D,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,KAAK,MAAM,GAAG,IAAI,IAAI,CAAC,MAAM,CAAC,SAAS,CAAa,CAAE;gBACpD,IAAI,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,IAAI,IAAI,EAAE;oBAC5B,SAAS;iBACV;gBACD,IAAI,OAAO,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,KAAK,QAAQ,EAAE;oBACxC,IAAI,CAAC,GAAG,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,CAAW,GAAG,IAAI,CAAC,IAAI,CAAC;iBACpD,MAAM;qBACL,6KAAA,AAAI,EAAC,GAAG,EAAE;wBACR,MAAM,GAAG,6KAAW,MAAA,AAAG,4KAAC,MAAA,AAAG,EAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;wBAC7D,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,CAAC;wBACf,IAAI,CAAC,MAAM,CAAC,GAAG,CAAY,CAAC,OAAO,EAAE,CAAC;+LACvC,OAAA,AAAI,EAAC,IAAI,CAAC,GAAG,CAAW,CAAC,CAAC;oBAC5B,CAAC,CAAC,CAAC;iBACJ;aACF;SACF;IACH,CAAC;CACF;AAOK,MAAO,OAAQ,SAAQ,YAAY;IAI9B,KAAK,CAAC,YAAY,CAAC,IAAqB,EAAA;QAC/C,IAAI,CAAC,KAAK,GAAG,EAAE,CAAC;QAChB,IAAI,CAAC,OAAO,GAAG,CAAA,CAAE,CAAC;IACpB,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QAC5D,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;QACvB,IAAK,MAAM,GAAG,IAAI,IAAI,CAAE;YACtB,IAAI,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,IAAI,IAAI,EAAE;gBAC7B,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,GAAG,EAAE,CAAC;aACxB;YACD,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;SACnC;IACH,CAAC;IAED;;OAEG,CACH,KAAK,CAAC,QAAQ,GAAA;QACZ,MAAM,QAAQ,GAAuD,EAAE,CAAC;QACxE,MAAM,IAAI,GAAa,EAAE,CAAC;QAC1B,MAAM,OAAO,GAAa,EAAE,CAAC;QAC7B,IAAK,MAAM,GAAG,IAAI,IAAI,CAAC,OAAO,CAAE;YAC9B,MAAM,UAAU,GAAG,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;YACrC,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;gBAC1C,IAAI,OAAO,UAAU,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;oBACrC,MAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAW,CAAC;oBAC5C,QAAQ,CAAC,IAAI,CAAC,WAAW,CAAC,IAAI,EAAE,CAAC,CAAC;oBAClC,IAAI,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;oBACf,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;iBACjB;aACF;SACF;QACD,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC;QAC3C,IAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,EAAE,CAAC,CAAE;YACtC,MAAM,eAAe,GAAG,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAW,CAAC;YACpE,eAAe,CAAC,OAAO,EAAE,CAAC;YAC1B,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;SAClD;IACH,CAAC;CACF;AAkBK,MAAO,cAAe,SAAQ,YAAY;IAmB9C,YAAY,IAAwB,EAAE,UAA8B,CAAA;QAClE,KAAK,EAAE,CAAC;QALF,IAAA,CAAA,YAAY,GAAG,CAAC,CAAC;QAMvB,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,CAAC;QAC5B,IAAI,CAAC,aAAa,GAAG,IAAI,CAAC,aAAa,4KAAI,YAAS,CAAC;QACrD,IAAI,CAAC,UAAU,GAAG,UAAU,IAAI,MAAM,CAAC;QACvC,IAAI,IAAI,CAAC,UAAU,KAAK,MAAM,EAAE;YAC9B,IAAI,CAAC,UAAU,GAAG,sBAAsB,CAAC;SAC1C;QACD,IAAI,IAAI,CAAC,UAAU,KAAK,OAAO,IAAI,IAAI,CAAC,OAAO,IAAI,IAAI,EAAE;YACvD,MAAM,IAAI,KAAK,CACX,gEAAgE,GAChE,mDAAmD,CAAC,CAAC;SAC1D;QACD,oMAAI,OAAI,CAAC,QAAQ,CAAC,IAAI,CAAC,UAAU,CAAC,EAAE;YAClC,+DAA+D;YAC/D,mBAAmB;YACnB,IAAI,CAAC,SAAS,2LAAG,WAAsB,AAAR,EAC3B,AAD0B,CAAC,GACvB,CAAC,SAAS,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,IAAI,CAAC,UAAoB,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC;SACzE;QACD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,YAAY,CAAC;QACpC,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,UAAU,CAAC;QAChC,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,YAAY,CAAC;QACpC,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,UAAU,CAAC;QAChC,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,YAAY,CAAC;QACpC,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,UAAU,CAAC;QAChC,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,OAAO,CAAC;IAC5B,CAAC;IAED,KAAK,CAAC,SAAS,CAAC,KAAa,EAAE,KAAa,EAAE,IAAoB,EAAA;QAChE,MAAM,EAAE,GAA8B,EAAE,CAAC;QACzC,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,4KAAM,uBAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,KAAK,EAAE,KAAK,EAAE,IAAY,CAAC,CAAC,CAAC;SACjD;QACD,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,CAAC,CAAC;QAC9B,MAAM,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;IACxB,CAAC;IAEQ,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA;QAE9D,IAAI,CAAC,YAAY,GAAG,KAAK,CAAC;QAC1B,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,MAAM,6LAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,MAAM,IAAI,CAAC,UAAU,CAAC,KAAK,EAAE,IAAY,CAAC,CAAC;SAC5C;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QAE5D,MAAM,EAAE,GAA8B,EAAE,CAAC;QACzC,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;YACzB,4KAAM,uBAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,QAAQ,CAAC,KAAK,EAAE,IAAY,CAAC,CAAC,CAAC;SAC7C;QACD,IAAI,IAAI,CAAC,UAAU,KAAK,OAAO,EAAE;YAC/B,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,CAAC,CAAC;SAC/B;QACD,MAAM,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;IACxB,CAAC;IAEQ,KAAK,CAAC,YAAY,CAAC,KAAa,EAAE,IAAqB,EAAA;QAE9D,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,4KAAM,uBAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,MAAM,IAAI,CAAC,UAAU,CAAC,KAAK,EAAE,IAAY,CAAC,CAAC;SAC5C;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAqB,EAAA;QAE5D,MAAM,EAAE,GAA8B,EAAE,CAAC;QACzC,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;YACzB,MAAM,6LAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,QAAQ,CAAC,KAAK,EAAE,IAAY,CAAC,CAAC,CAAC;SAC7C;QACD,IAAI,IAAI,CAAC,UAAU,KAAK,OAAO,EAAE;YAC/B,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,CAAC,CAAC;SAC/B,MAAM,oMAAI,OAAI,CAAC,QAAQ,CAAC,IAAI,CAAC,UAAU,CAAC,EAAE;YACzC,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,YAAY,EAAE,KAAK,EAAE,IAAI,CAAC,CAAC,CAAC;SACzD;QACD,MAAM,OAAO,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;IACxB,CAAC;IAEQ,KAAK,CAAC,YAAY,CAAC,IAAqB,EAAA;QAC/C,IAAI,IAAI,CAAC,UAAU,IAAI,IAAI,EAAE;YAC3B,4KAAM,uBAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,MAAM,IAAI,CAAC,UAAU,CAAC,IAAY,CAAC,CAAC;SACrC;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,IAAqB,EAAA;QAC7C,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;YACzB,OAAM,4LAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;YACjC,MAAM,IAAI,CAAC,QAAQ,CAAC,IAAY,CAAC,CAAC;SACnC;IACH,CAAC;CACF;AAKK,SAAU,oBAAoB,CAChC,SACoB,EACpB,UAA6B;IAC/B,IAAI,SAAS,IAAI,IAAI,EAAE;QACrB,SAAS,GAAG,CAAA,CAAkB,CAAC;KAChC;IACD,IAAI,SAAS,YAAY,YAAY,EAAE;QACrC,OAAO;YAAC,SAAS;SAAC,CAAC;KACpB;IACD,IAAI,KAAK,CAAC,OAAO,CAAC,SAAS,CAAC,IAAI,SAAS,CAAC,CAAC,CAAC,YAAY,YAAY,EAAE;QACpE,OAAO,SAA2B,CAAC;KACpC;IACD,8DAA8D;IAC9D,MAAM,eAAe,GACjB,aAAa,CAAC,mLAAA,AAAM,EAClB,SAAS,CAAyB,CAAC;IACzC,OAAO,eAAe,CAAC,GAAG,EACtB,cAAc,CAAC,EAAE,AAAC,IAAI,cAAc,CAAC,cAAc,EAAE,UAAU,CAAC,CAAC,CAAC;AACxE,CAAC;AAMD;;;GAGG,CACH,MAAa,2BAA2B;IAItC;;OAEG,CACH,aAAA,CAAuB,CAAC;IAExB;;;;;;;;;;;OAWG,CACH,MAAM,CAAC,2BAA2B,CAC9B,cAAsB,EAAE,mBAA4C,EAAA;wMACtE,OAAI,CAAC,MAAM,CACP,cAAc,IAAI,CAAC,IAAI,MAAM,CAAC,SAAS,CAAC,cAAc,CAAC,EACvD,GAAG,CAAG,CAAA,AAAD,mDAAC,CAAqD,GACvD,CAAA,QAAA,EAAW,cAAc,EAAE,CAAC,CAAC;QACrC,2BAA2B,CAAC,iBAAiB,CAAC,mBAAmB,CAAC,CAAC;QACnE,IAAI,2BAA2B,CAAC,YAAY,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;YACpE,2BAA2B,CAAC,YAAY,CAAC,cAAc,CAAC,GAAG,EAAE,CAAC;SAC/D;QACD,2BAA2B,CAAC,YAAY,CAAC,cAAc,CAAC,CAAC,IAAI,CACzD,mBAAmB,CAAC,CAAC;IAC3B,CAAC;IAEO,MAAM,CAAC,iBAAiB,CAAC,mBAC2B,EAAA;QAC1D,IAAK,MAAM,SAAS,IAAI,2BAA2B,CAAC,YAAY,CAAE;YAChE,MAAM,YAAY,GAAG,2BAA2B,CAAC,YAAY,CAAC,CAAC,SAAS,CAAC,CAAC;YAC1E,YAAY,CAAC,OAAO,EAAC,IAAI,CAAC,EAAE;gBAC1B,IAAI,IAAI,KAAK,mBAAmB,EAAE;oBAChC,MAAM,wKAAI,aAAU,CAAC,iCAAiC,CAAC,CAAC;iBACzD;YACH,CAAC,CAAC,CAAC;SACJ;IACH,CAAC;IAED;;OAEG,CACO,MAAM,CAAC,KAAK,GAAA;QACpB,2BAA2B,CAAC,YAAY,GAAG,CAAA,CAAE,CAAC;IAChD,CAAC;IAED;;;;;;;OAOG,CACH,MAAM,CAAC,eAAe,CAAC,cAAsB,EAAA;QAC3C,MAAM,YAAY,GAA8B,EAAE,CAAC;QACnD,IAAK,MAAM,SAAS,IAAI,2BAA2B,CAAC,YAAY,CAAE;YAChE,MAAM,KAAK,GAAG,CAAC,SAAS,CAAC;YACzB,IAAI,cAAc,IAAI,KAAK,EAAE;gBAC3B,YAAY,CAAC,IAAI,CAAC,GAAG,2BAA2B,CAAC,YAAY,CAAC,KAAK,CAAC,CAAC,CAAC;aACvE;SACF;QACD,OAAO,YAAY,CAAC,GAAG,EAAC,IAAI,CAAC,EAAE,AAAC,IAAI,IAAI,EAAE,CAAC,CAAC;IAC9C,CAAC;;AAtEc,4BAAA,YAAY,GACiC,CAAA,CAAE,CAAC;;AAwE3D,SAAU,kBAAkB,CAC9B,SAAyB,EAAE,OAA8B,EAAE,MAAc,EACzE,YAAoB,EAAE,eAAuB,EAAE,aAAqB,EACpE,SAAiB,EAAE,YAAqB,EACxC,eAAyB;IAC3B,MAAM,OAAO,GAAG,IAAI,OAAO,EAAE,CAAC;IAC9B,MAAM,eAAe,GAAmB;QACtC,IAAI,UAAU,EAAE,EAAE;WAAG,2BAA2B,CAAC,eAAe,CAAC,OAAO,CAAC;KAC1E,CAAC;IACF,IAAI,SAAS,IAAI,IAAI,EAAE;QACrB,eAAe,CAAC,IAAI,CAAC,GAAG,SAAS,CAAC,CAAC;KACpC;IACD,eAAe,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;IAC9B,MAAM,YAAY,GAAG,IAAI,YAAY,CAAC,eAAe,CAAC,CAAC;IAEvD,mEAAmE;IACnE,cAAc;IACd,wDAAwD;IAExD,YAAY,CAAC,SAAS,CAAC;QACrB,MAAM;QACN,YAAY;QACZ,OAAO,EAAE,eAAe;QACxB,KAAK,EAAE,aAAa;QACpB,SAAS;QACT,OAAO;QACP,YAAY;QACZ,OAAO,EAAE,eAAe;KACzB,CAAC,CAAC;IACH,OAAO;QAAC,YAAY;QAAE,OAAO;IAAA,CAAC,CAAC;AACjC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1835, "column": 0}, "map": {"version":3,"file":"losses.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/losses.js/__/__/__/__/__/tfjs-layers/src/losses.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Tensor, Tensor1D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {epsilon} from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport {ValueError} from './errors';\nimport {LossOrMetricFn} from './types';\n\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nexport function l2Normalize(x: Tensor, axis?: number): Tensor {\n  return tidy(() => {\n    if (x.dtype !== 'float32') {\n      x = tfc.cast(x, 'float32');\n    }\n    const squareSum = tfc.sum(K.square(x), axis, true);\n    const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\n\nexport function meanSquaredError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsoluteError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsolutePercentageError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const diff = tfc.sub(yTrue, yPred);\n    const clippedTrue =\n        tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\n\nexport function meanSquaredLogarithmicError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    const firstLog = tfc.log(tfc.add(1, clippedPred));\n\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    const secondLog = tfc.log(tfc.add(1, clippedTrue));\n\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\n\nexport function squaredHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\n\nexport function hinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\n\nexport function categoricalHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nexport function logcosh(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const log2 = Math.log(2);\n    const predictionDiff = tfc.sub(yPred, yTrue);\n    const logcoshResult = tfc.sub(\n        tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))),\n        log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\n\nexport function categoricalCrossentropy(\n    target: Tensor, output: Tensor, fromLogits = false): Tensor {\n  return tidy(() => {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      const outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(\n        tfc.mul(tfc.cast(target, 'float32'), tfc.log(output)),\n        output.shape.length - 1));\n  });\n}\n\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nexport function sparseCategoricalCrossentropy(\n    target: Tensor, output: Tensor, fromLogits = false): Tensor {\n  return tidy(() => {\n    const flatTarget =\n        tfc.cast(tfc.floor(K.flatten(target)), 'int32') as Tensor1D;\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    const outputShape = output.shape;\n    const oneHotTarget = tfc.reshape(\n        tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]),\n        outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nexport function sigmoidCrossEntropyWithLogits(\n    labels: Tensor, logits: Tensor): Tensor {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(\n        `logits and labels must have the same shape, but got shapes ` +\n        `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n  }\n  return tidy(() => {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    const reluLogits = tfc.relu(logits);\n    const negAbsLogits = tfc.neg(tfc.abs(logits));\n    return tfc.add(\n        tfc.sub(reluLogits, tfc.mul(logits, labels)),\n        tfc.log1p(tfc.exp(negAbsLogits)));\n  });\n}\n\nexport function binaryCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    let y: Tensor;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\n\nexport function kullbackLeiblerDivergence(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(\n        tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\n\nexport function poisson(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\n\nexport function cosineProximity(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const trueNormalized = l2Normalize(yTrue, -1);\n    const predNormalized = l2Normalize(yPred, -1);\n    const trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\n\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity;\n\n// TODO(michaelterry): Add deserialize() function.\n\nexport const lossesMap: {[functionName: string]: LossOrMetricFn} = {\n  meanSquaredError,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  binaryCrossentropy,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity\n};\n\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function get(identifierOrFn: string|LossOrMetricFn): LossOrMetricFn {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n    let errMsg = `Unknown loss ${identifierOrFn}`;\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = `Unknown loss ${identifierOrFn}. ` +\n          'Use \"categoricalCrossentropy\" as the string name for ' +\n          'tf.losses.softmaxCrossEntropy';\n    }\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,8BAAA,EAAgC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAChC,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;AAC7C,OAAO,EAAmB,IAAI,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAEnE,OAAO,EAAC,OAAO,EAAC,MAAM,kBAAkB,CAAC;AACzC,OAAO,KAAK,CAAC,MAAM,wBAAwB,CAAC;AAC5C,OAAO,EAAC,UAAU,EAAC,MAAM,UAAU,CAAC;;;;;;AAQ9B,SAAU,WAAW,CAAC,CAAS,EAAE,IAAa;IAClD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,IAAI,CAAC,CAAC,KAAK,KAAK,SAAS,EAAE;YACzB,CAAC,GAAG,GAAG,CAAC,8KAAA,AAAI,EAAC,CAAC,EAAE,SAAS,CAAC,CAAC;SAC5B;QACD,MAAM,SAAS,GAAG,GAAG,CAAC,4KAAA,AAAG,2LAAC,CAAC,CAAC,OAAA,AAAM,EAAC,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;QACnD,MAAM,aAAa,OAAG,GAAG,CAAC,0KAAA,AAAI,EAAC,SAAS,CAAC,KAAK,qLAAE,UAAA,AAAO,EAAE,CAAC,CAAC;QAC3D,MAAM,IAAI,8KAAG,GAAG,CAAC,GAAI,AAAJ,gLAAK,GAAG,CAAC,MAAA,AAAO,EAAC,SAAS,EAAE,aAAa,CAAC,CAAC,CAAC;QAC7D,OAAO,GAAG,CAAC,4KAAG,AAAH,EAAI,CAAC,EAAE,IAAI,CAAC,CAAC;IAC1B,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,gBAAgB,CAAC,KAAa,EAAE,KAAa;IAC3D,8KAAO,OAAA,AAAI,EAAC,GAAG,CAAG,CAAD,EAAI,CAAC,8KAAA,AAAI,2LAAC,CAAC,CAAC,OAAM,AAAN,4KAAO,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;AACnE,CAAC;AAEK,SAAU,iBAAiB,CAAC,KAAa,EAAE,KAAa;IAC5D,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE,0KAAC,GAAG,CAAC,GAAI,AAAJ,4KAAK,GAAG,CAAC,EAAG,AAAH,4KAAI,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;AAClE,CAAC;AAEK,SAAU,2BAA2B,CACvC,KAAa,EAAE,KAAa;IAC9B,8KAAO,OAAI,AAAJ,EAAK,GAAG,EAAE;QACf,MAAM,IAAI,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QACnC,MAAM,WAAW,GACb,GAAG,CAAC,8LAAA,AAAW,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,CAAC,qLAAE,UAAA,AAAO,EAAE,GAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACjE,MAAM,SAAS,GAAG,GAAG,CAAC,4KAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,IAAI,EAAE,WAAW,CAAC,CAAC,CAAC;QACtD,iLAAO,GAAG,CAAC,EAAA,AAAG,EAAC,GAAG,EAAE,GAAG,CAAC,8KAAA,AAAI,EAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC/C,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,2BAA2B,CACvC,KAAa,EAAE,KAAa;IAC9B,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,WAAW,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,KAAK,EAAE,6LAAA,AAAO,EAAE,GAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACxE,MAAM,QAAQ,6KAAG,GAAG,CAAC,EAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,WAAW,CAAC,CAAC,CAAC;QAElD,MAAM,WAAW,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,KAAK,qLAAE,UAAA,AAAO,EAAE,GAAE,MAAM,CAAC,SAAS,CAAC,CAAC;QACxE,MAAM,SAAS,OAAG,GAAG,CAAC,wKAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,WAAW,CAAC,CAAC,CAAC;QAEnD,kLAAO,GAAG,CAAC,GAAA,AAAI,0LAAC,CAAC,CAAC,QAAA,AAAM,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC9D,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,YAAY,CAAC,KAAa,EAAE,KAAa;IACvD,WAAO,0KAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,SAAS,iLAAG,GAAG,CAAC,MAAA,AAAO,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,GAAG,CAAC,4KAAG,AAAH,EAAI,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACpE,kLAAO,GAAG,CAAC,GAAA,AAAI,GAAC,CAAC,CAAC,+LAAA,AAAM,EAAC,SAAS,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,KAAK,CAAC,KAAa,EAAE,KAAa;IAChD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,SAAS,GAAG,GAAG,CAAC,oLAAO,AAAP,EAAQ,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC;QACpE,kLAAO,GAAG,CAAC,GAAA,AAAI,EAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC;IACjC,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,gBAAgB,CAAC,KAAa,EAAE,KAAa;IAC3D,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,GAAG,6KAAG,GAAG,CAAC,EAAA,AAAG,MAAC,GAAG,CAAC,wKAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/C,MAAM,GAAG,6KAAG,GAAG,CAAC,EAAA,AAAG,MAAC,GAAG,CAAC,wKAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,KAAK,CAAC,EAAE,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC3D,oLAAO,GAAG,CAAC,OAAA,AAAO,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAG,AAAH,EAAI,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC,CAAC;IACvD,CAAC,CAAC,CAAC;AACL,CAAC;AAUK,SAAU,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,IAAI,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QACzB,MAAM,cAAc,OAAG,GAAG,CAAC,wKAAA,AAAG,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QAC7C,MAAM,aAAa,6KAAG,GAAG,CAAC,EAAA,AAAG,GACzB,GAAG,CAAC,2KAAA,AAAG,EAAC,cAAc,iLAAE,GAAG,CAAC,OAAA,AAAQ,4KAAC,GAAG,CAAC,EAAG,AAAH,EAAI,CAAC,CAAC,EAAE,cAAc,CAAC,CAAC,CAAC,EAClE,IAAI,CAAC,CAAC;QACV,iLAAO,GAAG,CAAC,IAAA,AAAI,EAAC,aAAa,EAAE,CAAC,CAAC,CAAC,CAAC;IACrC,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,uBAAuB,CACnC,MAAc,EAAE,MAAc,EAAE,UAAU,GAAG,KAAK;IACpD,OAAO,8KAAA,AAAI,EAAC,GAAG,EAAE;QACf,IAAI,UAAU,EAAE;YACd,MAAM,iLAAG,GAAG,CAAC,MAAA,AAAO,EAAC,MAAM,CAAC,CAAC;SAC9B,MAAM;YACL,uEAAuE;YACvE,MAAM,SAAS,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,MAAM,EAAE,MAAM,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE,IAAI,CAAC,CAAC;YACjE,MAAM,IAAG,GAAG,CAAC,2KAAA,AAAG,EAAC,MAAM,EAAE,SAAS,CAAC,CAAC;SACrC;QACD,MAAM,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,MAAM,MAAE,yLAAA,AAAO,EAAE,GAAE,CAAC,sLAAG,UAAA,AAAO,EAAE,CAAC,CAAC;QAC3D,iLAAO,GAAG,CAAC,EAAG,AAAH,GAAI,GAAG,CAAC,2KAAA,AAAG,4KAClB,GAAG,CAAC,EAAA,AAAG,GAAC,GAAG,CAAC,6KAAA,AAAI,EAAC,MAAM,EAAE,SAAS,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,MAAM,CAAC,CAAC,EACrD,MAAM,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;IAChC,CAAC,CAAC,CAAC;AACL,CAAC;AAWK,SAAU,6BAA6B,CACzC,MAAc,EAAE,MAAc,EAAE,UAAU,GAAG,KAAK;IACpD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,UAAU,8KACZ,GAAG,CAAC,GAAA,AAAI,8KAAC,GAAG,CAAC,IAAA,AAAK,2LAAC,CAAC,CAAC,QAAA,AAAO,EAAC,MAAM,CAAC,CAAC,EAAE,OAAO,CAAa,CAAC;QAChE,MAAM,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,MAAM,qLAAE,UAAA,AAAO,EAAE,GAAE,CAAC,qLAAG,WAAA,AAAO,EAAE,CAAC,CAAC;QAC3D,MAAM,WAAW,GAAG,MAAM,CAAC,KAAK,CAAC;QACjC,MAAM,YAAY,iLAAG,GAAG,CAAC,MAAA,AAAO,gLAC5B,GAAG,CAAC,KAAA,AAAM,EAAC,UAAU,EAAE,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,EAC3D,WAAW,CAAC,CAAC;QACjB,OAAO,uBAAuB,CAAC,YAAY,EAAE,MAAM,EAAE,UAAU,CAAC,CAAC;IACnE,CAAC,CAAC,CAAC;AACL,CAAC;AAuBK,SAAU,6BAA6B,CACzC,MAAc,EAAE,MAAc;IAChC,IAAI,iMAAC,OAAI,CAAC,WAAW,CAAC,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,CAAC,EAAE;QACjD,MAAM,wKAAI,aAAU,CAChB,CAAA,2DAAA,CAA6D,GAC7D,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAA,KAAA,EAAQ,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC;KAC5E;IACD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,0CAA0C;QAC1C,iCAAiC;QACjC,kDAAkD;QAClD,6BAA6B;QAC7B,sEAAsE;QACtE,8CAA8C;QAC9C,MAAM,UAAU,8KAAG,GAAG,CAAC,GAAA,AAAI,EAAC,MAAM,CAAC,CAAC;QACpC,MAAM,YAAY,6KAAG,GAAG,CAAC,EAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,MAAM,CAAC,CAAC,CAAC;QAC9C,QAAO,GAAG,CAAC,2KAAA,AAAG,4KACV,GAAG,CAAC,EAAA,AAAG,EAAC,UAAU,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,MAAM,EAAE,MAAM,CAAC,CAAC,6KAC5C,GAAG,CAAC,KAAA,AAAK,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,YAAY,CAAC,CAAC,CAAC,CAAC;IACxC,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,kBAAkB,CAAC,KAAa,EAAE,KAAa;IAC7D,WAAO,0KAAA,AAAI,EAAC,GAAG,EAAE;QACf,IAAI,CAAS,CAAC;QACd,CAAC,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,KAAK,MAAE,yLAAA,AAAO,EAAE,GAAE,CAAC,sLAAG,UAAA,AAAO,EAAE,CAAC,CAAC;QACrD,CAAC,6KAAG,GAAG,CAAC,EAAA,AAAG,GAAC,GAAG,CAAC,2KAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvC,WAAO,GAAG,CAAC,0KAAA,AAAI,EAAC,6BAA6B,CAAC,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC/D,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,yBAAyB,CACrC,KAAa,EAAE,KAAa;IAC9B,QAAO,6KAAI,AAAJ,EAAK,GAAG,EAAE;QACf,MAAM,WAAW,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,KAAK,GAAE,4LAAA,AAAO,EAAE,GAAE,CAAC,CAAC,CAAC;QACzD,MAAM,WAAW,uLAAG,GAAG,CAAC,UAAA,AAAW,EAAC,KAAK,GAAE,4LAAA,AAAO,EAAE,GAAE,CAAC,CAAC,CAAC;QACzD,iLAAO,GAAG,CAAC,EAAA,AAAG,4KACV,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,2KAAE,GAAG,CAAC,GAAA,AAAG,4KAAC,GAAG,CAAC,EAAG,AAAH,EAAI,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACtE,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,OAAO,6KAAG,GAAG,CAAC,EAAA,AAAG,GAAC,GAAG,CAAC,2KAAA,AAAG,qLAAC,UAAA,AAAO,EAAE,GAAE,KAAK,CAAC,CAAC,CAAC;QACnD,QAAO,GAAG,CAAC,6KAAA,AAAI,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,KAAK,MAAE,GAAG,CAAC,wKAAA,AAAG,EAAC,KAAK,EAAE,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC/D,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,eAAe,CAAC,KAAa,EAAE,KAAa;IAC1D,QAAO,6KAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,cAAc,GAAG,WAAW,CAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,cAAc,GAAG,WAAW,CAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9C,MAAM,SAAS,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,cAAc,EAAE,cAAc,CAAC,CAAC;QAC1D,iLAAO,GAAG,CAAC,EAAA,AAAG,EAAC,GAAG,CAAC,4KAAA,AAAG,EAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzC,CAAC,CAAC,CAAC;AACL,CAAC;AAEM,MAAM,GAAG,GAAG,gBAAgB,CAAC;AAC7B,MAAM,GAAG,GAAG,gBAAgB,CAAC;AAC7B,MAAM,GAAG,GAAG,iBAAiB,CAAC;AAC9B,MAAM,GAAG,GAAG,iBAAiB,CAAC;AAC9B,MAAM,IAAI,GAAG,2BAA2B,CAAC;AACzC,MAAM,IAAI,GAAG,2BAA2B,CAAC;AACzC,MAAM,IAAI,GAAG,2BAA2B,CAAC;AACzC,MAAM,IAAI,GAAG,2BAA2B,CAAC;AACzC,MAAM,GAAG,GAAG,yBAAyB,CAAC;AACtC,MAAM,GAAG,GAAG,yBAAyB,CAAC;AACtC,MAAM,MAAM,GAAG,eAAe,CAAC;AAI/B,MAAM,SAAS,GAA6C;IACjE,gBAAgB;IAChB,iBAAiB;IACjB,2BAA2B;IAC3B,2BAA2B;IAC3B,YAAY;IACZ,KAAK;IACL,gBAAgB;IAChB,OAAO;IACP,uBAAuB;IACvB,6BAA6B;IAC7B,kBAAkB;IAClB,yBAAyB;IACzB,OAAO;IACP,eAAe;CAChB,CAAC;AAII,SAAU,GAAG,CAAC,cAAqC;IACvD,IAAI,OAAO,cAAc,KAAK,QAAQ,EAAE;QACtC,IAAI,cAAc,IAAI,SAAS,EAAE;YAC/B,OAAO,SAAS,CAAC,cAAc,CAAC,CAAC;SAClC;QACD,IAAI,MAAM,GAAG,CAAA,aAAA,EAAgB,cAAc,EAAE,CAAC;QAC9C,IAAI,cAAc,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,qBAAqB,CAAC,EAAE;YAChE,MAAM,GAAG,CAAA,aAAA,EAAgB,cAAc,CAAA,EAAA,CAAI,GACvC,uDAAuD,GACvD,+BAA+B,CAAC;SACrC;QACD,MAAM,wKAAI,aAAU,CAAC,MAAM,CAAC,CAAC;KAC9B,MAAM;QACL,OAAO,cAAc,CAAC;KACvB;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2083, "column": 0}, "map": {"version":3,"file":"metrics.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/metrics.js/__/__/__/__/__/tfjs-layers/src/metrics.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Built-in metrics.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Tensor, tidy} from '@tensorflow/tfjs-core';\n\nimport * as K from './backend/tfjs_backend';\nimport {NotImplementedError, ValueError} from './errors';\nimport {binaryCrossentropy as lossBinaryCrossentropy, categoricalCrossentropy as categoricalCrossentropyLoss, cosineProximity, lossesMap, meanAbsoluteError, meanAbsolutePercentageError, meanSquaredError, sparseCategoricalCrossentropy as sparseCategoricalCrossentropyLoss} from './losses';\nimport {LossOrMetricFn} from './types';\nimport * as util from './utils/generic_utils';\n\nexport function binaryAccuracy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const threshold = tfc.mul(.5, tfc.onesLike(yPred));\n    const yPredThresholded = K.cast(tfc.greater(yPred, threshold), yTrue.dtype);\n    return tfc.mean(tfc.equal(yTrue, yPredThresholded), -1);\n  });\n}\n\nexport function categoricalAccuracy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(\n      () => K.cast(\n          tfc.equal(tfc.argMax(yTrue, -1), tfc.argMax(yPred, -1)), 'float32'));\n}\n\nfunction truePositives(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    return tfc.cast(\n        tfc.sum(tfc.logicalAnd(tfc.equal(yTrue, 1), tfc.equal(yPred, 1))),\n        'float32');\n  });\n}\n\nfunction falseNegatives(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    return tfc.cast(\n        tfc.sum(tfc.logicalAnd(tfc.equal(yTrue, 1), tfc.equal(yPred, 0))),\n        'float32');\n  });\n}\n\nfunction falsePositives(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    return tfc.cast(\n        tfc.sum(tfc.logicalAnd(tfc.equal(yTrue, 0), tfc.equal(yPred, 1))),\n        'float32');\n  });\n}\n\nexport function precision(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const tp = truePositives(yTrue, yPred);\n    const fp = falsePositives(yTrue, yPred);\n\n    const denominator = tfc.add(tp, fp);\n\n    return tfc.cast(\n        tfc.where(tfc.greater(denominator, 0), tfc.div(tp, denominator), 0),\n        'float32');\n  });\n}\n\nexport function recall(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const tp = truePositives(yTrue, yPred);\n    const fn = falseNegatives(yTrue, yPred);\n\n    const denominator = tfc.add(tp, fn);\n\n    return tfc.cast(\n        tfc.where(tfc.greater(denominator, 0), tfc.div(tp, denominator), 0),\n        'float32');\n  });\n}\n\nexport function binaryCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return lossBinaryCrossentropy(yTrue, yPred);\n}\n\nexport function sparseCategoricalAccuracy(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  if (yTrue.rank === yPred.rank) {\n    yTrue = tfc.squeeze(yTrue, [yTrue.rank - 1]);\n  }\n  yPred = tfc.argMax(yPred, -1);\n  if (yPred.dtype !== yTrue.dtype) {\n    yPred = tfc.cast(yPred, yTrue.dtype);\n  }\n  return tfc.cast(tfc.equal(yTrue, yPred), 'float32');\n}\n\nexport function topKCategoricalAccuracy(yTrue: Tensor, yPred: Tensor): Tensor {\n  throw new NotImplementedError();\n}\n\nexport function sparseTopKCategoricalAccuracy(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  throw new NotImplementedError();\n}\n\nexport function r2Score(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const sumSquaresResiduals = yTrue.sub(yPred).square().sum();\n    const sumSquares = yTrue.sub(yTrue.mean()).square().sum();\n    return tfc.scalar(1).sub(sumSquaresResiduals.div(sumSquares));\n  });\n}\n\n// Aliases.\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const categoricalCrossentropy = categoricalCrossentropyLoss;\nexport const cosine = cosineProximity;\nexport const sparseCategoricalCrossentropy = sparseCategoricalCrossentropyLoss;\n\n// TODO(cais, nielsene): Add serialize().\n\nexport const metricsMap: {[functionName: string]: LossOrMetricFn} = {\n  binaryAccuracy,\n  categoricalAccuracy,\n  precision,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  mse,\n  MSE,\n  mae,\n  MAE,\n  mape,\n  MAPE,\n  cosine\n};\n\nexport function get(identifier: string|LossOrMetricFn): LossOrMetricFn {\n  if (typeof identifier === 'string' && identifier in metricsMap) {\n    return metricsMap[identifier];\n  } else if (typeof identifier !== 'string' && identifier != null) {\n    return identifier;\n  } else {\n    throw new ValueError(`Unknown metric ${identifier}`);\n  }\n}\n\n/**\n * Get the shortcut function name.\n *\n * If the fn name is a string,\n *   directly return the string name.\n * If the function is included in metricsMap or lossesMap,\n *   return key of the map.\n *   - If the function relative to multiple keys,\n *     return the first found key as the function name.\n *   - If the function exists in both lossesMap and metricsMap,\n *     search lossesMap first.\n * If the function is not included in metricsMap or lossesMap,\n *   return the function name.\n *\n * @param fn loss function, metric function, or short cut name.\n * @returns Loss or Metric name in string.\n */\nexport function getLossOrMetricName(fn: string|LossOrMetricFn): string {\n  util.assert(fn !== null, `Unknown LossOrMetricFn ${fn}`);\n  if (typeof fn === 'string') {\n    return fn;\n  } else {\n    let fnName;\n    for (const key of Object.keys(lossesMap)) {\n      if (lossesMap[key] === fn) {\n        fnName = key;\n        break;\n      }\n    }\n    if (fnName !== undefined) {\n      return fnName;\n    }\n    for (const key of Object.keys(metricsMap)) {\n      if (metricsMap[key] === fn) {\n        fnName = key;\n        break;\n      }\n    }\n    if (fnName !== undefined) {\n      return fnName;\n    }\n    return (fn as Function).name;\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;;;;;;;;;;;;;;;;;;;;;AAEH,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;;;;;;;;;AAC7C,OAAO,EAAS,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAEnD,OAAO,KAAK,CAAC,MAAM,wBAAwB,CAAC;AAC5C,OAAO,EAAC,mBAAmB,EAAE,UAAU,EAAC,MAAM,UAAU,CAAC;AACzD,OAAO,EAAC,kBAAkB,IAAI,sBAAsB,EAAE,uBAAuB,IAAI,2BAA2B,EAAE,eAAe,EAAE,SAAS,EAAE,iBAAiB,EAAE,2BAA2B,EAAE,gBAAgB,EAAE,6BAA6B,IAAI,iCAAiC,EAAC,MAAM,UAAU,CAAC;AAEhS,OAAO,KAAK,IAAI,MAAM,uBAAuB,CAAC;;;;;;;AAExC,SAAU,cAAc,CAAC,KAAa,EAAE,KAAa;IACzD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,SAAS,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,EAAE,kLAAE,GAAG,CAAC,OAAA,AAAQ,EAAC,KAAK,CAAC,CAAC,CAAC;QACnD,MAAM,gBAAgB,4LAAG,CAAC,CAAC,KAAA,AAAI,gLAAC,GAAG,CAAC,MAAA,AAAO,EAAC,KAAK,EAAE,SAAS,CAAC,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC;QAC5E,iLAAO,GAAG,CAAC,IAAA,AAAI,6KAAC,GAAG,CAAC,KAAA,AAAK,EAAC,KAAK,EAAE,gBAAgB,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,mBAAmB,CAAC,KAAa,EAAE,KAAa;IAC9D,8KAAO,OAAA,AAAI,EACP,GAAG,EAAE,wLAAC,CAAC,CAAC,KAAA,AAAI,MACR,GAAG,CAAC,4KAAA,AAAK,gLAAC,GAAG,CAAC,KAAA,AAAM,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,gLAAE,GAAG,CAAC,KAAA,AAAM,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC,CAAC;AAC/E,CAAC;AAED,SAAS,aAAa,CAAC,KAAa,EAAE,KAAa;IACjD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,kLAAO,GAAG,CAAC,GAAA,AAAI,4KACX,GAAG,CAAC,EAAA,AAAG,mLAAC,GAAG,CAAC,UAAA,AAAU,8KAAC,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,8KAAE,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC,EACjE,SAAS,CAAC,CAAC;IACjB,CAAC,CAAC,CAAC;AACL,CAAC;AAED,SAAS,cAAc,CAAC,KAAa,EAAE,KAAa;IAClD,OAAO,8KAAA,AAAI,EAAC,GAAG,EAAE;QACf,kLAAO,GAAG,CAAC,GAAA,AAAI,4KACX,GAAG,CAAC,EAAA,AAAG,EAAC,GAAG,CAAC,2LAAA,AAAU,8KAAC,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,8KAAE,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC,EACjE,SAAS,CAAC,CAAC;IACjB,CAAC,CAAC,CAAC;AACL,CAAC;AAED,SAAS,cAAc,CAAC,KAAa,EAAE,KAAa;IAClD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,kLAAO,GAAG,CAAC,GAAA,AAAI,4KACX,GAAG,CAAC,EAAA,AAAG,mLAAC,GAAG,CAAC,UAAA,AAAU,8KAAC,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,8KAAE,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC,EACjE,SAAS,CAAC,CAAC;IACjB,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,SAAS,CAAC,KAAa,EAAE,KAAa;IACpD,OAAO,8KAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,EAAE,GAAG,aAAa,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QACvC,MAAM,EAAE,GAAG,cAAc,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QAExC,MAAM,WAAW,OAAG,GAAG,CAAC,wKAAA,AAAG,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;QAEpC,kLAAO,GAAG,CAAC,GAAA,AAAI,8KACX,GAAG,CAAC,IAAA,AAAK,gLAAC,GAAG,CAAC,MAAA,AAAO,EAAC,WAAW,EAAE,CAAC,CAAC,EAAE,GAAG,CAAC,4KAAA,AAAG,EAAC,EAAE,EAAE,WAAW,CAAC,EAAE,CAAC,CAAC,EACnE,SAAS,CAAC,CAAC;IACjB,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,MAAM,CAAC,KAAa,EAAE,KAAa;IACjD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,EAAE,GAAG,aAAa,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QACvC,MAAM,EAAE,GAAG,cAAc,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;QAExC,MAAM,WAAW,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,EAAE,EAAE,EAAE,CAAC,CAAC;QAEpC,OAAO,GAAG,CAAC,8KAAA,AAAI,8KACX,GAAG,CAAC,IAAA,AAAK,gLAAC,GAAG,CAAC,MAAA,AAAO,EAAC,WAAW,EAAE,CAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,EAAC,EAAE,EAAE,WAAW,CAAC,EAAE,CAAC,CAAC,EACnE,SAAS,CAAC,CAAC;IACjB,CAAC,CAAC,CAAC;AACL,CAAC;AAEK,SAAU,kBAAkB,CAAC,KAAa,EAAE,KAAa;IAC7D,QAAO,4LAAA,AAAsB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC9C,CAAC;AAEK,SAAU,yBAAyB,CACrC,KAAa,EAAE,KAAa;IAC9B,IAAI,KAAK,CAAC,IAAI,KAAK,KAAK,CAAC,IAAI,EAAE;QAC7B,KAAK,iLAAG,GAAG,CAAC,MAAA,AAAO,EAAC,KAAK,EAAE;YAAC,KAAK,CAAC,IAAI,GAAG,CAAC;SAAC,CAAC,CAAC;KAC9C;IACD,KAAK,iLAAG,GAAG,CAAC,KAAA,AAAM,EAAC,KAAK,EAAE,CAAC,CAAC,CAAC,CAAC;IAC9B,IAAI,KAAK,CAAC,KAAK,KAAK,KAAK,CAAC,KAAK,EAAE;QAC/B,KAAK,OAAG,GAAG,CAAC,0KAAA,AAAI,EAAC,KAAK,EAAE,KAAK,CAAC,KAAK,CAAC,CAAC;KACtC;IACD,kLAAO,GAAG,CAAC,GAAA,AAAI,8KAAC,GAAG,CAAC,IAAA,AAAK,EAAC,KAAK,EAAE,KAAK,CAAC,EAAE,SAAS,CAAC,CAAC;AACtD,CAAC;AAEK,SAAU,uBAAuB,CAAC,KAAa,EAAE,KAAa;IAClE,MAAM,wKAAI,sBAAmB,EAAE,CAAC;AAClC,CAAC;AAEK,SAAU,6BAA6B,CACzC,KAAa,EAAE,KAAa;IAC9B,MAAM,wKAAI,sBAAmB,EAAE,CAAC;AAClC,CAAC;AAEK,SAAU,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;QACf,MAAM,mBAAmB,GAAG,KAAK,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAC5D,MAAM,UAAU,GAAG,KAAK,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAC1D,oLAAO,GAAG,CAAC,KAAA,AAAM,EAAC,CAAC,CAAC,CAAC,GAAG,CAAC,mBAAmB,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC;IAChE,CAAC,CAAC,CAAC;AACL,CAAC;AAGM,MAAM,GAAG,uKAAG,mBAAgB,CAAC;AAC7B,MAAM,GAAG,uKAAG,mBAAgB,CAAC;AAC7B,MAAM,GAAG,uKAAG,oBAAiB,CAAC;AAC9B,MAAM,GAAG,GAAG,wLAAiB,CAAC;AAC9B,MAAM,IAAI,uKAAG,8BAA2B,CAAC;AACzC,MAAM,IAAI,uKAAG,8BAA2B,CAAC;AACzC,MAAM,uBAAuB,sKAAG,2BAA2B,CAAC;AAC5D,MAAM,MAAM,uKAAG,kBAAe,CAAC;AAC/B,MAAM,6BAA6B,uKAAG,gCAAiC,CAAC;AAIxE,MAAM,UAAU,GAA6C;IAClE,cAAc;IACd,mBAAmB;IACnB,SAAS;IACT,uBAAuB;IACvB,6BAA6B;IAC7B,GAAG;IACH,GAAG;IACH,GAAG;IACH,GAAG;IACH,IAAI;IACJ,IAAI;IACJ,MAAM;CACP,CAAC;AAEI,SAAU,GAAG,CAAC,UAAiC;IACnD,IAAI,OAAO,UAAU,KAAK,QAAQ,IAAI,UAAU,IAAI,UAAU,EAAE;QAC9D,OAAO,UAAU,CAAC,UAAU,CAAC,CAAC;KAC/B,MAAM,IAAI,OAAO,UAAU,KAAK,QAAQ,IAAI,UAAU,IAAI,IAAI,EAAE;QAC/D,OAAO,UAAU,CAAC;KACnB,MAAM;QACL,MAAM,wKAAI,aAAU,CAAC,CAAA,eAAA,EAAkB,UAAU,EAAE,CAAC,CAAC;KACtD;AACH,CAAC;AAmBK,SAAU,mBAAmB,CAAC,EAAyB;4LAC3D,IAAI,CAAC,IAAA,AAAM,EAAC,EAAE,KAAK,IAAI,EAAE,CAAA,uBAAA,EAA0B,EAAE,EAAE,CAAC,CAAC;IACzD,IAAI,OAAO,EAAE,KAAK,QAAQ,EAAE;QAC1B,OAAO,EAAE,CAAC;KACX,MAAM;QACL,IAAI,MAAM,CAAC;QACX,KAAK,MAAM,GAAG,IAAI,MAAM,CAAC,IAAI,qKAAC,YAAS,CAAC,CAAE;YACxC,IAAI,gLAAS,CAAC,GAAG,CAAC,KAAK,EAAE,EAAE;gBACzB,MAAM,GAAG,GAAG,CAAC;gBACb,MAAM;aACP;SACF;QACD,IAAI,MAAM,KAAK,SAAS,EAAE;YACxB,OAAO,MAAM,CAAC;SACf;QACD,KAAK,MAAM,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,CAAE;YACzC,IAAI,UAAU,CAAC,GAAG,CAAC,KAAK,EAAE,EAAE;gBAC1B,MAAM,GAAG,GAAG,CAAC;gBACb,MAAM;aACP;SACF;QACD,IAAI,MAAM,KAAK,SAAS,EAAE;YACxB,OAAO,MAAM,CAAC;SACf;QACD,OAAQ,EAAe,CAAC,IAAI,CAAC;KAC9B;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2276, "column": 0}, "map": {"version":3,"file":"optimizers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/optimizers.js/__/__/__/__/__/tfjs-layers/src/optimizers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Optimizers.\n */\n\nimport {Optimizer, train} from '@tensorflow/tfjs-core';\n\nimport {epsilon} from './backend/common';\n\nimport {ValueError} from './errors';\n\n// Add (de)serialize()\n\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function getOptimizer(identifier: string): Optimizer {\n  const optimizerMap: {[optimizerName: string]: () => Optimizer} = {\n    'Adagrad': () => train.adagrad(0.01),\n    'Adadelta': () => train.adadelta(1, 0.95, epsilon()),\n    'Adam': () => train.adam(0.001, 0.9, 0.999, epsilon()),\n    'Adamax': () => train.adamax(0.002, 0.9, 0.999, epsilon(), 0),\n    'RMSProp': () => train.rmsprop(0.001, 0.9, 0, epsilon()),\n    'SGD': () => train.sgd(0.01)\n  };\n  optimizerMap['adagrad'] = optimizerMap['Adagrad'];\n  optimizerMap['adadelta'] = optimizerMap['Adadelta'];\n  optimizerMap['adam'] = optimizerMap['Adam'];\n  optimizerMap['adamax'] = optimizerMap['Adamax'];\n  optimizerMap['rmsprop'] = optimizerMap['RMSProp'];\n  optimizerMap['sgd'] = optimizerMap['SGD'];\n\n  if (identifier in optimizerMap) {\n    return optimizerMap[identifier]();\n  }\n  throw new ValueError(`Unknown Optimizer ${identifier}`);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;;AAEH,OAAO,EAAY,KAAK,EAAC,MAAM,uBAAuB,CAAC;AAEvD,OAAO,EAAC,OAAO,EAAC,MAAM,kBAAkB,CAAC;AAEzC,OAAO,EAAC,UAAU,EAAC,MAAM,UAAU,CAAC;;;;AAM9B,SAAU,YAAY,CAAC,UAAkB;IAC7C,MAAM,YAAY,GAA+C;QAC/D,SAAS,EAAE,GAAG,EAAE,gKAAC,QAAK,CAAC,OAAO,CAAC,IAAI,CAAC;QACpC,UAAU,EAAE,GAAG,EAAE,gKAAC,QAAK,CAAC,QAAQ,CAAC,CAAC,EAAE,IAAI,qLAAE,UAAA,AAAO,EAAE,CAAC;QACpD,MAAM,EAAE,GAAG,EAAE,gKAAC,QAAK,CAAC,IAAI,CAAC,KAAK,EAAE,GAAG,EAAE,KAAK,GAAE,4LAAA,AAAO,EAAE,CAAC;QACtD,QAAQ,EAAE,GAAG,EAAE,gKAAC,QAAK,CAAC,MAAM,CAAC,KAAK,EAAE,GAAG,EAAE,KAAK,GAAE,4LAAA,AAAO,EAAE,GAAE,CAAC,CAAC;QAC7D,SAAS,EAAE,GAAG,EAAE,gKAAC,QAAK,CAAC,OAAO,CAAC,KAAK,EAAE,GAAG,EAAE,CAAC,qLAAE,UAAA,AAAO,EAAE,CAAC;QACxD,KAAK,EAAE,GAAG,CAAG,CAAD,wKAAM,CAAC,GAAG,CAAC,IAAI,CAAC;KAC7B,CAAC;IACF,YAAY,CAAC,SAAS,CAAC,GAAG,YAAY,CAAC,SAAS,CAAC,CAAC;IAClD,YAAY,CAAC,UAAU,CAAC,GAAG,YAAY,CAAC,UAAU,CAAC,CAAC;IACpD,YAAY,CAAC,MAAM,CAAC,GAAG,YAAY,CAAC,MAAM,CAAC,CAAC;IAC5C,YAAY,CAAC,QAAQ,CAAC,GAAG,YAAY,CAAC,QAAQ,CAAC,CAAC;IAChD,YAAY,CAAC,SAAS,CAAC,GAAG,YAAY,CAAC,SAAS,CAAC,CAAC;IAClD,YAAY,CAAC,KAAK,CAAC,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC;IAE1C,IAAI,UAAU,IAAI,YAAY,EAAE;QAC9B,OAAO,YAAY,CAAC,UAAU,CAAC,EAAE,CAAC;KACnC;IACD,MAAM,wKAAI,aAAU,CAAC,CAAA,kBAAA,EAAqB,UAAU,EAAE,CAAC,CAAC;AAC1D,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2322, "column": 0}, "map": {"version":3,"file":"user_defined_metadata.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/user_defined_metadata.js/__/__/__/__/__/tfjs-layers/src/user_defined_metadata.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/** Utility functions related to user-defined metadata. */\n\n// Maximum recommended serialized size for user-defined metadata.\n// Beyond this limit, a warning message will be printed during model loading and\n// saving.\nexport const MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH = 1 * 1024 * 1024;\n\n/**\n * Check validity of user-defined metadata.\n *\n * @param userDefinedMetadata\n * @param modelName Name of the model that the user-defined metadata belongs to.\n *   Used during construction of error messages.\n * @param checkSize Whether to check the size of the metadata is under\n *   recommended limit. Default: `false`. If `true`, will try stringify the\n *   JSON object and print a console warning if the serialzied size is above the\n *   limit.\n * @throws Error if `userDefinedMetadata` is not a plain JSON object.\n */\nexport function checkUserDefinedMetadata(\n    userDefinedMetadata: {}, modelName: string, checkSize = false): void {\n  if (userDefinedMetadata == null ||\n      typeof userDefinedMetadata !== 'object' ||\n      Object.getPrototypeOf(userDefinedMetadata) !== Object.prototype ||\n      !plainObjectCheck(userDefinedMetadata)) {\n    throw new Error(\n        'User-defined metadata is expected to be a JSON object, but is not.');\n  }\n\n  if (checkSize) {\n    const out = JSON.stringify(userDefinedMetadata);\n    if (out.length > MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH) {\n      console.warn(\n          `User-defined metadata of model \"${modelName}\" is too large in ` +\n          `size (length=${out.length} when serialized). It is not ` +\n          `recommended to store such large objects in user-defined metadata. ` +\n          `Please make sure its serialized length is <= ` +\n          `${MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH}.`);\n    }\n  }\n}\n\n/**\n * Check if an input is plain JSON object or any valid subfield of it.\n *\n * @param x The input to be checked.\n * @param assertObject Whether to assert `x` is a JSON object, i.e., reject\n *   cases of arrays and primitives.\n * @return Returns `true` if and only if `x` is a plain JSON object,\n *   a JSON-valid primitive including string, number, boolean and null,\n *   or an array of the said types.\n */\n// tslint:disable-next-line:no-any\nexport function plainObjectCheck(x: any): boolean {\n  if (x === null) {\n    // Note: typeof `null` is 'object', and `null` is valid in JSON.\n    return true;\n  } else if (typeof x === 'object') {\n    if (Object.getPrototypeOf(x) === Object.prototype) {\n      // `x` is a JavaScript object and its prototype is Object.\n      const keys = Object.keys(x);\n      for (const key of keys) {\n        if (typeof key !== 'string') {\n          // JSON keys must be strings.\n          return false;\n        }\n        if (!plainObjectCheck(x[key])) {  // Recursive call.\n          return false;\n        }\n      }\n      return true;\n    } else {\n      // `x` is a JavaScript object but its prototype is not Object.\n      if (Array.isArray(x)) {\n        // `x` is a JavaScript array.\n        for (const item of x) {\n          if (!plainObjectCheck(item)) {  // Recursive call.\n            return false;\n          }\n        }\n        return true;\n      } else {\n        // `x` is a JavaScript object and its prototype is not Object,\n        // and it's not an Array. I.e., it's a complex object such as\n        // `Error` and `Date`.\n        return false;\n      }\n    }\n  } else {\n    // `x` is not a JavaScript object or `null`.\n    const xType = typeof x;\n    return xType === 'string' || xType === 'number' || xType === 'boolean';\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,wDAAA,EAA0D,CAE1D,iEAAiE;AACjE,gFAAgF;AAChF,UAAU;;;;;;AACH,MAAM,2CAA2C,GAAG,CAAC,GAAG,IAAI,GAAG,IAAI,CAAC;AAcrE,SAAU,wBAAwB,CACpC,mBAAuB,EAAE,SAAiB,EAAE,SAAS,GAAG,KAAK;IAC/D,IAAI,mBAAmB,IAAI,IAAI,IAC3B,OAAO,mBAAmB,KAAK,QAAQ,IACvC,MAAM,CAAC,cAAc,CAAC,mBAAmB,CAAC,KAAK,MAAM,CAAC,SAAS,IAC/D,CAAC,gBAAgB,CAAC,mBAAmB,CAAC,EAAE;QAC1C,MAAM,IAAI,KAAK,CACX,oEAAoE,CAAC,CAAC;KAC3E;IAED,IAAI,SAAS,EAAE;QACb,MAAM,GAAG,GAAG,IAAI,CAAC,SAAS,CAAC,mBAAmB,CAAC,CAAC;QAChD,IAAI,GAAG,CAAC,MAAM,GAAG,2CAA2C,EAAE;YAC5D,OAAO,CAAC,IAAI,CACR,CAAA,gCAAA,EAAmC,SAAS,CAAA,kBAAA,CAAoB,GAChE,CAAA,aAAA,EAAgB,GAAG,CAAC,MAAM,CAAA,6BAAA,CAA+B,GACzD,CAAA,kEAAA,CAAoE,GACpE,CAAA,6CAAA,CAA+C,GAC/C,GAAG,2CAA2C,CAAA,CAAA,CAAG,CAAC,CAAC;SACxD;KACF;AACH,CAAC;AAaK,SAAU,gBAAgB,CAAC,CAAM;IACrC,IAAI,CAAC,KAAK,IAAI,EAAE;QACd,gEAAgE;QAChE,OAAO,IAAI,CAAC;KACb,MAAM,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;QAChC,IAAI,MAAM,CAAC,cAAc,CAAC,CAAC,CAAC,KAAK,MAAM,CAAC,SAAS,EAAE;YACjD,0DAA0D;YAC1D,MAAM,IAAI,GAAG,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YAC5B,KAAK,MAAM,GAAG,IAAI,IAAI,CAAE;gBACtB,IAAI,OAAO,GAAG,KAAK,QAAQ,EAAE;oBAC3B,6BAA6B;oBAC7B,OAAO,KAAK,CAAC;iBACd;gBACD,IAAI,CAAC,gBAAgB,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,EAAE,EAAG,kBAAkB;oBAClD,OAAO,KAAK,CAAC;iBACd;aACF;YACD,OAAO,IAAI,CAAC;SACb,MAAM;YACL,8DAA8D;YAC9D,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;gBACpB,6BAA6B;gBAC7B,KAAK,MAAM,IAAI,IAAI,CAAC,CAAE;oBACpB,IAAI,CAAC,gBAAgB,CAAC,IAAI,CAAC,EAAE,EAAG,kBAAkB;wBAChD,OAAO,KAAK,CAAC;qBACd;iBACF;gBACD,OAAO,IAAI,CAAC;aACb,MAAM;gBACL,8DAA8D;gBAC9D,6DAA6D;gBAC7D,sBAAsB;gBACtB,OAAO,KAAK,CAAC;aACd;SACF;KACF,MAAM;QACL,4CAA4C;QAC5C,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC;QACvB,OAAO,KAAK,KAAK,QAAQ,IAAI,KAAK,KAAK,QAAQ,IAAI,KAAK,KAAK,SAAS,CAAC;KACxE;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2397, "column": 0}, "map": {"version":3,"file":"version.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/version.js/__/__/__/__/__/tfjs-layers/src/version.ts"],"sourcesContent":["/** @license See the LICENSE file. */\n\n// This code is auto-generated, do not modify this file!\nconst version = '4.22.0';\nexport {version};\n"],"names":[],"mappings":"AAAA,mCAAA,EAAqC,CAErC,wDAAwD;;;;AACxD,MAAM,OAAO,GAAG,QAAQ,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2410, "column": 0}, "map": {"version":3,"file":"models.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/models.js/__/__/__/__/__/tfjs-layers/src/models.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source keras/models.py */\n\nimport {dispose, io, NamedTensorMap, Optimizer, Scalar, serialization, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport {getUid} from './backend/state';\nimport {History} from './base_callbacks';\nimport {Dataset} from './engine/dataset_stub';\nimport {Input} from './engine/input_layer';\nimport {getSourceInputs, Layer, Node, SymbolicTensor} from './engine/topology';\nimport {LayersModel, ModelCompileArgs, ModelEvaluateArgs} from './engine/training';\nimport {ModelEvaluateDatasetArgs, ModelFitDatasetArgs} from './engine/training_dataset';\nimport {ModelFitArgs} from './engine/training_tensors';\nimport {NotImplementedError, RuntimeError, ValueError} from './errors';\nimport {Shape} from './keras_format/common';\nimport {TrainingConfig} from './keras_format/training_config';\nimport {PyJsonDict} from './keras_format/types';\nimport {deserialize} from './layers/serialization';\nimport {Kwargs, NamedTensor} from './types';\nimport * as generic_utils from './utils/generic_utils';\nimport {convertPythonicToTs} from './utils/serialization_utils';\nimport {getExactlyOneShape} from './utils/types_utils';\n\n/**\n * Parses a JSON model configuration file and returns a model instance.\n *\n * ```js\n * // This example shows how to serialize a model using `toJSON()` and\n * // deserialize it as another model using `tf.models.modelFromJSON()`.\n * // Note: this example serializes and deserializes only the topology\n * // of the model; the weights of the loaded model will be different\n * // from those of the the original model, due to random weight\n * // initialization.\n * // To load the topology and weights of a model, use `tf.loadLayersModel()`.\n * const model1 = tf.sequential();\n * model1.add(tf.layers.repeatVector({inputShape: [2], n: 4}));\n * // Serialize `model1` as a JSON object.\n * const model1JSON = model1.toJSON(null, false);\n * model1.summary();\n *\n * const model2 = await tf.models.modelFromJSON(model1JSON);\n * model2.summary();\n * ```\n *\n *  @param modelAndWeightsConfig JSON object or string encoding a model and\n *       weights configuration. It can also be only the topology JSON of the\n *       model, in which case the weights will not be loaded.\n *  @param custom_objects Optional dictionary mapping names\n *       (strings) to custom classes or functions to be\n *       considered during deserialization.\n * @returns A TensorFlow.js Layers `tf.LayersModel` instance (uncompiled).\n */\nexport async function modelFromJSON(\n    modelAndWeightsConfig: ModelAndWeightsConfig|PyJsonDict,\n    customObjects?: serialization.ConfigDict): Promise<LayersModel> {\n  if (!('modelTopology' in modelAndWeightsConfig)) {\n    modelAndWeightsConfig = {modelTopology: modelAndWeightsConfig};\n  }\n  modelAndWeightsConfig = modelAndWeightsConfig as ModelAndWeightsConfig;\n\n  let modelTopology = modelAndWeightsConfig.modelTopology;\n  if (modelTopology['model_config'] != null) {\n    // If the model-topology JSON contains a 'model_config' field, then it is\n    // a full model JSON (e.g., from `keras.Model.save()`), which contains\n    // not only the model's architecture in its 'model_config' field, but\n    // additional information such as the model's optimizer. We use only the\n    // 'model_config' field currently.\n    modelTopology = modelTopology['model_config'] as PyJsonDict;\n  }\n  const tsConfig =\n      convertPythonicToTs(modelTopology) as serialization.ConfigDict;\n  const model = deserialize(tsConfig, customObjects) as LayersModel;\n\n  if (modelAndWeightsConfig.weightsManifest != null) {\n    // Load the weight values keyed by the original tensor names in the model\n    // file that was loaded.  These should match the keys of the weight\n    // manifest.\n    const weightValues = await io.loadWeights(\n        modelAndWeightsConfig.weightsManifest, modelAndWeightsConfig.pathPrefix,\n        model.weights.map(weight => weight.originalName));\n\n    // Map the weights to the unique tensor names generated during model loading\n    const uniqueWeightValues: NamedTensorMap = {};\n    for (const weight of model.weights) {\n      uniqueWeightValues[weight.originalName] =\n          weightValues[weight.originalName];\n    }\n\n    model.loadWeights(uniqueWeightValues);\n    // Dispose temporary weight values.\n    dispose(weightValues);\n  }\n  return model;\n}\n\n/**\n * Options for loading a saved mode in TensorFlow.js format.\n */\nexport interface ModelAndWeightsConfig {\n  /**\n   * A JSON object or JSON string containing the model config.\n   *\n   * This can be either of the following two formats:\n   *   - A model archiecture-only config,  i.e., a format consistent with the\n   *     return value of`keras.Model.to_json()`.\n   *   - A full model config, containing not only model architecture, but also\n   *     training options and state, i.e., a format consistent with the return\n   *     value of `keras.models.save_model()`.\n   */\n  modelTopology: PyJsonDict;\n\n  /**\n   * A weights manifest in TensorFlow.js format.\n   */\n  weightsManifest?: io.WeightsManifestConfig;\n\n  /**\n   * Path to prepend to the paths in `weightManifest` before fetching.\n   *\n   * The path may optionally end in a slash ('/').\n   */\n  pathPrefix?: string;\n}\n\n// TODO(nielsene): Remove after: https://github.com/tensorflow/tfjs/issues/400\nexport interface ModelPredictArgs {\n  /**\n   * Optional. Batch size (Integer). If unspecified, it will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Optional. Verbosity mode. Defaults to false.\n   */\n  verbose?: boolean;\n}\n\n/**\n * Load a model composed of Layer objects, including its topology and optionally\n * weights. See the Tutorial named \"How to import a Keras Model\" for usage\n * examples.\n *\n * This method is applicable to:\n *\n * 1. Models created with the `tf.layers.*`, `tf.sequential`, and\n * `tf.model` APIs of TensorFlow.js and later saved with the\n * `tf.LayersModel.save` method.\n * 2. Models converted from Keras or TensorFlow tf.keras using the\n * [tensorflowjs_converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter).\n *\n * This mode is *not* applicable to TensorFlow `SavedModel`s or their converted\n * forms. For those models, use `tf.loadGraphModel`.\n *\n * Example 1. Load a model from an HTTP server.\n *\n * ```js\n * const model = await tf.loadLayersModel(\n *     'https://storage.googleapis.com/tfjs-models/tfjs/iris_v1/model.json');\n * model.summary();\n * ```\n *\n * Example 2: Save `model`'s topology and weights to browser [local\n * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n * then load it back.\n *\n * ```js\n * const model = tf.sequential(\n *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n * console.log('Prediction from original model:');\n * model.predict(tf.ones([1, 3])).print();\n *\n * const saveResults = await model.save('localstorage://my-model-1');\n *\n * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n * console.log('Prediction from loaded model:');\n * loadedModel.predict(tf.ones([1, 3])).print();\n * ```\n *\n * Example 3. Saving `model`'s topology and weights to browser\n * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n * then load it back.\n *\n * ```js\n * const model = tf.sequential(\n *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n * console.log('Prediction from original model:');\n * model.predict(tf.ones([1, 3])).print();\n *\n * const saveResults = await model.save('indexeddb://my-model-1');\n *\n * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n * console.log('Prediction from loaded model:');\n * loadedModel.predict(tf.ones([1, 3])).print();\n * ```\n *\n * Example 4. Load a model from user-selected files from HTML\n * [file input\n * elements](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/file).\n *\n * ```js\n * // Note: this code snippet will not work without the HTML elements in the\n * //   page\n * const jsonUpload = document.getElementById('json-upload');\n * const weightsUpload = document.getElementById('weights-upload');\n *\n * const model = await tf.loadLayersModel(\n *     tf.io.browserFiles([jsonUpload.files[0], weightsUpload.files[0]]));\n * ```\n *\n * @param pathOrIOHandler Can be either of the two formats\n *   1. A string path to the `ModelAndWeightsConfig` JSON describing\n *      the model in the canonical TensorFlow.js format. For file://\n *      (tfjs-node-only), http:// and https:// schemas, the path can be\n *      either absolute or relative. The content of the JSON file is assumed to\n *      be a JSON object with the following fields and values:\n *      - 'modelTopology': A JSON object that can be either of:\n *        1. a model architecture JSON consistent with the format of the return\n *            value of `keras.Model.to_json()`\n *        2. a full model JSON in the format of `keras.models.save_model()`.\n *      - 'weightsManifest': A TensorFlow.js weights manifest.\n *      See the Python converter function `save_model()` for more details.\n *      It is also assumed that model weights can be accessed from relative\n *      paths described by the `paths` fields in weights manifest.\n *   2. A `tf.io.IOHandler` object that loads model artifacts with its `load`\n *      method.\n * @param options Optional configuration arguments for the model loading,\n *   including:\n *   - `strict`: Require that the provided weights exactly match those required\n *     by the layers.  Default true.  Passing false means that both extra\n *     weights and missing weights will be silently ignored.\n *   - `onProgress`: A progress callback of the form:\n *     `(fraction: number) => void`. This callback can be used to monitor the\n *     model-loading process.\n * @returns A `Promise` of `tf.LayersModel`, with the topology and weights\n *     loaded.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadLayersModel(\n    pathOrIOHandler: string|io.IOHandler,\n    options?: io.LoadOptions): Promise<LayersModel> {\n  if (options == null) {\n    options = {};\n  }\n  if (typeof pathOrIOHandler === 'string') {\n    const handlers = io.getLoadHandlers(pathOrIOHandler, options);\n    if (handlers.length === 0) {\n      // For backward compatibility: if no load handler can be found,\n      // assume it is a relative http path.\n      // TODO(cais): Reformat the args into a single `LoadOptions` once the core\n      // is refactored.\n      handlers.push(io.browserHTTPRequest(pathOrIOHandler, options));\n    } else if (handlers.length > 1) {\n      throw new ValueError(\n          `Found more than one (${handlers.length}) load handlers for ` +\n          `URL '${pathOrIOHandler}'`);\n    }\n    pathOrIOHandler = handlers[0];\n  }\n  return loadLayersModelFromIOHandler(pathOrIOHandler, undefined, options);\n}\n\n/**\n * Load a model and optionally its weights, using an IOHandler object.\n *\n * @param handler The instance of `IOHandler` to be used during the model\n *   loading.\n * @param customObjects Any optional custom objects to be used during model\n *   loading.\n * @param strict Whether the weight loading will be done in strict mode.\n *   Default: `true`.\n */\nexport async function loadLayersModelFromIOHandler(\n    handler: io.IOHandler, customObjects?: serialization.ConfigDict,\n    options?: io.LoadOptions): Promise<LayersModel> {\n  if (options == null) {\n    options = {};\n  }\n  if (handler.load == null) {\n    throw new ValueError(\n        'Cannot proceed with model loading because the IOHandler provided ' +\n        'does not have the `load` method implemented.');\n  }\n  const artifacts = await handler.load();\n  let modelTopology = artifacts.modelTopology as PyJsonDict;\n  if (modelTopology['model_config'] != null) {\n    modelTopology = modelTopology['model_config'] as PyJsonDict;\n  }\n\n  const strict = options.strict == null ? true : options.strict;\n  // If weights are provided and the weight-loading mode is strict, use\n  // fast weight initialization. This skips costly initializers such as\n  // 'orthogonal' and saves unnecessary computation in cases where\n  // the initialized weight values will immediately be overwritten by\n  // loaded weight values.\n  const fastWeightInit =\n      artifacts.weightData != null && artifacts.weightSpecs != null && strict;\n  const model =\n      deserialize(\n          convertPythonicToTs(modelTopology) as serialization.ConfigDict,\n          customObjects, fastWeightInit) as LayersModel;\n\n  const trainingConfig = artifacts.trainingConfig as TrainingConfig;\n  if (trainingConfig != null) {\n    model.loadTrainingConfig(trainingConfig);\n  }\n  if (artifacts.userDefinedMetadata != null) {\n    model.setUserDefinedMetadata(artifacts.userDefinedMetadata);\n  }\n\n  // If weightData is present, load the weights into the model.\n  if (artifacts.weightData != null) {\n    // Loading weights requires weightSpecs.\n    if (artifacts.weightSpecs == null) {\n      throw new ValueError(\n          'LayersModel artifacts contains weight data, but not weight specs. ' +\n          'Therefore loading of weights cannot proceed.');\n    }\n\n    const {modelWeights, optimizerWeights} = decodeModelAndOptimizerWeights(\n        artifacts.weightData, artifacts.weightSpecs);\n    model.loadWeights(modelWeights, strict);\n\n    if (model.optimizer != null && optimizerWeights.length > 0) {\n      await model.optimizer.setWeights(optimizerWeights);\n    }\n\n    // Dispose temporary weight values.\n    dispose(modelWeights);\n    dispose(optimizerWeights.map(w => w.tensor));\n  }\n  return model;\n}\n\nfunction decodeModelAndOptimizerWeights(\n    weightData: io.WeightData, specs: io.WeightsManifestEntry[]):\n    {modelWeights: NamedTensorMap, optimizerWeights: NamedTensor[]} {\n  const name2Tensor = io.decodeWeights(weightData, specs);\n  const modelWeights: NamedTensorMap = {};\n  const optimizerWeights: NamedTensor[] = [];\n  specs.forEach(spec => {\n    if (spec.group === 'optimizer') {\n      optimizerWeights.push({name: spec.name, tensor: name2Tensor[spec.name]});\n    } else {\n      modelWeights[spec.name] = name2Tensor[spec.name];\n    }\n  });\n  return {modelWeights, optimizerWeights};\n}\n\n/**\n * Configuration for a Sequential model.\n */\nexport interface SequentialArgs {\n  /** Stack of layers for the model. */\n  layers?: Layer[];\n\n  /** The name of this model. */\n  name?: string;\n}\n\n/**\n * A model with a stack of layers, feeding linearly from one to the next.\n *\n * `tf.sequential` is a factory function that creates an instance of\n * `tf.Sequential`.\n *\n * ```js\n *  // Define a model for linear regression.\n *  const model = tf.sequential();\n *  model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n *\n *  // Prepare the model for training: Specify the loss and the optimizer.\n *  model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n *\n *  // Generate some synthetic data for training.\n *  const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n *  const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n *\n *  // Train the model using the data then do inference on a data point the\n *  // model hasn't seen:\n *  await model.fit(xs, ys);\n *  model.predict(tf.tensor2d([5], [1, 1])).print();\n * ```\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class Sequential extends LayersModel {\n  /** @nocollapse */\n  static override className = 'Sequential';\n  private model: LayersModel;\n  constructor(args?: SequentialArgs) {\n    super({inputs: [], outputs: []});\n    args = args || {};\n\n    this.trainable = true;\n    this.built = false;\n\n    // Set model name.\n    this.name = (args.name != null) ? args.name : getUid('sequential_');\n\n    // Add to the model any layers passed to the constructor.\n    if (args.layers != null) {\n      for (const layer of args.layers) {\n        this.add(layer);\n      }\n    }\n  }\n\n  // Helper function to Sequential.add  Throws if the new output shape will be\n  // invalid.\n  private checkShape(layer: Layer) {\n    const shape = layer.inboundNodes[0].outputTensors[0].shape;\n    if (shape.some(x => x < 0)) {\n      throw new ValueError(\n          'Negative dimension size caused by adding layer ' +\n          `${layer.name} with input shape [` +\n          `${layer.inboundNodes[0].inputTensors[0].shape}]`);\n    }\n  }\n\n  /**\n   * Adds a layer instance on top of the layer stack.\n   *\n   * ```js\n   *  const model = tf.sequential();\n   *  model.add(tf.layers.dense({units: 8, inputShape: [1]}));\n   *  model.add(tf.layers.dense({units: 4, activation: 'relu6'}));\n   *  model.add(tf.layers.dense({units: 1, activation: 'relu6'}));\n   *  // Note that the untrained model is random at this point.\n   *  model.predict(tf.randomNormal([10, 1])).print();\n   * ```\n   * @param layer Layer instance.\n   *\n   * @exception ValueError In case the `layer` argument does not know its\n   * input shape.\n   * @exception ValueError In case the `layer` argument has multiple output\n   *   tensors, or is already connected somewhere else (forbidden in\n   *   `Sequential` models).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  add(layer: Layer): void {\n    const isLayerModelInstance =\n        layer instanceof Sequential || layer instanceof LayersModel;\n    let modelLayer: LayersModel;\n    if (isLayerModelInstance) {\n      modelLayer = layer as LayersModel;\n      if (modelLayer.outputs.length !== 1) {\n        throw new ValueError(\n            'All layers in a Sequential model ' +\n            'should have a single output tensor. ' +\n            'For multi-output layers, ' +\n            'use the functional API.');\n      }\n      if (modelLayer.inputs.length !== 1) {\n        throw new ValueError(\n            'All layers in a Sequential model ' +\n            'should have a single input tensor. ' +\n            'For multi-input layers, ' +\n            'use the functional API.');\n      }\n    }\n\n    if (this.outputs.length === 0) {\n      // first layer in model: check that it is an input layer\n      if (layer.inboundNodes.length === 0) {\n        // create an input layer\n        if (layer.batchInputShape == null) {\n          throw new ValueError(\n              'The first layer in a Sequential model must ' +\n              'get an `inputShape` or `batchInputShape` argument.');\n        }\n        // Instantiate the input layer.\n        const x = Input({\n          batchShape: layer.batchInputShape,\n          dtype: layer.dtype,\n          name: layer.name + '_input'\n        });\n        // This will build the current layer and create the node connecting\n        // the current layer to the input layer we just created.\n        layer.apply(x);\n      }\n\n      if (isLayerModelInstance) {\n        this.outputs = modelLayer.outputs;\n        this.inputs = modelLayer.inputs;\n      } else {\n        if (layer.inboundNodes.length !== 1) {\n          throw new ValueError(\n              'A layer added to a Sequential model must not already be ' +\n              `connected somewhere else. LayersModel received layer ${\n                  layer.name} ` +\n              `which has ${layer.inboundNodes.length} pre-existing inbound ` +\n              'connections.');\n        }\n\n        if (layer.inboundNodes[0].outputTensors.length !== 1) {\n          throw new ValueError(\n              'All layers in a Sequential model ' +\n              'should have a single output tensor. ' +\n              'For multi-output layers, ' +\n              'use the functional API.');\n        }\n        this.checkShape(layer);\n        this.outputs = [layer.inboundNodes[0].outputTensors[0]];\n        this.inputs = getSourceInputs(this.outputs[0]);\n      }\n\n      this.inboundNodes = [];\n      // We create an input node, which we will keep updated\n      // as we add more layers.\n      // (This call has side effects.)\n      // tslint:disable-next-line:no-unused-expression\n      new Node({\n        outboundLayer: this,\n        inboundLayers: [],\n        nodeIndices: [],\n        tensorIndices: [],\n        inputTensors: this.inputs,\n        outputTensors: this.outputs,\n        // no model-level masking for now\n        inputMasks: generic_utils.pyListRepeat(null, this.inputs.length),\n        outputMasks: [null],\n        inputShapes: this.inputs.map(x => x.shape),\n        outputShapes: this.outputs[0].shape\n      });\n    } else {\n      const outputTensor = layer.apply(this.outputs[0]);\n      if (Array.isArray(outputTensor)) {\n        throw new TypeError(\n            'All layers in a Sequential model ' +\n            'should have a single output tensor. ' +\n            'For multi-output layers, ' +\n            'use the functional API.');\n      }\n      this.checkShape(layer);\n      this.outputs = [outputTensor as SymbolicTensor];\n      // update self.inbound_nodes\n      this.inboundNodes[0].outputTensors = this.outputs;\n      this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n\n    this.layers.push(layer);\n    this.built = false;\n  }\n\n  /**\n   * Removes the last layer in the model.\n   *\n   * @exception TypeError if there are no layers in the model.\n   */\n  pop(): void {\n    if (this.layers.length === 0) {\n      throw new TypeError('There are no layers in the model.');\n    }\n\n    this.layers.pop();\n    if (this.layers.length === 0) {\n      this.outputs = [];\n      this.inboundNodes = [];\n      this.outboundNodes = [];\n    } else {\n      const lastLayerIndex = this.layers.length - 1;\n      this.layers[lastLayerIndex].outboundNodes = [];\n      this.outputs = [this.layers[lastLayerIndex].output as SymbolicTensor];\n      // update self.inbound_nodes\n      this.inboundNodes[0].outputTensors = this.outputs;\n      this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    if (this.model == null) {\n      this.build();\n    }\n    return this.model.call(inputs, kwargs);\n  }\n\n  override build(inputShape?: Shape|Shape[]) {\n    // Call `getExactlyOneShape` without using its return value,\n    // to verify that exactly one input shape is provided.\n    getExactlyOneShape(inputShape);\n\n    if (this.inputs.length === 0 || this.outputs.length === 0) {\n      throw new TypeError(\n          'Sequential model cannot be built: model is empty.' +\n          ' Add some layers first.');\n    }\n    // actually create the model\n    this.model = new LayersModel({\n      inputs: this.inputs,\n      outputs: this.outputs[0],\n      name: this.name + '_model'\n    });\n    this.model.trainable = this.trainable;\n\n    // mirror model attributes\n    this.supportsMasking = this.model.supportsMasking;\n    // TODO(michaelterry): Add caches\n    this.inputLayers = this.model.inputLayers;\n    this.inputLayersNodeIndices = this.model.inputLayersNodeIndices;\n    this.inputLayersTensorIndices = this.model.inputLayersTensorIndices;\n    this.outputLayers = this.model.outputLayers;\n    this.outputLayersNodeIndices = this.model.outputLayersNodeIndices;\n    this.outputLayersTensorIndices = this.model.outputLayersTensorIndices;\n    this.nodesByDepth = this.model.nodesByDepth;\n    this.containerNodes = this.model.containerNodes;\n    this.outputNames = this.model.outputNames;\n    this.inputNames = this.model.inputNames;\n    // TODO(michaelterry): Add feedInputNames, feedInputs, if needed.\n    // TODO(michaelterry): Add callbackModel if needed.\n    this.built = true;\n  }\n\n  override countParams(): number {\n    if (!this.built) {\n      this.build();\n    }\n    return super.countParams();\n  }\n\n  /**\n   * Print a text summary of the Sequential model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - The total number of trainable and non-trainable parameters of the\n   * model.\n   *\n   * ```js\n   * const model = tf.sequential();\n   * model.add(\n   *     tf.layers.dense({units: 100, inputShape: [10], activation: 'relu'}));\n   * model.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n   *\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override summary(\n      lineLength?: number, positions?: number[],\n      printFn:\n          // tslint:disable-next-line:no-any\n      (message?: any, ...optionalParams: any[]) => void = console.log) {\n    if (!this.built) {\n      this.build();\n    }\n    super.summary(lineLength, positions, printFn);\n  }\n\n  /**\n   * Sets the weights of the model.\n   *\n   * @param weights Should be a list of Tensors with shapes and types matching\n   *   the output of `model.getWeights()`.\n   */\n  override setWeights(weights: Tensor[]): void {\n    if (this.model == null) {\n      this.build();\n    }\n    this.model.setWeights(weights);\n  }\n\n  /**\n   * Returns the loss value & metrics values for the model in test mode.\n   *\n   * Loss and metrics are specified during `compile()`, which needs to happen\n   * before calls to `evaluate()`.\n   *\n   * Computation is done in batches.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const result = model.evaluate(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *   batchSize: 4,\n   * });\n   * result.print();\n   * ```\n   *\n   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple inputs.\n   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple outputs.\n   * @param args A `ModelEvaluateConfig`, containing optional fields.\n   *\n   * @return `Scalar` test loss (if the model has a single output and no\n   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n   *   and/or metrics). The attribute `model.metricsNames`\n   *   will give you the display labels for the scalar outputs.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override evaluate(\n      x: Tensor|Tensor[], y: Tensor|Tensor[],\n      args: ModelEvaluateArgs = {}): Scalar|Scalar[] {\n    if (!this.built) {\n      throw new RuntimeError(\n          'The model needs to be compiled before being used.');\n    }\n    return this.model.evaluate(x, y, args);\n  }\n\n  // TODO(cais): Add code snippet below once real dataset objects are\n  //   available.\n  /**\n   * Evaluate model using a dataset object.\n   *\n   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for evaluation. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs. Of the two items in the array, the\n   *   first is the input feature(s) and the second is the output target(s).\n   * @param args A configuration object for the dataset-based evaluation.\n   * @returns Loss and metric values as an Array of `Scalar` objects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override async evaluateDataset(dataset: Dataset<{}>,\n      args: ModelEvaluateDatasetArgs): Promise<Scalar|Scalar[]> {\n    if (!this.built) {\n      throw new RuntimeError(\n          'The model needs to be compiled before being used.');\n    }\n    return this.model.evaluateDataset(dataset, args);\n  }\n\n  /**\n   * Generates output predictions for the input samples.\n   *\n   * Computation is done in batches.\n   *\n   * Note: the \"step\" mode of predict() is currently not supported.\n   *   This is because the TensorFlow.js core backend is imperative only.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predict(tf.ones([2, 10])).print();\n   * ```\n   *\n   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n   *   the model has multiple inputs.\n   * @param conifg A `ModelPredictConfig` object containing optional fields.\n   *\n   * @return `tf.Tensor`(s) of predictions.\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and the model's expectations, or in case a stateful model receives a\n   *   number of samples that is not a multiple of the batch size.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override predict(x: Tensor|Tensor[], args: ModelPredictArgs = {}):\n      Tensor|Tensor[] {\n    if (this.model == null) {\n      this.build();\n    }\n    return this.model.predict(x, args);\n  }\n\n  /**\n   * Returns predictions for a single batch of samples.\n   *\n   * @param x: Input samples, as a Tensor, or list of Tensors (if the model\n   *   has multiple inputs).\n   * @return Tensor(s) of predictions\n   */\n  override predictOnBatch(x: Tensor): Tensor|Tensor[] {\n    if (this.model == null) {\n      this.build();\n    }\n    return this.model.predictOnBatch(x);\n  }\n\n  /**\n   * See `LayersModel.compile`.\n   *\n   * @param args\n   */\n  override compile(args: ModelCompileArgs): void {\n    this.build();\n    this.model.compile(args);\n    this.optimizer_ = this.model.optimizer;\n    // tslint:disable-next-line:no-any\n    this.isOptimizerOwned = (this.model as any).isOptimizerOwned;\n    this.loss = this.model.loss;\n    this.metrics = this.model.metrics;\n    // TODO(cais): Add this.lossWeights, this.sampleWeightMode,\n    //   this.weightedMetrics, this.targets.\n    this.metricsTensors = this.model.metricsTensors;\n    this.metricsNames = this.model.metricsNames;\n    // TODO(cais): Add sampleWeights.\n  }\n\n  override get optimizer(): Optimizer {\n    return this.model == null ? undefined : this.model.optimizer;\n  }\n\n  override set optimizer(optimizer: Optimizer) {\n    this.model.optimizer = optimizer;\n  }\n\n  /**\n   * Trains the model for a fixed number of epochs (iterations on a dataset).\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const history = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *   batchSize: 4,\n   *   epochs: 3\n   * });\n   * console.log(history.history.loss[0]);\n   * ```\n   *\n   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n   * model has multiple inputs. If all inputs in the model are named, you can\n   * also pass a dictionary mapping input names to `tf.Tensor`s.\n   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n   * the model has multiple outputs. If all outputs in the model are named, you\n   *  can also pass a dictionary mapping output names to `tf.Tensor`s.\n   * @param args  A `ModelFitConfig`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and what the model expects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override async fit(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      args: ModelFitArgs = {}): Promise<History> {\n    if (!this.built) {\n      throw new RuntimeError(\n          'The model needs to be compiled before ' +\n          'being used.');\n    }\n    return this.model.fit(x, y, args);\n  }\n\n  /**\n   * Trains the model using a dataset object.\n   *\n   * ```js\n   * const xArray = [\n   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],\n   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],\n   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],\n   *   [1, 1, 1, 1, 1, 1, 1, 1, 1],\n   * ];\n   * const yArray = [1, 1, 1, 1];\n   * // Create a dataset from the JavaScript array.\n   * const xDataset = tf.data.array(xArray);\n   * const yDataset = tf.data.array(yArray);\n   * // Zip combines the `x` and `y` Datasets into a single Dataset, the\n   * // iterator of which will return an object containing of two tensors,\n   * // corresponding to `x` and `y`.  The call to `batch(4)` will bundle\n   * // four such samples into a single object, with the same keys now pointing\n   * // to tensors that hold 4 examples, organized along the batch dimension.\n   * // The call to `shuffle(4)` causes each iteration through the dataset to\n   * // happen in a different order.  The size of the shuffle window is 4.\n   * const xyDataset = tf.data.zip({xs: xDataset, ys: yDataset})\n   *     .batch(4)\n   *     .shuffle(4);\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [9]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const history = await model.fitDataset(xyDataset, {\n   *   epochs: 4,\n   *   callbacks: {onEpochEnd: (epoch, logs) => console.log(logs.loss)}\n   * });\n   * ```\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected to\n   *   generate a dataset iterator object, the `next()` method of which is\n   *   expected to produce data batches for evaluation. The return value of the\n   *   `next()` call ought to contain a boolean `done` field and a `value`\n   *   field.\n   *\n   *   The `value` field is expected to be an object of with fields\n   *   `xs` and `ys`, which point to the feature tensor and the target tensor,\n   *   respectively. This case is for models with exactly one input and one\n   *   output (e.g. a sequential model). For example:\n   *   ```js\n   *   {value: {xs: xsTensor, ys: ysTensor}, done: false}\n   *   ```\n   *\n   *   If the model has multiple inputs, the `xs` field of `value` should\n   *   be an object mapping input names to their respective feature tensors.\n   *   For example:\n   *   ```js\n   *   {\n   *     value: {\n   *       xs: {\n   *         input_1: xsTensor1,\n   *         input_2: xsTensor2\n   *       },\n   *       ys: ysTensor\n   *     },\n   *     done: false\n   *   }\n   *   ```\n   *   If the model has multiple outputs, the `ys` field of `value` should\n   *   be an object mapping output names to their respective target tensors.\n   *   For example:\n   *   ```js\n   *   {\n   *     value: {\n   *       xs: xsTensor,\n   *       ys: {\n   *         output_1: ysTensor1,\n   *         output_2: ysTensor2\n   *       },\n   *     },\n   *     done: false\n   *   }\n   *   ```\n   * @param args A `ModelFitDatasetArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  override async fitDataset<T>(dataset: Dataset<T>,\n      args: ModelFitDatasetArgs<T>): Promise<History> {\n    if (!this.built) {\n      throw new RuntimeError(\n          'The model needs to be compiled before ' +\n          'being used.');\n    }\n    return this.model.fitDataset(dataset, args);\n  }\n\n  /**\n   * Runs a single gradient update on a single batch of data.\n   *\n   * This method differs from `fit()` and `fitDataset()` in the following\n   * regards:\n   *   - It operates on exactly one batch of data.\n   *   - It returns only the loss and metric values, instead of\n   *     returning the batch-by-batch loss and metric values.\n   *   - It doesn't support fine-grained options such as verbosity and\n   *     callbacks.\n   *\n   * @param x Input data. It could be one of the following:\n   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n   *     multiple inputs).\n   *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n   *     model has named inputs).\n   * @param y Target data. It could be either a `tf.Tensor` or multiple\n   *   `tf.Tensor`s. It should be consistent with `x`.\n   * @returns Training loss or losses (in case the model has\n   *   multiple outputs), along with metrics (if any), as numbers.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  override async trainOnBatch(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|\n      {[inputName: string]: Tensor}): Promise<number|number[]> {\n    return this.model.trainOnBatch(x, y);\n  }\n\n  /* See parent class for JsDoc */\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict,\n      fastWeightInit = false): T {\n    let configArray: serialization.ConfigDictArray;\n    let extraModelConfig: serialization.ConfigDict = {};\n    if (config instanceof Array) {\n      if (!(config[0].className != null) ||\n          config[0]['className'] === 'Merge') {\n        throw new ValueError('Legacy serialization format not supported yet.');\n      }\n      configArray = config;\n    } else {\n      util.assert(\n          config['layers'] != null,\n          () =>\n              `When the config data for a Sequential model is not an Array, ` +\n              `it must be an Object that contains the 'layers' field.`);\n      configArray = config['layers'] as serialization.ConfigDictArray;\n      delete config['layers'];\n      extraModelConfig = config;\n    }\n\n    const model = new cls(extraModelConfig);\n    if (!(model instanceof Sequential)) {\n      throw new NotImplementedError(\n          `Sequential.fromConfig called on non-Sequential input: ${model}`);\n    }\n    for (const conf of configArray) {\n      const customObjects: serialization.ConfigDict = undefined;\n      const layer = deserialize(\n                        conf as serialization.ConfigDict, customObjects,\n                        fastWeightInit) as Layer;\n      if (fastWeightInit) {\n        layer.setFastWeightInitDuringBuild(true);\n      }\n      model.add(layer);\n    }\n    return model;\n  }\n\n  /**\n   * Setter used for force stopping of LayersModel.fit() (i.e., training).\n   *\n   * Example:\n   *\n   * ```js\n   * const model = tf.sequential();\n   * model.add(tf.layers.dense({units: 1, inputShape: [10]}));\n   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n   * const xs = tf.ones([8, 10]);\n   * const ys = tf.zeros([8, 1]);\n   *\n   * const history = await model.fit(xs, ys, {\n   *   epochs: 10,\n   *   callbacks: {\n   *     onEpochEnd: async (epoch, logs) => {\n   *       if (epoch === 2) {\n   *         model.stopTraining = true;\n   *       }\n   *     }\n   *   }\n   * });\n   *\n   * // There should be only 3 values in the loss array, instead of 10 values,\n   * // due to the stopping after 3 epochs.\n   * console.log(history.history.loss);\n   * ```\n   */\n  override set stopTraining(stop: boolean) {\n    // TODO(cais): When refactoring to remove the composition pattern happens,\n    // remove this method overriding.\n    if (this.model == null) {\n      throw new ValueError(\n          'Cannot set the stopTraining property of a sequential model before ' +\n          'it is compiled.');\n    }\n    this.model.stopTraining = stop;\n  }\n\n  override get stopTraining(): boolean {\n    if (this.model == null) {\n      throw new ValueError(\n          'Cannot get the stopTraining property of a sequential model before ' +\n          'it is compiled.');\n    }\n    return this.model.stopTraining;\n  }\n\n  // TODO(cais): Override get trainableWeights() here\n\n  // tslint:disable-next-line:no-any\n  override getConfig(): any {\n    // NOTE(cais): We override the return type of getConfig() to `any` here,\n    //   because the `Sequential` class is a special case among `Container`\n    //   subtypes in that its getConfig() method returns an Array (not a\n    //   dict).\n    const layers: serialization.ConfigDict[] = [];\n    for (const layer of this.layers) {\n      const dict: serialization.ConfigDict = {};\n      dict['className'] = layer.getClassName();\n      dict['config'] = layer.getConfig();\n      layers.push(dict);\n    }\n    return {name: this.name, layers};\n  }\n}\nserialization.registerClass(Sequential);\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,mCAAA,EAAqC;;;;;;AAErC,OAAO,EAAC,OAAO,EAAE,EAAE,EAAqC,aAAa,EAAU,IAAI,EAAC,MAAM,uBAAuB,CAAC;;;;;AAElH,OAAO,EAAC,MAAM,EAAC,MAAM,iBAAiB,CAAC;AAGvC,OAAO,EAAC,KAAK,EAAC,MAAM,sBAAsB,CAAC;AAC3C,OAAO,EAAC,eAAe,EAAS,IAAI,EAAiB,MAAM,mBAAmB,CAAC;AAC/E,OAAO,EAAC,WAAW,EAAsC,MAAM,mBAAmB,CAAC;AAGnF,OAAO,EAAC,mBAAmB,EAAE,YAAY,EAAE,UAAU,EAAC,MAAM,UAAU,CAAC;AAIvE,OAAO,EAAC,WAAW,EAAC,MAAM,wBAAwB,CAAC;AAEnD,OAAO,KAAK,aAAa,MAAM,uBAAuB,CAAC;AACvD,OAAO,EAAC,mBAAmB,EAAC,MAAM,6BAA6B,CAAC;AAChE,OAAO,EAAC,kBAAkB,EAAC,MAAM,qBAAqB,CAAC;;;;;;;;;;;AA+BhD,KAAK,UAAU,aAAa,CAC/B,qBAAuD,EACvD,aAAwC;IAC1C,IAAI,CAAC,CAAC,eAAe,IAAI,qBAAqB,CAAC,EAAE;QAC/C,qBAAqB,GAAG;YAAC,aAAa,EAAE,qBAAqB;QAAA,CAAC,CAAC;KAChE;IACD,qBAAqB,GAAG,qBAA8C,CAAC;IAEvE,IAAI,aAAa,GAAG,qBAAqB,CAAC,aAAa,CAAC;IACxD,IAAI,aAAa,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;QACzC,yEAAyE;QACzE,sEAAsE;QACtE,qEAAqE;QACrE,wEAAwE;QACxE,kCAAkC;QAClC,aAAa,GAAG,aAAa,CAAC,cAAc,CAAe,CAAC;KAC7D;IACD,MAAM,QAAQ,iMACV,sBAAA,AAAmB,EAAC,aAAa,CAA6B,CAAC;IACnE,MAAM,KAAK,2LAAG,eAAA,AAAW,EAAC,QAAQ,EAAE,aAAa,CAAgB,CAAC;IAElE,IAAI,qBAAqB,CAAC,eAAe,IAAI,IAAI,EAAE;QACjD,yEAAyE;QACzE,mEAAmE;QACnE,YAAY;QACZ,MAAM,YAAY,GAAG,wMAAM,KAAE,CAAC,WAAW,CACrC,qBAAqB,CAAC,eAAe,EAAE,qBAAqB,CAAC,UAAU,EACvE,KAAK,CAAC,OAAO,CAAC,GAAG,EAAC,MAAM,CAAC,EAAE,AAAC,MAAM,CAAC,YAAY,CAAC,CAAC,CAAC;QAEtD,4EAA4E;QAC5E,MAAM,kBAAkB,GAAmB,CAAA,CAAE,CAAC;QAC9C,KAAK,MAAM,MAAM,IAAI,KAAK,CAAC,OAAO,CAAE;YAClC,kBAAkB,CAAC,MAAM,CAAC,YAAY,CAAC,GACnC,YAAY,CAAC,MAAM,CAAC,YAAY,CAAC,CAAC;SACvC;QAED,KAAK,CAAC,WAAW,CAAC,kBAAkB,CAAC,CAAC;QACtC,mCAAmC;QACnC,iLAAA,AAAO,EAAC,YAAY,CAAC,CAAC;KACvB;IACD,OAAO,KAAK,CAAC;AACf,CAAC;AAiJM,KAAK,UAAU,eAAe,CACjC,eAAoC,EACpC,OAAwB;IAC1B,IAAI,OAAO,IAAI,IAAI,EAAE;QACnB,OAAO,GAAG,CAAA,CAAE,CAAC;KACd;IACD,IAAI,OAAO,eAAe,KAAK,QAAQ,EAAE;QACvC,MAAM,QAAQ,GAAG,uMAAE,CAAC,eAAe,CAAC,eAAe,EAAE,OAAO,CAAC,CAAC;QAC9D,IAAI,QAAQ,CAAC,MAAM,KAAK,CAAC,EAAE;YACzB,+DAA+D;YAC/D,qCAAqC;YACrC,0EAA0E;YAC1E,iBAAiB;YACjB,QAAQ,CAAC,IAAI,mMAAC,KAAE,CAAC,kBAAkB,CAAC,eAAe,EAAE,OAAO,CAAC,CAAC,CAAC;SAChE,MAAM,IAAI,QAAQ,CAAC,MAAM,GAAG,CAAC,EAAE;YAC9B,MAAM,IAAI,iLAAU,CAChB,CAAA,qBAAA,EAAwB,QAAQ,CAAC,MAAM,CAAA,oBAAA,CAAsB,GAC7D,CAAA,KAAA,EAAQ,eAAe,CAAA,CAAA,CAAG,CAAC,CAAC;SACjC;QACD,eAAe,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC;KAC/B;IACD,OAAO,4BAA4B,CAAC,eAAe,EAAE,SAAS,EAAE,OAAO,CAAC,CAAC;AAC3E,CAAC;AAYM,KAAK,UAAU,4BAA4B,CAC9C,OAAqB,EAAE,aAAwC,EAC/D,OAAwB;IAC1B,IAAI,OAAO,IAAI,IAAI,EAAE;QACnB,OAAO,GAAG,CAAA,CAAE,CAAC;KACd;IACD,IAAI,OAAO,CAAC,IAAI,IAAI,IAAI,EAAE;QACxB,MAAM,wKAAI,aAAU,CAChB,mEAAmE,GACnE,8CAA8C,CAAC,CAAC;KACrD;IACD,MAAM,SAAS,GAAG,MAAM,OAAO,CAAC,IAAI,EAAE,CAAC;IACvC,IAAI,aAAa,GAAG,SAAS,CAAC,aAA2B,CAAC;IAC1D,IAAI,aAAa,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;QACzC,aAAa,GAAG,aAAa,CAAC,cAAc,CAAe,CAAC;KAC7D;IAED,MAAM,MAAM,GAAG,OAAO,CAAC,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC;IAC9D,qEAAqE;IACrE,qEAAqE;IACrE,gEAAgE;IAChE,mEAAmE;IACnE,wBAAwB;IACxB,MAAM,cAAc,GAChB,SAAS,CAAC,UAAU,IAAI,IAAI,IAAI,SAAS,CAAC,WAAW,IAAI,IAAI,IAAI,MAAM,CAAC;IAC5E,MAAM,KAAK,4LACP,cAAA,AAAW,gMACP,sBAAA,AAAmB,EAAC,aAAa,CAA6B,EAC9D,aAAa,EAAE,cAAc,CAAgB,CAAC;IAEtD,MAAM,cAAc,GAAG,SAAS,CAAC,cAAgC,CAAC;IAClE,IAAI,cAAc,IAAI,IAAI,EAAE;QAC1B,KAAK,CAAC,kBAAkB,CAAC,cAAc,CAAC,CAAC;KAC1C;IACD,IAAI,SAAS,CAAC,mBAAmB,IAAI,IAAI,EAAE;QACzC,KAAK,CAAC,sBAAsB,CAAC,SAAS,CAAC,mBAAmB,CAAC,CAAC;KAC7D;IAED,6DAA6D;IAC7D,IAAI,SAAS,CAAC,UAAU,IAAI,IAAI,EAAE;QAChC,wCAAwC;QACxC,IAAI,SAAS,CAAC,WAAW,IAAI,IAAI,EAAE;YACjC,MAAM,wKAAI,aAAU,CAChB,oEAAoE,GACpE,8CAA8C,CAAC,CAAC;SACrD;QAED,MAAM,EAAC,YAAY,EAAE,gBAAgB,EAAC,GAAG,8BAA8B,CACnE,SAAS,CAAC,UAAU,EAAE,SAAS,CAAC,WAAW,CAAC,CAAC;QACjD,KAAK,CAAC,WAAW,CAAC,YAAY,EAAE,MAAM,CAAC,CAAC;QAExC,IAAI,KAAK,CAAC,SAAS,IAAI,IAAI,IAAI,gBAAgB,CAAC,MAAM,GAAG,CAAC,EAAE;YAC1D,MAAM,KAAK,CAAC,SAAS,CAAC,UAAU,CAAC,gBAAgB,CAAC,CAAC;SACpD;QAED,mCAAmC;+KACnC,UAAA,AAAO,EAAC,YAAY,CAAC,CAAC;+KACtB,UAAA,AAAO,EAAC,gBAAgB,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC;KAC9C;IACD,OAAO,KAAK,CAAC;AACf,CAAC;AAED,SAAS,8BAA8B,CACnC,UAAyB,EAAE,KAAgC;IAE7D,MAAM,WAAW,oMAAG,MAAE,CAAC,aAAa,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC;IACxD,MAAM,YAAY,GAAmB,CAAA,CAAE,CAAC;IACxC,MAAM,gBAAgB,GAAkB,EAAE,CAAC;IAC3C,KAAK,CAAC,OAAO,EAAC,IAAI,CAAC,EAAE;QACnB,IAAI,IAAI,CAAC,KAAK,KAAK,WAAW,EAAE;YAC9B,gBAAgB,CAAC,IAAI,CAAC;gBAAC,IAAI,EAAE,IAAI,CAAC,IAAI;gBAAE,MAAM,EAAE,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC;YAAA,CAAC,CAAC,CAAC;SAC1E,MAAM;YACL,YAAY,CAAC,IAAI,CAAC,IAAI,CAAC,GAAG,WAAW,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SAClD;IACH,CAAC,CAAC,CAAC;IACH,OAAO;QAAC,YAAY;QAAE,gBAAgB;IAAA,CAAC,CAAC;AAC1C,CAAC;AAaD;;;;;;;;;;;;;;;;;;;;;;;;;GAyBG,CACH,MAAa,UAAW,yLAAQ,cAAW;IAIzC,YAAY,IAAqB,CAAA;QAC/B,KAAK,CAAC;YAAC,MAAM,EAAE,EAAE;YAAE,OAAO,EAAE,EAAE;QAAA,CAAC,CAAC,CAAC;QACjC,IAAI,GAAG,IAAI,IAAI,CAAA,CAAE,CAAC;QAElB,IAAI,CAAC,SAAS,GAAG,IAAI,CAAC;QACtB,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;QAEnB,kBAAkB;QAClB,IAAI,CAAC,IAAI,GAAG,AAAC,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,AAAC,IAAI,CAAC,IAAI,CAAC,CAAC,kLAAC,UAAA,AAAM,EAAC,aAAa,CAAC,CAAC;QAEpE,yDAAyD;QACzD,IAAI,IAAI,CAAC,MAAM,IAAI,IAAI,EAAE;YACvB,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;gBAC/B,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;aACjB;SACF;IACH,CAAC;IAED,4EAA4E;IAC5E,WAAW;IACH,UAAU,CAAC,KAAY,EAAA;QAC7B,MAAM,KAAK,GAAG,KAAK,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;QAC3D,IAAI,KAAK,CAAC,IAAI,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,GAAG,CAAC,CAAC,EAAE;YAC1B,MAAM,wKAAI,aAAU,CAChB,iDAAiD,GACjD,GAAG,KAAK,CAAC,IAAI,CAAA,mBAAA,CAAqB,GAClC,GAAG,KAAK,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,KAAK,CAAA,CAAA,CAAG,CAAC,CAAC;SACxD;IACH,CAAC;IAED;;;;;;;;;;;;;;;;;;;;OAoBG,CACH,GAAG,CAAC,KAAY,EAAA;QACd,MAAM,oBAAoB,GACtB,KAAK,YAAY,UAAU,IAAI,KAAK,4LAAY,cAAW,CAAC;QAChE,IAAI,UAAuB,CAAC;QAC5B,IAAI,oBAAoB,EAAE;YACxB,UAAU,GAAG,KAAoB,CAAC;YAClC,IAAI,UAAU,CAAC,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;gBACnC,MAAM,wKAAI,aAAU,CAChB,mCAAmC,GACnC,sCAAsC,GACtC,2BAA2B,GAC3B,yBAAyB,CAAC,CAAC;aAChC;YACD,IAAI,UAAU,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;gBAClC,MAAM,wKAAI,aAAU,CAChB,mCAAmC,GACnC,qCAAqC,GACrC,0BAA0B,GAC1B,yBAAyB,CAAC,CAAC;aAChC;SACF;QAED,IAAI,IAAI,CAAC,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;YAC7B,wDAAwD;YACxD,IAAI,KAAK,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;gBACnC,wBAAwB;gBACxB,IAAI,KAAK,CAAC,eAAe,IAAI,IAAI,EAAE;oBACjC,MAAM,wKAAI,aAAU,CAChB,6CAA6C,GAC7C,oDAAoD,CAAC,CAAC;iBAC3D;gBACD,+BAA+B;gBAC/B,MAAM,CAAC,0LAAG,QAAA,AAAK,EAAC;oBACd,UAAU,EAAE,KAAK,CAAC,eAAe;oBACjC,KAAK,EAAE,KAAK,CAAC,KAAK;oBAClB,IAAI,EAAE,KAAK,CAAC,IAAI,GAAG,QAAQ;iBAC5B,CAAC,CAAC;gBACH,mEAAmE;gBACnE,wDAAwD;gBACxD,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;aAChB;YAED,IAAI,oBAAoB,EAAE;gBACxB,IAAI,CAAC,OAAO,GAAG,UAAU,CAAC,OAAO,CAAC;gBAClC,IAAI,CAAC,MAAM,GAAG,UAAU,CAAC,MAAM,CAAC;aACjC,MAAM;gBACL,IAAI,KAAK,CAAC,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE;oBACnC,MAAM,wKAAI,aAAU,CAChB,0DAA0D,GAC1D,CAAA,qDAAA,EACI,KAAK,CAAC,IAAI,CAAA,CAAA,CAAG,GACjB,CAAA,UAAA,EAAa,KAAK,CAAC,YAAY,CAAC,MAAM,CAAA,sBAAA,CAAwB,GAC9D,cAAc,CAAC,CAAC;iBACrB;gBAED,IAAI,KAAK,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,MAAM,KAAK,CAAC,EAAE;oBACpD,MAAM,wKAAI,aAAU,CAChB,mCAAmC,GACnC,sCAAsC,GACtC,2BAA2B,GAC3B,yBAAyB,CAAC,CAAC;iBAChC;gBACD,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC;gBACvB,IAAI,CAAC,OAAO,GAAG;oBAAC,KAAK,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,CAAC,CAAC;iBAAC,CAAC;gBACxD,IAAI,CAAC,MAAM,uLAAG,kBAAe,AAAf,EAAgB,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;aAChD;YAED,IAAI,CAAC,YAAY,GAAG,EAAE,CAAC;YACvB,sDAAsD;YACtD,yBAAyB;YACzB,gCAAgC;YAChC,gDAAgD;YAChD,oLAAI,OAAI,CAAC;gBACP,aAAa,EAAE,IAAI;gBACnB,aAAa,EAAE,EAAE;gBACjB,WAAW,EAAE,EAAE;gBACf,aAAa,EAAE,EAAE;gBACjB,YAAY,EAAE,IAAI,CAAC,MAAM;gBACzB,aAAa,EAAE,IAAI,CAAC,OAAO;gBAC3B,iCAAiC;gBACjC,UAAU,0LAAE,aAAa,CAAC,CAAA,AAAY,EAAC,IAAI,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC;gBAChE,WAAW,EAAE;oBAAC,IAAI;iBAAC;gBACnB,WAAW,EAAE,IAAI,CAAC,MAAM,CAAC,GAAG,EAAC,CAAC,CAAC,EAAE,AAAC,CAAC,CAAC,KAAK,CAAC;gBAC1C,YAAY,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK;aACpC,CAAC,CAAC;SACJ,MAAM;YACL,MAAM,YAAY,GAAG,KAAK,CAAC,KAAK,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;YAClD,IAAI,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,EAAE;gBAC/B,MAAM,IAAI,SAAS,CACf,mCAAmC,GACnC,sCAAsC,GACtC,2BAA2B,GAC3B,yBAAyB,CAAC,CAAC;aAChC;YACD,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC;YACvB,IAAI,CAAC,OAAO,GAAG;gBAAC,YAA8B;aAAC,CAAC;YAChD,4BAA4B;YAC5B,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,GAAG,IAAI,CAAC,OAAO,CAAC;YAClD,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,YAAY,GAAG;gBAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK;aAAC,CAAC;SAC7D;QAED,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;QACxB,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;IACrB,CAAC;IAED;;;;OAIG,CACH,GAAG,GAAA;QACD,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;YAC5B,MAAM,IAAI,SAAS,CAAC,mCAAmC,CAAC,CAAC;SAC1D;QAED,IAAI,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC;QAClB,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,EAAE;YAC5B,IAAI,CAAC,OAAO,GAAG,EAAE,CAAC;YAClB,IAAI,CAAC,YAAY,GAAG,EAAE,CAAC;YACvB,IAAI,CAAC,aAAa,GAAG,EAAE,CAAC;SACzB,MAAM;YACL,MAAM,cAAc,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC;YAC9C,IAAI,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC,aAAa,GAAG,EAAE,CAAC;YAC/C,IAAI,CAAC,OAAO,GAAG;gBAAC,IAAI,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC,MAAwB;aAAC,CAAC;YACtE,4BAA4B;YAC5B,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,aAAa,GAAG,IAAI,CAAC,OAAO,CAAC;YAClD,IAAI,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC,YAAY,GAAG;gBAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK;aAAC,CAAC;SAC7D;IACH,CAAC;IAEQ,IAAI,CAAC,MAAuB,EAAE,MAAc,EAAA;QACnD,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,CAAC,CAAC;IACzC,CAAC;IAEQ,KAAK,CAAC,UAA0B,EAAA;QACvC,4DAA4D;QAC5D,sDAAsD;8LACtD,qBAAA,AAAkB,EAAC,UAAU,CAAC,CAAC;QAE/B,IAAI,IAAI,CAAC,MAAM,CAAC,MAAM,KAAK,CAAC,IAAI,IAAI,CAAC,OAAO,CAAC,MAAM,KAAK,CAAC,EAAE;YACzD,MAAM,IAAI,SAAS,CACf,mDAAmD,GACnD,yBAAyB,CAAC,CAAC;SAChC;QACD,4BAA4B;QAC5B,IAAI,CAAC,KAAK,GAAG,oLAAI,cAAW,CAAC;YAC3B,MAAM,EAAE,IAAI,CAAC,MAAM;YACnB,OAAO,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC;YACxB,IAAI,EAAE,IAAI,CAAC,IAAI,GAAG,QAAQ;SAC3B,CAAC,CAAC;QACH,IAAI,CAAC,KAAK,CAAC,SAAS,GAAG,IAAI,CAAC,SAAS,CAAC;QAEtC,0BAA0B;QAC1B,IAAI,CAAC,eAAe,GAAG,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;QAClD,iCAAiC;QACjC,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,WAAW,CAAC;QAC1C,IAAI,CAAC,sBAAsB,GAAG,IAAI,CAAC,KAAK,CAAC,sBAAsB,CAAC;QAChE,IAAI,CAAC,wBAAwB,GAAG,IAAI,CAAC,KAAK,CAAC,wBAAwB,CAAC;QACpE,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,CAAC;QAC5C,IAAI,CAAC,uBAAuB,GAAG,IAAI,CAAC,KAAK,CAAC,uBAAuB,CAAC;QAClE,IAAI,CAAC,yBAAyB,GAAG,IAAI,CAAC,KAAK,CAAC,yBAAyB,CAAC;QACtE,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,CAAC;QAC5C,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;QAChD,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,WAAW,CAAC;QAC1C,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC;QACxC,iEAAiE;QACjE,mDAAmD;QACnD,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC;IACpB,CAAC;IAEQ,WAAW,GAAA;QAClB,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,OAAO,KAAK,CAAC,WAAW,EAAE,CAAC;IAC7B,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA6BG,CACM,OAAO,CACZ,UAAmB,EAAE,SAAoB,EACzC,UAEoD,OAAO,CAAC,GAAG,EAAA;QACjE,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,KAAK,CAAC,OAAO,CAAC,UAAU,EAAE,SAAS,EAAE,OAAO,CAAC,CAAC;IAChD,CAAC;IAED;;;;;OAKG,CACM,UAAU,CAAC,OAAiB,EAAA;QACnC,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;IACjC,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA+BG,CACM,QAAQ,CACb,CAAkB,EAAE,CAAkB,EACtC,OAA0B,CAAA,CAAE,EAAA;QAC9B,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,wKAAI,eAAY,CAClB,mDAAmD,CAAC,CAAC;SAC1D;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,QAAQ,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;IACzC,CAAC;IAED,mEAAmE;IACnE,eAAe;IACf;;;;;;;;;;;;;;;;;;;OAmBG,CACM,KAAK,CAAC,eAAe,CAAC,OAAoB,EAC/C,IAA8B,EAAA;QAChC,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,wKAAI,eAAY,CAClB,mDAAmD,CAAC,CAAC;SAC1D;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC,OAAO,EAAE,IAAI,CAAC,CAAC;IACnD,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;OA0BG,CACM,OAAO,CAAC,CAAkB,EAAE,OAAyB,CAAA,CAAE,EAAA;QAE9D,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IACrC,CAAC;IAED;;;;;;OAMG,CACM,cAAc,CAAC,CAAS,EAAA;QAC/B,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,IAAI,CAAC,KAAK,EAAE,CAAC;SACd;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC,CAAC,CAAC,CAAC;IACtC,CAAC;IAED;;;;OAIG,CACM,OAAO,CAAC,IAAsB,EAAA;QACrC,IAAI,CAAC,KAAK,EAAE,CAAC;QACb,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC;QACzB,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,KAAK,CAAC,SAAS,CAAC;QACvC,kCAAkC;QAClC,IAAI,CAAC,gBAAgB,GAAI,IAAI,CAAC,KAAa,CAAC,gBAAgB,CAAC;QAC7D,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC;QAC5B,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC;QAClC,2DAA2D;QAC3D,wCAAwC;QACxC,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;QAChD,IAAI,CAAC,YAAY,GAAG,IAAI,CAAC,KAAK,CAAC,YAAY,CAAC;IAC5C,iCAAiC;IACnC,CAAC;IAED,IAAa,SAAS,GAAA;QACpB,OAAO,IAAI,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,SAAS,CAAC;IAC/D,CAAC;IAED,IAAa,SAAS,CAAC,SAAoB,EAAA;QACzC,IAAI,CAAC,KAAK,CAAC,SAAS,GAAG,SAAS,CAAC;IACnC,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OA8BG,CACM,KAAK,CAAC,GAAG,CACd,CAAgD,EAChD,CAAgD,EAChD,OAAqB,CAAA,CAAE,EAAA;QACzB,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,wKAAI,eAAY,CAClB,wCAAwC,GACxC,aAAa,CAAC,CAAC;SACpB;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;IACpC,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;OAoFG,CACM,KAAK,CAAC,UAAU,CAAI,OAAmB,EAC5C,IAA4B,EAAA;QAC9B,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE;YACf,MAAM,wKAAI,eAAY,CAClB,wCAAwC,GACxC,aAAa,CAAC,CAAC;SACpB;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,OAAO,EAAE,IAAI,CAAC,CAAC;IAC9C,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;OAsBG,CACM,KAAK,CAAC,YAAY,CACvB,CAAgD,EAChD,CAC6B,EAAA;QAC/B,OAAO,IAAI,CAAC,KAAK,CAAC,YAAY,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IACvC,CAAC;IAED,8BAAA,EAAgC,CAChC,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA6C,EAC7C,MAAgC,EAChC,gBAAgB,CAAA,CAA8B,EAC9C,cAAc,GAAG,KAAK,EAAA;QACxB,IAAI,WAA0C,CAAC;QAC/C,IAAI,gBAAgB,GAA6B,CAAA,CAAE,CAAC;QACpD,IAAI,MAAM,YAAY,KAAK,EAAE;YAC3B,IAAI,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,SAAS,IAAI,IAAI,CAAC,IAC9B,MAAM,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,KAAK,OAAO,EAAE;gBACtC,MAAM,wKAAI,aAAU,CAAC,gDAAgD,CAAC,CAAC;aACxE;YACD,WAAW,GAAG,MAAM,CAAC;SACtB,MAAM;4MACL,OAAI,CAAC,MAAM,CACP,MAAM,CAAC,QAAQ,CAAC,IAAI,IAAI,EACxB,GAAG,CACC,CAAA,AADC,6DACD,CAA+D,GAC/D,CAAA,sDAAA,CAAwD,CAAC,CAAC;YAClE,WAAW,GAAG,MAAM,CAAC,QAAQ,CAAkC,CAAC;YAChE,OAAO,MAAM,CAAC,QAAQ,CAAC,CAAC;YACxB,gBAAgB,GAAG,MAAM,CAAC;SAC3B;QAED,MAAM,KAAK,GAAG,IAAI,GAAG,CAAC,gBAAgB,CAAC,CAAC;QACxC,IAAI,CAAC,CAAC,KAAK,YAAY,UAAU,CAAC,EAAE;YAClC,MAAM,IAAI,0LAAmB,CACzB,CAAA,sDAAA,EAAyD,KAAK,EAAE,CAAC,CAAC;SACvE;QACD,KAAK,MAAM,IAAI,IAAI,WAAW,CAAE;YAC9B,MAAM,aAAa,GAA6B,SAAS,CAAC;YAC1D,MAAM,KAAK,4LAAG,cAAA,AAAW,EACP,IAAgC,EAAE,aAAa,EAC/C,cAAc,CAAU,CAAC;YAC3C,IAAI,cAAc,EAAE;gBAClB,KAAK,CAAC,4BAA4B,CAAC,IAAI,CAAC,CAAC;aAC1C;YACD,KAAK,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;SAClB;QACD,OAAO,KAAK,CAAC;IACf,CAAC;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;OA2BG,CACH,IAAa,YAAY,CAAC,IAAa,EAAA;QACrC,0EAA0E;QAC1E,iCAAiC;QACjC,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,MAAM,wKAAI,aAAU,CAChB,oEAAoE,GACpE,iBAAiB,CAAC,CAAC;SACxB;QACD,IAAI,CAAC,KAAK,CAAC,YAAY,GAAG,IAAI,CAAC;IACjC,CAAC;IAED,IAAa,YAAY,GAAA;QACvB,IAAI,IAAI,CAAC,KAAK,IAAI,IAAI,EAAE;YACtB,MAAM,wKAAI,aAAU,CAChB,oEAAoE,GACpE,iBAAiB,CAAC,CAAC;SACxB;QACD,OAAO,IAAI,CAAC,KAAK,CAAC,YAAY,CAAC;IACjC,CAAC;IAED,mDAAmD;IAEnD,kCAAkC;IACzB,SAAS,GAAA;QAChB,wEAAwE;QACxE,uEAAuE;QACvE,oEAAoE;QACpE,WAAW;QACX,MAAM,MAAM,GAA+B,EAAE,CAAC;QAC9C,KAAK,MAAM,KAAK,IAAI,IAAI,CAAC,MAAM,CAAE;YAC/B,MAAM,IAAI,GAA6B,CAAA,CAAE,CAAC;YAC1C,IAAI,CAAC,WAAW,CAAC,GAAG,KAAK,CAAC,YAAY,EAAE,CAAC;YACzC,IAAI,CAAC,QAAQ,CAAC,GAAG,KAAK,CAAC,SAAS,EAAE,CAAC;YACnC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACnB;QACD,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,IAAI;YAAE,MAAM;QAAA,CAAC,CAAC;IACnC,CAAC;;AA1sBD,gBAAA,EAAkB,CACF,WAAA,SAAS,GAAG,YAAY,CAAC;;kNA2sB3C,gBAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3214, "column": 0}, "map": {"version":3,"file":"exports.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports.js/__/__/__/__/__/tfjs-layers/src/exports.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Exported functions.\n */\n\nimport {BaseCallbackConstructor, CallbackConstructorRegistry} from './base_callbacks';\nimport {ContainerArgs} from './engine/container';\nimport {Input, InputConfig,} from './engine/input_layer';\nimport {SymbolicTensor} from './engine/topology';\nimport {LayersModel} from './engine/training';\nimport {Sequential, SequentialArgs} from './models';\n\nexport {loadLayersModel} from './models';\n\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n\n// LayersModel and related factory methods.\n\n/**\n * A model is a data structure that consists of `Layers` and defines inputs\n * and outputs.\n *\n * The key difference between `tf.model` and `tf.sequential` is that\n * `tf.model` is more generic, supporting an arbitrary graph (without\n * cycles) of layers. `tf.sequential` is less generic and supports only a linear\n * stack of layers.\n *\n * When creating a `tf.LayersModel`, specify its input(s) and output(s). Layers\n * are used to wire input(s) to output(s).\n *\n * For example, the following code snippet defines a model consisting of\n * two `dense` layers, with 10 and 4 units, respectively.\n *\n * ```js\n * // Define input, which has a size of 5 (not including batch dimension).\n * const input = tf.input({shape: [5]});\n *\n * // First dense layer uses relu activation.\n * const denseLayer1 = tf.layers.dense({units: 10, activation: 'relu'});\n * // Second dense layer uses softmax activation.\n * const denseLayer2 = tf.layers.dense({units: 4, activation: 'softmax'});\n *\n * // Obtain the output symbolic tensor by applying the layers on the input.\n * const output = denseLayer2.apply(denseLayer1.apply(input));\n *\n * // Create the model based on the inputs.\n * const model = tf.model({inputs: input, outputs: output});\n *\n * // The model can be used for training, evaluation and prediction.\n * // For example, the following line runs prediction with the model on\n * // some fake data.\n * model.predict(tf.ones([2, 5])).print();\n * ```\n * See also:\n *   `tf.sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Creation'}\n */\nexport function model(args: ContainerArgs): LayersModel {\n  return new LayersModel(args);\n}\n\n/**\n * Creates a `tf.Sequential` model.  A sequential model is any model where the\n * outputs of one layer are the inputs to the next layer, i.e. the model\n * topology is a simple 'stack' of layers, with no branching or skipping.\n *\n * This means that the first layer passed to a `tf.Sequential` model should have\n * a defined input shape. What that means is that it should have received an\n * `inputShape` or `batchInputShape` argument, or for some type of layers\n * (recurrent, Dense...) an `inputDim` argument.\n *\n * The key difference between `tf.model` and `tf.sequential` is that\n * `tf.sequential` is less generic, supporting only a linear stack of layers.\n * `tf.model` is more generic and supports an arbitrary graph (without\n * cycles) of layers.\n *\n * Examples:\n *\n * ```js\n * const model = tf.sequential();\n *\n * // First layer must have an input shape defined.\n * model.add(tf.layers.dense({units: 32, inputShape: [50]}));\n * // Afterwards, TF.js does automatic shape inference.\n * model.add(tf.layers.dense({units: 4}));\n *\n * // Inspect the inferred shape of the model's output, which equals\n * // `[null, 4]`. The 1st dimension is the undetermined batch dimension; the\n * // 2nd is the output size of the model's last layer.\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * It is also possible to specify a batch size (with potentially undetermined\n * batch dimension, denoted by \"null\") for the first layer using the\n * `batchInputShape` key. The following example is equivalent to the above:\n *\n * ```js\n * const model = tf.sequential();\n *\n * // First layer must have a defined input shape\n * model.add(tf.layers.dense({units: 32, batchInputShape: [null, 50]}));\n * // Afterwards, TF.js does automatic shape inference.\n * model.add(tf.layers.dense({units: 4}));\n *\n * // Inspect the inferred shape of the model's output.\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * You can also use an `Array` of already-constructed `Layer`s to create\n * a `tf.Sequential` model:\n *\n * ```js\n * const model = tf.sequential({\n *   layers: [tf.layers.dense({units: 32, inputShape: [50]}),\n *            tf.layers.dense({units: 4})]\n * });\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * @doc {heading: 'Models', subheading: 'Creation'}\n */\nexport function sequential(config?: SequentialArgs): Sequential {\n  return new Sequential(config);\n}\n\n/**\n * Used to instantiate an input to a model as a `tf.SymbolicTensor`.\n *\n * Users should call the `input` factory function for\n * consistency with other generator functions.\n *\n * Example:\n *\n * ```js\n * // Defines a simple logistic regression model with 32 dimensional input\n * // and 3 dimensional output.\n * const x = tf.input({shape: [32]});\n * const y = tf.layers.dense({units: 3, activation: 'softmax'}).apply(x);\n * const model = tf.model({inputs: x, outputs: y});\n * model.predict(tf.ones([2, 32])).print();\n * ```\n *\n * Note: `input` is only necessary when using `model`. When using\n * `sequential`, specify `inputShape` for the first layer or use `inputLayer`\n * as the first layer.\n *\n * @doc {heading: 'Models', subheading: 'Inputs'}\n */\nexport function input(config: InputConfig): SymbolicTensor {\n  return Input(config);\n}\n\nexport function registerCallbackConstructor(\n    verbosityLevel: number,\n    callbackConstructor: BaseCallbackConstructor): void {\n  CallbackConstructorRegistry.registerCallbackConstructor(\n      verbosityLevel, callbackConstructor);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH;;GAEG;;;;;;AAEH,OAAO,EAA0B,2BAA2B,EAAC,MAAM,kBAAkB,CAAC;AAEtF,OAAO,EAAC,KAAK,GAAe,MAAM,sBAAsB,CAAC;AAEzD,OAAO,EAAC,WAAW,EAAC,MAAM,mBAAmB,CAAC;AAC9C,OAAO,EAAC,UAAU,EAAiB,MAAM,UAAU,CAAC;;;;;;AAkD9C,SAAU,KAAK,CAAC,IAAmB;IACvC,OAAO,oLAAI,cAAW,CAAC,IAAI,CAAC,CAAC;AAC/B,CAAC;AA8DK,SAAU,UAAU,CAAC,MAAuB;IAChD,OAAO,uKAAI,cAAU,CAAC,MAAM,CAAC,CAAC;AAChC,CAAC;AAyBK,SAAU,KAAK,CAAC,MAAmB;IACvC,8LAAO,QAAA,AAAK,EAAC,MAAM,CAAC,CAAC;AACvB,CAAC;AAEK,SAAU,2BAA2B,CACvC,cAAsB,EACtB,mBAA4C;gLAC9C,8BAA2B,CAAC,2BAA2B,CACnD,cAAc,EAAE,mBAAmB,CAAC,CAAC;AAC3C,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3269, "column": 0}, "map": {"version":3,"file":"activations.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/activations.js/__/__/__/__/__/tfjs-layers/src/activations.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport {ActivationIdentifier} from './keras_format/activation_config';\nimport {deserializeKerasObject} from './utils/generic_utils';\n\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport abstract class Activation extends serialization.Serializable {\n  abstract apply(tensor: Tensor, axis?: number): Tensor;\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'elu';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return K.elu(x, alpha);\n  }\n}\nserialization.registerClass(Elu);\n\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'selu';\n  apply(x: Tensor): Tensor {\n    return tfc.selu(x);\n  }\n}\nserialization.registerClass(Selu);\n\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu';\n  apply(x: Tensor): Tensor {\n    return tfc.relu(x);\n  }\n}\nserialization.registerClass(Relu);\n\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu6';\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n}\nserialization.registerClass(Relu6);\n\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n  /** @nocollapse */\n  static readonly className = 'linear';\n  apply(x: Tensor): Tensor {\n    return x;\n  }\n}\nserialization.registerClass(Linear);\n\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'sigmoid';\n  apply(x: Tensor): Tensor {\n    return tfc.sigmoid(x);\n  }\n}\nserialization.registerClass(Sigmoid);\n\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'hardSigmoid';\n  apply(x: Tensor): Tensor {\n    return K.hardSigmoid(x);\n  }\n}\nserialization.registerClass(HardSigmoid);\n\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softplus';\n  apply(x: Tensor): Tensor {\n    return tfc.softplus(x);\n  }\n}\nserialization.registerClass(Softplus);\n\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softsign';\n  apply(x: Tensor): Tensor {\n    return K.softsign(x);\n  }\n}\nserialization.registerClass(Softsign);\n\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n  /** @nocollapse */\n  static readonly className = 'tanh';\n  apply(x: Tensor): Tensor {\n    return tfc.tanh(x);\n  }\n}\nserialization.registerClass(Tanh);\n\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softmax';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.softmax(x, axis);\n  }\n}\nserialization.registerClass(Softmax);\n\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'logSoftmax';\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.logSoftmax(x, axis);\n  }\n}\nserialization.registerClass(LogSoftmax);\n\n/**\n * Gelu activation function\n */\nexport class Gelu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'gelu';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor): Tensor {\n    return tidy(() => {\n      return tfc.tidy(() => {\n        const sqrtTwo = Math.sqrt(2);\n        // Compute Φ(x) using the erf function\n        const cdf = tfc.mul(0.5, tfc.add(1, tfc.erf(tfc.div(x, sqrtTwo))));\n        // Compute GELU(x) = x * Φ(x)\n        return tfc.mul(x, cdf);\n      });\n    });\n  }\n}\nserialization.registerClass(Gelu);\n\n/**\n * GeluNew activation function\n */\nexport class GeluNew extends Activation {\n  /** @nocollapse */\n  static readonly className = 'gelu_new';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor): Tensor {\n    return tidy(() => {\n      return tfc.mul(\n        0.5,\n        tfc.mul(\n          x,\n          tfc.add(\n              1,\n              tfc.tanh(\n                tfc.mul(\n                  tfc.sqrt(tfc.div(2, Math.PI)),\n                  tfc.add(x, tfc.mul(0.044715, tfc.pow(x, 3)))\n                  )\n              )\n          )\n        )\n      );\n    });\n  }\n}\nserialization.registerClass(GeluNew);\n\n/**\n * Mish activation function\n */\nexport class Mish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'mish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n  }\n}\nserialization.registerClass(Mish);\n\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'swish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n  }\n}\nserialization.registerClass(Swish);\n\nexport function serializeActivation(activation: Activation): string {\n  return activation.getClassName();\n}\n\nexport function deserializeActivation(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Activation {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'activation');\n}\n\nexport function getActivation(identifier: ActivationIdentifier|\n                              serialization.ConfigDict|Activation): Activation {\n  if (identifier == null) {\n    const config: serialization.ConfigDict = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n  if (typeof identifier === 'string') {\n    const config: serialization.ConfigDict = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,6BAA6B;;;;;;;;;;;;;;;;;;;;;;;AAC7B,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;;;;;;;;;;AAC7C,OAAO,EAAC,aAAa,EAAU,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAClE,OAAO,KAAK,CAAC,MAAM,wBAAwB,CAAC;AAE5C,OAAO,EAAC,sBAAsB,EAAC,MAAM,uBAAuB,CAAC;;;;;AASvD,MAAgB,UAAW,2NAAQ,gBAAa,CAAC,YAAY;IAEjE,SAAS,GAAA;QACP,OAAO,CAAA,CAAE,CAAC;IACZ,CAAC;CACF;AAED;;;GAGG,CACH,MAAa,GAAI,SAAQ,UAAU;IAGjC;;;;;;OAMG,CACH,KAAK,CAAC,CAAS,EAAE,KAAK,GAAG,CAAC,EAAA;QACxB,OAAO,CAAC,CAAC,6LAAA,AAAG,EAAC,CAAC,EAAE,KAAK,CAAC,CAAC;IACzB,CAAC;;AAXD,gBAAA,EAAkB,CACF,IAAA,SAAS,GAAG,KAAK,CAAC;;kNAYpC,gBAAa,CAAC,aAAa,CAAC,GAAG,CAAC,CAAC;AAEjC;;;;;;GAMG,CACH,MAAa,IAAK,SAAQ,UAAU;IAGlC,KAAK,CAAC,CAAS,EAAA;QACb,iLAAO,GAAG,CAAC,IAAA,AAAI,EAAC,CAAC,CAAC,CAAC;IACrB,CAAC;;AAJD,gBAAA,EAAkB,CACF,KAAA,SAAS,GAAG,MAAM,CAAC;;kNAKrC,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG,CACH,MAAa,IAAK,SAAQ,UAAU;IAGlC,KAAK,CAAC,CAAS,EAAA;QACb,kLAAO,GAAG,CAAC,GAAA,AAAI,EAAC,CAAC,CAAC,CAAC;IACrB,CAAC;;AAJD,gBAAA,EAAkB,CACF,KAAA,SAAS,GAAG,MAAM,CAAC;;kNAKrC,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG,CACH,MAAa,KAAM,SAAQ,UAAU;IAGnC,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EAAC,GAAG,CAAG,CAAD,EAAI,CAAC,oLAAA,AAAO,EAAC,GAAG,6KAAE,GAAG,CAAC,GAAA,AAAI,EAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACnD,CAAC;;AAJD,gBAAA,EAAkB,CACF,MAAA,SAAS,GAAG,OAAO,CAAC;;kNAKtC,gBAAa,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC;AAEnC,gCAAgC;AAChC,MAAa,MAAO,SAAQ,UAAU;IAGpC,KAAK,CAAC,CAAS,EAAA;QACb,OAAO,CAAC,CAAC;IACX,CAAC;;AAJD,gBAAA,EAAkB,CACF,OAAA,SAAS,GAAG,QAAQ,CAAC;;kNAKvC,gBAAa,CAAC,aAAa,CAAC,MAAM,CAAC,CAAC;AAEpC;;GAEG,CACH,MAAa,OAAQ,SAAQ,UAAU;IAGrC,KAAK,CAAC,CAAS,EAAA;QACb,OAAO,GAAG,CAAC,oLAAA,AAAO,EAAC,CAAC,CAAC,CAAC;IACxB,CAAC;;AAJD,gBAAA,EAAkB,CACF,QAAA,SAAS,GAAG,SAAS,CAAC;;kNAKxC,gBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;GAEG,CACH,MAAa,WAAY,SAAQ,UAAU;IAGzC,KAAK,CAAC,CAAS,EAAA;QACb,gMAAO,CAAC,CAAC,YAAA,AAAW,EAAC,CAAC,CAAC,CAAC;IAC1B,CAAC;;AAJD,gBAAA,EAAkB,CACF,YAAA,SAAS,GAAG,aAAa,CAAC;;kNAK5C,gBAAa,CAAC,aAAa,CAAC,WAAW,CAAC,CAAC;AAEzC;;GAEG,CACH,MAAa,QAAS,SAAQ,UAAU;IAGtC,KAAK,CAAC,CAAS,EAAA;QACb,sLAAO,GAAG,CAAC,OAAA,AAAQ,EAAC,CAAC,CAAC,CAAC;IACzB,CAAC;;AAJD,gBAAA,EAAkB,CACF,SAAA,SAAS,GAAG,UAAU,CAAC;;kNAKzC,gBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;GAEG,CACH,MAAa,QAAS,SAAQ,UAAU;IAGtC,KAAK,CAAC,CAAS,EAAA;QACb,gMAAO,CAAC,CAAC,SAAA,AAAQ,EAAC,CAAC,CAAC,CAAC;IACvB,CAAC;;AAJD,gBAAA,EAAkB,CACF,SAAA,SAAS,GAAG,UAAU,CAAC;;kNAKzC,gBAAa,CAAC,aAAa,CAAC,QAAQ,CAAC,CAAC;AAEtC;;GAEG,CACH,MAAa,IAAK,SAAQ,UAAU;IAGlC,KAAK,CAAC,CAAS,EAAA;QACb,kLAAO,GAAG,CAAC,GAAA,AAAI,EAAC,CAAC,CAAC,CAAC;IACrB,CAAC;;AAJD,gBAAA,EAAkB,CACF,KAAA,SAAS,GAAG,MAAM,CAAC;;kNAKrC,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG,CACH,MAAa,OAAQ,SAAQ,UAAU;IAGrC;;;;;;;;;;;OAWG,CACH,KAAK,CAAC,CAAS,EAAE,OAAe,AAAC,CAAC,CAAC,AAAC,EAAA;QAClC,qLAAO,GAAG,CAAC,MAAA,AAAO,EAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IAC9B,CAAC;;AAhBD,gBAAA,EAAkB,CACF,QAAA,SAAS,GAAG,SAAS,CAAC;;kNAiBxC,gBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;GAEG,CACH,MAAa,UAAW,SAAQ,UAAU;IAGxC;;;;;;;;;;;;OAYG,CACH,KAAK,CAAC,CAAS,EAAE,OAAe,AAAC,CAAC,CAAE,AAAD,EAAC;QAClC,yLAAO,GAAG,CAAC,SAAA,AAAU,EAAC,CAAC,EAAE,IAAI,CAAC,CAAC;IACjC,CAAC;;AAjBD,gBAAA,EAAkB,CACF,WAAA,SAAS,GAAG,YAAY,CAAC;;kNAkB3C,gBAAa,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;AAExC;;GAEG,CACH,MAAa,IAAK,SAAQ,UAAU;IAGlC;;;;;OAKG,CACH,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;YACf,QAAO,GAAG,CAAC,yKAAA,AAAI,EAAC,GAAG,EAAE;gBACnB,MAAM,OAAO,GAAG,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;gBAC7B,sCAAsC;gBACtC,MAAM,GAAG,6KAAG,GAAG,CAAC,EAAA,AAAG,EAAC,GAAG,GAAE,GAAG,CAAC,2KAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;gBACnE,6BAA6B;gBAC7B,iLAAO,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,EAAE,GAAG,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC,CAAC,CAAC;IACL,CAAC;;AAlBD,gBAAA,EAAkB,CACF,KAAA,SAAS,GAAG,MAAM,CAAC;;kNAmBrC,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG,CACH,MAAa,OAAQ,SAAQ,UAAU;IAGrC;;;;;OAKG,CACH,KAAK,CAAC,CAAS,EAAA;QACb,OAAO,8KAAI,AAAJ,EAAK,GAAG,EAAE;YACf,iLAAO,GAAG,CAAC,EAAA,AAAG,EACZ,GAAG,4KACH,GAAG,CAAC,EAAA,AAAG,EACL,CAAC,MACD,GAAG,CAAC,wKAAA,AAAG,EACH,CAAC,6KACD,GAAG,CAAC,GAAA,AAAI,4KACN,GAAG,CAAC,EAAA,AAAG,6KACL,GAAG,CAAC,GAAA,AAAI,GAAC,GAAG,CAAC,2KAAA,AAAG,EAAC,CAAC,EAAE,IAAI,CAAC,EAAE,CAAC,CAAC,4KAC7B,GAAG,CAAC,EAAA,AAAG,EAAC,CAAC,4KAAE,GAAG,CAAC,EAAG,AAAH,EAAI,QAAQ,MAAE,GAAG,CAAC,wKAAA,AAAG,EAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAC3C,CACJ,CACJ,CACF,CACF,CAAC;QACJ,CAAC,CAAC,CAAC;IACL,CAAC;;AA1BD,gBAAA,EAAkB,CACF,QAAA,SAAS,GAAG,UAAU,CAAC;;kNA2BzC,gBAAa,CAAC,aAAa,CAAC,OAAO,CAAC,CAAC;AAErC;;GAEG,CACH,MAAa,IAAK,SAAQ,UAAU;IAGlC;;;;;OAKG,CACH,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAI,AAAJ,EAAK,GAAG,EAAE,AAAC,GAAG,CAAC,2KAAA,AAAG,EAAC,CAAC,6KAAE,GAAG,CAAC,GAAA,AAAI,iLAAC,GAAG,CAAC,OAAA,AAAQ,EAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC3D,CAAC;;AAVD,gBAAA,EAAkB,CACF,KAAA,SAAS,GAAG,MAAM,CAAC;;AAWrC,kOAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAElC;;GAEG,CACH,MAAa,KAAM,SAAQ,UAAU;IAGnC;;;;;;OAMG,CACH,KAAK,CAAC,CAAS,EAAE,KAAK,GAAG,CAAC,EAAA;QACxB,WAAO,0KAAA,AAAI,EAAC,GAAG,EAAE,yKAAC,GAAG,CAAC,EAAA,AAAG,gLAAC,GAAG,CAAC,MAAA,AAAO,4KAAC,GAAG,CAAC,EAAG,AAAH,EAAI,CAAC,EAAE,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAChE,CAAC;;AAXD,gBAAA,EAAkB,CACF,MAAA,SAAS,GAAG,OAAO,CAAC;;kNAYtC,gBAAa,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC;AAE7B,SAAU,mBAAmB,CAAC,UAAsB;IACxD,OAAO,UAAU,CAAC,YAAY,EAAE,CAAC;AACnC,CAAC;AAEK,SAAU,qBAAqB,CACjC,MAAgC,EAChC,gBAA0C,CAAA,CAAE;IAC9C,+LAAO,yBAAsB,AAAtB,EACH,MAAM,EAAE,kOAAa,CAAC,gBAAgB,CAAC,MAAM,EAAE,CAAC,YAAY,EAC5D,aAAa,EAAE,YAAY,CAAC,CAAC;AACnC,CAAC;AAEK,SAAU,aAAa,CAAC,UACmC;IAC/D,IAAI,UAAU,IAAI,IAAI,EAAE;QACtB,MAAM,MAAM,GAA6B,CAAA,CAAE,CAAC;QAC5C,MAAM,CAAC,WAAW,CAAC,GAAG,QAAQ,CAAC;QAC/B,MAAM,CAAC,QAAQ,CAAC,GAAG,CAAA,CAAE,CAAC;QACtB,OAAO,qBAAqB,CAAC,MAAM,CAAC,CAAC;KACtC;IACD,IAAI,OAAO,UAAU,KAAK,QAAQ,EAAE;QAClC,MAAM,MAAM,GAA6B,CAAA,CAAE,CAAC;QAC5C,MAAM,CAAC,WAAW,CAAC,GAAG,UAAU,CAAC;QACjC,MAAM,CAAC,QAAQ,CAAC,GAAG,CAAA,CAAE,CAAC;QACtB,OAAO,qBAAqB,CAAC,MAAM,CAAC,CAAC;KACtC,MAAM,IAAI,UAAU,YAAY,UAAU,EAAE;QAC3C,OAAO,UAAU,CAAC;KACnB,MAAM;QACL,OAAO,qBAAqB,CAAC,UAAU,CAAC,CAAC;KAC1C;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3582, "column": 0}, "map": {"version":3,"file":"regularizers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/regularizers.js/__/__/__/__/__/tfjs-layers/src/regularizers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* original source: keras/regularizers.py */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {abs, add, Scalar, serialization, sum, Tensor, tidy, zeros} from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport {deserializeKerasObject, serializeKerasObject} from './utils/generic_utils';\n\nfunction assertObjectArgs(args: L1Args|L2Args|L1L2Args): void {\n  if (args != null && typeof args !== 'object') {\n    throw new Error(\n        `Argument to L1L2 regularizer's constructor is expected to be an ` +\n        `object, but received: ${args}`);\n  }\n}\n\n/**\n * Regularizer base class.\n */\nexport abstract class Regularizer extends serialization.Serializable {\n  abstract apply(x: Tensor): Scalar;\n}\n\nexport interface L1L2Args {\n  /** L1 regularization rate. Defaults to 0.01. */\n  l1?: number;\n  /** L2 regularization rate. Defaults to 0.01. */\n  l2?: number;\n}\n\nexport interface L1Args {\n  /** L1 regularization rate. Defaults to 0.01. */\n  l1: number;\n}\n\nexport interface L2Args {\n  /** L2 regularization rate. Defaults to 0.01. */\n  l2: number;\n}\n\nexport class L1L2 extends Regularizer {\n  /** @nocollapse */\n  static className = 'L1L2';\n\n  private readonly l1: number;\n  private readonly l2: number;\n  private readonly hasL1: boolean;\n  private readonly hasL2: boolean;\n  constructor(args?: L1L2Args) {\n    super();\n\n    assertObjectArgs(args);\n\n    this.l1 = args == null || args.l1 == null ? 0.01 : args.l1;\n    this.l2 = args == null || args.l2 == null ? 0.01 : args.l2;\n    this.hasL1 = this.l1 !== 0;\n    this.hasL2 = this.l2 !== 0;\n  }\n\n  /**\n   * Porting note: Renamed from __call__.\n   * @param x Variable of which to calculate the regularization score.\n   */\n  apply(x: Tensor): Scalar {\n    return tidy(() => {\n      let regularization: Tensor = zeros([1]);\n      if (this.hasL1) {\n        regularization = add(regularization, sum(tfc.mul(this.l1, abs(x))));\n      }\n      if (this.hasL2) {\n        regularization =\n            add(regularization, sum(tfc.mul(this.l2, K.square(x))));\n      }\n      return tfc.reshape(regularization, []);\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    return {'l1': this.l1, 'l2': this.l2};\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    return new cls({l1: config['l1'] as number, l2: config['l2'] as number});\n  }\n}\nserialization.registerClass(L1L2);\n\nexport function l1(args?: L1Args) {\n  assertObjectArgs(args);\n  return new L1L2({l1: args != null ? args.l1 : null, l2: 0});\n}\n\nexport function l2(args: L2Args) {\n  assertObjectArgs(args);\n  return new L1L2({l2: args != null ? args.l2 : null, l1: 0});\n}\n\n/** @docinline */\nexport type RegularizerIdentifier = 'l1l2'|string;\n\n// Maps the JavaScript-like identifier keys to the corresponding keras symbols.\nexport const REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP:\n    {[identifier in RegularizerIdentifier]: string} = {\n      'l1l2': 'L1L2'\n    };\n\nexport function serializeRegularizer(constraint: Regularizer):\n    serialization.ConfigDictValue {\n  return serializeKerasObject(constraint);\n}\n\nexport function deserializeRegularizer(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Regularizer {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'regularizer');\n}\n\nexport function getRegularizer(identifier: RegularizerIdentifier|\n                               serialization.ConfigDict|\n                               Regularizer): Regularizer {\n  if (identifier == null) {\n    return null;\n  }\n  if (typeof identifier === 'string') {\n    const className = identifier in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ?\n        REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[identifier] :\n        identifier;\n    const config = {className, config: {}};\n    return deserializeRegularizer(config);\n  } else if (identifier instanceof Regularizer) {\n    return identifier;\n  } else {\n    return deserializeRegularizer(identifier);\n  }\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,0CAAA,EAA4C;;;;;;;;;;AAE5C,OAAO,KAAK,GAAG,MAAM,uBAAuB,CAAC;;;;;;;;AAC7C,OAAO,EAAC,GAAG,EAAE,GAAG,EAAU,aAAa,EAAE,GAAG,EAAU,IAAI,EAAE,KAAK,EAAC,MAAM,uBAAuB,CAAC;AAChG,OAAO,KAAK,CAAC,MAAM,wBAAwB,CAAC;AAC5C,OAAO,EAAC,sBAAsB,EAAE,oBAAoB,EAAC,MAAM,uBAAuB,CAAC;;;;;AAEnF,SAAS,gBAAgB,CAAC,IAA4B;IACpD,IAAI,IAAI,IAAI,IAAI,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;QAC5C,MAAM,IAAI,KAAK,CACX,CAAA,gEAAA,CAAkE,GAClE,CAAA,sBAAA,EAAyB,IAAI,EAAE,CAAC,CAAC;KACtC;AACH,CAAC;AAKK,MAAgB,WAAY,2NAAQ,gBAAa,CAAC,YAAY;CAEnE;AAmBD,MAAa,IAAK,SAAQ,WAAW;IAQnC,YAAY,IAAe,CAAA;QACzB,KAAK,EAAE,CAAC;QAER,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAEvB,IAAI,CAAC,EAAE,GAAG,IAAI,IAAI,IAAI,IAAI,IAAI,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC;QAC3D,IAAI,CAAC,EAAE,GAAG,IAAI,IAAI,IAAI,IAAI,IAAI,CAAC,EAAE,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC;QAC3D,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,EAAE,KAAK,CAAC,CAAC;QAC3B,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,EAAE,KAAK,CAAC,CAAC;IAC7B,CAAC;IAED;;;OAGG,CACH,KAAK,CAAC,CAAS,EAAA;QACb,8KAAO,OAAA,AAAI,EAAC,GAAG,EAAE;YACf,IAAI,cAAc,+KAAW,QAAA,AAAK,EAAC;gBAAC,CAAC;aAAC,CAAC,CAAC;YACxC,IAAI,IAAI,CAAC,KAAK,EAAE;gBACd,cAAc,6KAAG,MAAA,AAAG,EAAC,cAAc,4KAAE,MAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,IAAI,CAAC,EAAE,4KAAE,MAAA,AAAG,EAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;aACrE;YACD,IAAI,IAAI,CAAC,KAAK,EAAE;gBACd,cAAc,6KACV,MAAA,AAAG,EAAC,cAAc,EAAE,gLAAA,AAAG,4KAAC,GAAG,CAAC,EAAA,AAAG,EAAC,IAAI,CAAC,EAAE,GAAE,CAAC,CAAC,+LAAA,AAAM,EAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;aAC7D;YACD,qLAAO,GAAG,CAAC,MAAA,AAAO,EAAC,cAAc,EAAE,EAAE,CAAC,CAAC;QACzC,CAAC,CAAC,CAAC;IACL,CAAC;IAED,SAAS,GAAA;QACP,OAAO;YAAC,IAAI,EAAE,IAAI,CAAC,EAAE;YAAE,IAAI,EAAE,IAAI,CAAC,EAAE;QAAA,CAAC,CAAC;IACxC,CAAC;IAED,gBAAA,EAAkB,CAClB,MAAM,CAAU,UAAU,CACtB,GAA6C,EAC7C,MAAgC,EAAA;QAClC,OAAO,IAAI,GAAG,CAAC;YAAC,EAAE,EAAE,MAAM,CAAC,IAAI,CAAW;YAAE,EAAE,EAAE,MAAM,CAAC,IAAI,CAAW;QAAA,CAAC,CAAC,CAAC;IAC3E,CAAC;;AA7CD,gBAAA,EAAkB,CACX,KAAA,SAAS,GAAG,MAAM,CAAC;;kNA8C5B,gBAAa,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC;AAE5B,SAAU,EAAE,CAAC,IAAa;IAC9B,gBAAgB,CAAC,IAAI,CAAC,CAAC;IACvB,OAAO,IAAI,IAAI,CAAC;QAAC,EAAE,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI;QAAE,EAAE,EAAE,CAAC;IAAA,CAAC,CAAC,CAAC;AAC9D,CAAC;AAEK,SAAU,EAAE,CAAC,IAAY;IAC7B,gBAAgB,CAAC,IAAI,CAAC,CAAC;IACvB,OAAO,IAAI,IAAI,CAAC;QAAC,EAAE,EAAE,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI;QAAE,EAAE,EAAE,CAAC;IAAA,CAAC,CAAC,CAAC;AAC9D,CAAC;AAMM,MAAM,0CAA0C,GACD;IAChD,MAAM,EAAE,MAAM;CACf,CAAC;AAEA,SAAU,oBAAoB,CAAC,UAAuB;IAE1D,+LAAO,uBAAA,AAAoB,EAAC,UAAU,CAAC,CAAC;AAC1C,CAAC;AAEK,SAAU,sBAAsB,CAClC,MAAgC,EAChC,gBAA0C,CAAA,CAAE;IAC9C,+LAAO,yBAAA,AAAsB,EACzB,MAAM,EAAE,kOAAa,CAAC,gBAAgB,CAAC,MAAM,EAAE,CAAC,YAAY,EAC5D,aAAa,EAAE,aAAa,CAAC,CAAC;AACpC,CAAC;AAEK,SAAU,cAAc,CAAC,UAEW;IACxC,IAAI,UAAU,IAAI,IAAI,EAAE;QACtB,OAAO,IAAI,CAAC;KACb;IACD,IAAI,OAAO,UAAU,KAAK,QAAQ,EAAE;QAClC,MAAM,SAAS,GAAG,UAAU,IAAI,0CAA0C,CAAC,CAAC,CACxE,0CAA0C,CAAC,UAAU,CAAC,CAAC,CAAC,CACxD,UAAU,CAAC;QACf,MAAM,MAAM,GAAG;YAAC,SAAS;YAAE,MAAM,EAAE,CAAA,CAAE;QAAA,CAAC,CAAC;QACvC,OAAO,sBAAsB,CAAC,MAAM,CAAC,CAAC;KACvC,MAAM,IAAI,UAAU,YAAY,WAAW,EAAE;QAC5C,OAAO,UAAU,CAAC;KACnB,MAAM;QACL,OAAO,sBAAsB,CAAC,UAAU,CAAC,CAAC;KAC3C;AACH,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3710, "column": 0}, "map": {"version":3,"file":"exports_layers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_layers.js/__/__/__/__/__/tfjs-layers/src/exports_layers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {InputLayer, InputLayerArgs} from './engine/input_layer';\nimport {Layer, LayerArgs} from './engine/topology';\nimport {input} from './exports';\nimport {ELU, ELULayerArgs, LeakyReLU, LeakyReLULayerArgs, PReLU, PReLULayerArgs, ReLU, ReLULayerArgs, Softmax, SoftmaxLayerArgs, ThresholdedReLU, ThresholdedReLULayerArgs} from './layers/advanced_activations';\nimport {Conv1D, Conv2D, Conv2DTranspose, Conv3D, ConvLayerArgs, Cropping2D, Cropping2DLayerArgs, SeparableConv2D, SeparableConvLayerArgs, UpSampling2D, UpSampling2DLayerArgs, Conv3DTranspose} from './layers/convolutional';\nimport {DepthwiseConv2D, DepthwiseConv2DLayerArgs} from './layers/convolutional_depthwise';\nimport {ConvLSTM2D, ConvLSTM2DArgs, ConvLSTM2DCell, ConvLSTM2DCellArgs} from './layers/convolutional_recurrent';\nimport {Activation, ActivationLayerArgs, Dense, DenseLayerArgs, Dropout, DropoutLayerArgs, Flatten, FlattenLayerArgs, Masking, MaskingArgs, Permute, PermuteLayerArgs, RepeatVector, RepeatVectorLayerArgs, Reshape, ReshapeLayerArgs, SpatialDropout1D, SpatialDropout1DLayerConfig} from './layers/core';\nimport {Embedding, EmbeddingLayerArgs} from './layers/embeddings';\nimport {Add, Average, Concatenate, ConcatenateLayerArgs, Dot, DotLayerArgs, Maximum, Minimum, Multiply} from './layers/merge';\nimport {AlphaDropout, AlphaDropoutArgs, GaussianDropout, GaussianDropoutArgs, GaussianNoise, GaussianNoiseArgs} from './layers/noise';\nimport {BatchNormalization, BatchNormalizationLayerArgs, LayerNormalization, LayerNormalizationLayerArgs} from './layers/normalization';\nimport {ZeroPadding2D, ZeroPadding2DLayerArgs} from './layers/padding';\nimport {AveragePooling1D, AveragePooling2D, AveragePooling3D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalPooling2DLayerArgs, MaxPooling1D, MaxPooling2D, MaxPooling3D, Pooling1DLayerArgs, Pooling2DLayerArgs, Pooling3DLayerArgs} from './layers/pooling';\nimport {GRU, GRUCell, GRUCellLayerArgs, GRULayerArgs, LSTM, LSTMCell, LSTMCellLayerArgs, LSTMLayerArgs, RNN, RNNCell, RNNLayerArgs, SimpleRNN, SimpleRNNCell, SimpleRNNCellLayerArgs, SimpleRNNLayerArgs, StackedRNNCells, StackedRNNCellsArgs} from './layers/recurrent';\nimport {Bidirectional, BidirectionalLayerArgs, TimeDistributed, WrapperLayerArgs} from './layers/wrappers';\nimport {Rescaling, RescalingArgs} from './layers/preprocessing/image_preprocessing';\nimport {CenterCrop, CenterCropArgs} from './layers/preprocessing/center_crop';\nimport {CategoryEncoding, CategoryEncodingArgs} from './layers/preprocessing/category_encoding';\nimport {Resizing, ResizingArgs} from './layers/preprocessing/image_resizing';\nimport {RandomWidth, RandomWidthArgs} from './layers/preprocessing/random_width';\n\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n\n// Input Layer.\n/**\n * An input layer is an entry point into a `tf.LayersModel`.\n *\n * `InputLayer` is generated automatically for `tf.Sequential` models by\n * specifying the `inputshape` or `batchInputShape` for the first layer.  It\n * should not be specified explicitly. However, it can be useful sometimes,\n * e.g., when constructing a sequential model from a subset of another\n * sequential model's layers. Like the code snippet below shows.\n *\n * ```js\n * // Define a model which simply adds two inputs.\n * const model1 = tf.sequential();\n * model1.add(tf.layers.dense({inputShape: [4], units: 3, activation: 'relu'}));\n * model1.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n * model1.summary();\n * model1.predict(tf.zeros([1, 4])).print();\n *\n * // Construct another model, reusing the second layer of `model1` while\n * // not using the first layer of `model1`. Note that you cannot add the second\n * // layer of `model` directly as the first layer of the new sequential model,\n * // because doing so will lead to an error related to the fact that the layer\n * // is not an input layer. Instead, you need to create an `inputLayer` and add\n * // it to the new sequential model before adding the reused layer.\n * const model2 = tf.sequential();\n * // Use an inputShape that matches the input shape of `model1`'s second\n * // layer.\n * model2.add(tf.layers.inputLayer({inputShape: [3]}));\n * model2.add(model1.layers[1]);\n * model2.summary();\n * model2.predict(tf.zeros([1, 3])).print();\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Inputs', namespace: 'layers'}\n */\nexport function inputLayer(args: InputLayerArgs) {\n  return new InputLayer(args);\n}\n\n// Advanced Activation Layers.\n\n/**\n * Exponential Linear Unit (ELU).\n *\n * It follows:\n * `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n * `f(x) = x for x >= 0`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n * (ELUs)](https://arxiv.org/abs/1511.07289v1)\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function elu(args?: ELULayerArgs) {\n  return new ELU(args);\n}\n\n/**\n * Rectified Linear Unit activation function.\n *\n * Input shape:\n *   Arbitrary. Use the config field `inputShape` (Array of integers, does\n *   not include the sample axis) when using this layer as the first layer\n *   in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function reLU(args?: ReLULayerArgs) {\n  return new ReLU(args);\n}\n\n/**\n * Leaky version of a rectified linear unit.\n *\n * It allows a small gradient when the unit is not active:\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function leakyReLU(args?: LeakyReLULayerArgs) {\n  return new LeakyReLU(args);\n}\n\n/**\n * Parameterized version of a leaky rectified linear unit.\n *\n * It follows\n * `f(x) = alpha * x for x < 0.`\n * `f(x) = x for x >= 0.`\n * wherein `alpha` is a trainable weight.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function prelu(args?: PReLULayerArgs) {\n  return new PReLU(args);\n}\n\n/**\n * Softmax activation layer.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function softmax(args?: SoftmaxLayerArgs) {\n  return new Softmax(args);\n}\n\n/**\n * Thresholded Rectified Linear Unit.\n *\n * It follows:\n * `f(x) = x for x > theta`,\n * `f(x) = 0 otherwise`.\n *\n * Input shape:\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n * Output shape:\n *   Same shape as the input.\n *\n * References:\n *   - [Zero-Bias Autoencoders and the Benefits of Co-Adapting\n * Features](http://arxiv.org/abs/1402.3337)\n *\n * @doc {\n *   heading: 'Layers',\n *   subheading: 'Advanced Activation',\n *   namespace: 'layers'\n * }\n */\nexport function thresholdedReLU(args?: ThresholdedReLULayerArgs) {\n  return new ThresholdedReLU(args);\n}\n\n// Convolutional Layers.\n\n/**\n * 1D convolution layer (e.g., temporal convolution).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input over a single spatial (or temporal) dimension\n * to produce a tensor of outputs.\n *\n * If `use_bias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model, provide an\n * `inputShape` argument `Array` or `null`.\n *\n * For example, `inputShape` would be:\n * - `[10, 128]` for sequences of 10 vectors of 128-dimensional vectors\n * - `[null, 128]` for variable-length sequences of 128-dimensional vectors.\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional',  namespace: 'layers'}\n */\nexport function conv1d(args: ConvLayerArgs) {\n  return new Conv1D(args);\n}\n\n/**\n * 2D convolution layer (e.g. spatial convolution over images).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 3]` for 128x128 RGB pictures\n * in `dataFormat='channelsLast'`.\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv2d(args: ConvLayerArgs) {\n  return new Conv2D(args);\n}\n\n/**\n * Transposed convolutional layer (sometimes called Deconvolution).\n *\n * The need for transposed convolutions generally arises\n * from the desire to use a transformation going in the opposite direction of\n * a normal convolution, i.e., from something that has the shape of the output\n * of some convolution to something that has the shape of its input while\n * maintaining a connectivity pattern that is compatible with said\n * convolution.\n *\n * When using this layer as the first layer in a model, provide the\n * configuration `inputShape` (`Array` of integers, does not include the\n * sample axis), e.g., `inputShape: [128, 128, 3]` for 128x128 RGB pictures in\n * `dataFormat: 'channelsLast'`.\n *\n * Input shape:\n *   4D tensor with shape:\n *   `[batch, channels, rows, cols]` if `dataFormat` is `'channelsFirst'`.\n *   or 4D tensor with shape\n *   `[batch, rows, cols, channels]` if `dataFormat` is `'channelsLast'`.\n *\n * Output shape:\n *   4D tensor with shape:\n *   `[batch, filters, newRows, newCols]` if `dataFormat` is\n * `'channelsFirst'`. or 4D tensor with shape:\n *   `[batch, newRows, newCols, filters]` if `dataFormat` is `'channelsLast'`.\n *\n * References:\n *   - [A guide to convolution arithmetic for deep\n * learning](https://arxiv.org/abs/1603.07285v1)\n *   - [Deconvolutional\n * Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv2dTranspose(args: ConvLayerArgs) {\n  return new Conv2DTranspose(args);\n}\n\n/**\n * 3D convolution layer (e.g. spatial convolution over volumes).\n *\n * This layer creates a convolution kernel that is convolved\n * with the layer input to produce a tensor of outputs.\n *\n * If `useBias` is True, a bias vector is created and added to the outputs.\n *\n * If `activation` is not `null`, it is applied to the outputs as well.\n *\n * When using this layer as the first layer in a model,\n * provide the keyword argument `inputShape`\n * (Array of integers, does not include the sample axis),\n * e.g. `inputShape=[128, 128, 128, 1]` for 128x128x128 grayscale volumes\n * in `dataFormat='channelsLast'`.\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function conv3d(args: ConvLayerArgs) {\n  return new Conv3D(args);\n}\n\nexport function conv3dTranspose(args: ConvLayerArgs): Layer {\n  return new Conv3DTranspose(args);\n}\n\n/**\n * Depthwise separable 2D convolution.\n *\n * Separable convolution consists of first performing\n * a depthwise spatial convolution\n * (which acts on each input channel separately)\n * followed by a pointwise convolution which mixes together the resulting\n * output channels. The `depthMultiplier` argument controls how many\n * output channels are generated per input channel in the depthwise step.\n *\n * Intuitively, separable convolutions can be understood as\n * a way to factorize a convolution kernel into two smaller kernels,\n * or as an extreme version of an Inception block.\n *\n * Input shape:\n *   4D tensor with shape:\n *     `[batch, channels, rows, cols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, rows, cols, channels]` if data_format='channelsLast'.\n *\n * Output shape:\n *   4D tensor with shape:\n *     `[batch, filters, newRows, newCols]` if data_format='channelsFirst'\n *   or 4D tensor with shape:\n *     `[batch, newRows, newCols, filters]` if data_format='channelsLast'.\n *     `rows` and `cols` values might have changed due to padding.\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function separableConv2d(args: SeparableConvLayerArgs) {\n  return new SeparableConv2D(args);\n}\n\n/**\n * Cropping layer for 2D input (e.g., image).\n *\n * This layer can crop an input\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, croppedRows, croppedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, croppedRows, croppedCols]`.\n *\n * Examples\n * ```js\n *\n * const model = tf.sequential();\n * model.add(tf.layers.cropping2D({cropping:[[2, 2], [2, 2]],\n *                                inputShape: [128, 128, 3]}));\n * //now output shape is [batch, 124, 124, 3]\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function cropping2D(args: Cropping2DLayerArgs) {\n  return new Cropping2D(args);\n}\n\n/**\n * Upsampling layer for 2D inputs.\n *\n * Repeats the rows and columns of the data\n * by size[0] and size[1] respectively.\n *\n *\n * Input shape:\n *    4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *         `[batch, rows, cols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *        `[batch, channels, rows, cols]`\n *\n * Output shape:\n *     4D tensor with shape:\n *     - If `dataFormat` is `\"channelsLast\"`:\n *        `[batch, upsampledRows, upsampledCols, channels]`\n *     - If `dataFormat` is `\"channelsFirst\"`:\n *         `[batch, channels, upsampledRows, upsampledCols]`\n *\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function upSampling2d(args: UpSampling2DLayerArgs) {\n  return new UpSampling2D(args);\n}\n\n// Convolutional(depthwise) Layers.\n\n/**\n * Depthwise separable 2D convolution.\n *\n * Depthwise Separable convolutions consists in performing just the first step\n * in a depthwise spatial convolution (which acts on each input channel\n * separately). The `depthMultiplier` argument controls how many output channels\n * are generated per input channel in the depthwise step.\n *\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\n */\nexport function depthwiseConv2d(args: DepthwiseConv2DLayerArgs) {\n  return new DepthwiseConv2D(args);\n}\n\n// Basic Layers.\n\n/**\n * Applies an activation function to an output.\n *\n * This layer applies element-wise activation function.  Other layers, notably\n * `dense` can also apply activation functions.  Use this isolated activation\n * function to extract the values before and after the\n * activation. For instance:\n *\n * ```js\n * const input = tf.input({shape: [5]});\n * const denseLayer = tf.layers.dense({units: 1});\n * const activationLayer = tf.layers.activation({activation: 'relu6'});\n *\n * // Obtain the output symbolic tensors by applying the layers in order.\n * const denseOutput = denseLayer.apply(input);\n * const activationOutput = activationLayer.apply(denseOutput);\n *\n * // Create the model based on the inputs.\n * const model = tf.model({\n *     inputs: input,\n *     outputs: [denseOutput, activationOutput]\n * });\n *\n * // Collect both outputs and print separately.\n * const [denseOut, activationOut] = model.predict(tf.randomNormal([6, 5]));\n * denseOut.print();\n * activationOut.print();\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function activation(args: ActivationLayerArgs) {\n  return new Activation(args);\n}\n\n/**\n * Creates a dense (fully connected) layer.\n *\n * This layer implements the operation:\n *   `output = activation(dot(input, kernel) + bias)`\n *\n * `activation` is the element-wise activation function\n *   passed as the `activation` argument.\n *\n * `kernel` is a weights matrix created by the layer.\n *\n * `bias` is a bias vector created by the layer (only applicable if `useBias`\n * is `true`).\n *\n * **Input shape:**\n *\n *   nD `tf.Tensor` with shape: `(batchSize, ..., inputDim)`.\n *\n *   The most common situation would be\n *   a 2D input with shape `(batchSize, inputDim)`.\n *\n * **Output shape:**\n *\n *   nD tensor with shape: `(batchSize, ..., units)`.\n *\n *   For instance, for a 2D input with shape `(batchSize, inputDim)`,\n *   the output would have shape `(batchSize, units)`.\n *\n * Note: if the input to the layer has a rank greater than 2, then it is\n * flattened prior to the initial dot product with the kernel.\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function dense(args: DenseLayerArgs) {\n  return new Dense(args);\n}\n\n/**\n * Applies\n * [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) to\n * the input.\n *\n * Dropout consists in randomly setting a fraction `rate` of input units to 0 at\n * each update during training time, which helps prevent overfitting.\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function dropout(args: DropoutLayerArgs) {\n  return new Dropout(args);\n}\n\n/**\n * Spatial 1D version of Dropout.\n *\n * This Layer type performs the same function as the Dropout layer, but it drops\n * entire 1D feature maps instead of individual elements. For example, if an\n * input example consists of 3 timesteps and the feature map for each timestep\n * has a size of 4, a `spatialDropout1d` layer may zero out the feature maps\n * of the 1st timesteps and 2nd timesteps completely while sparing all feature\n * elements of the 3rd timestep.\n *\n * If adjacent frames (timesteps) are strongly correlated (as is normally the\n * case in early convolution layers), regular dropout will not regularize the\n * activation and will otherwise just result in merely an effective learning\n * rate decrease. In this case, `spatialDropout1d` will help promote\n * independence among feature maps and should be used instead.\n *\n * **Arguments:**\n *   rate: A floating-point number >=0 and <=1. Fraction of the input elements\n *     to drop.\n *\n * **Input shape:**\n *   3D tensor with shape `(samples, timesteps, channels)`.\n *\n * **Output shape:**\n *   Same as the input shape.\n *\n * References:\n *   - [Efficient Object Localization Using Convolutional\n *      Networks](https://arxiv.org/abs/1411.4280)\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function spatialDropout1d(args: SpatialDropout1DLayerConfig) {\n  return new SpatialDropout1D(args);\n}\n\n/**\n * Flattens the input. Does not affect the batch size.\n *\n * A `Flatten` layer flattens each batch in its inputs to 1D (making the output\n * 2D).\n *\n * For example:\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const flattenLayer = tf.layers.flatten();\n * // Inspect the inferred output shape of the flatten layer, which\n * // equals `[null, 12]`. The 2nd dimension is 4 * 3, i.e., the result of the\n * // flattening. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(flattenLayer.apply(input).shape));\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function flatten(args?: FlattenLayerArgs) {\n  return new Flatten(args);\n}\n\n/**\n * Repeats the input n times in a new dimension.\n *\n * ```js\n *  const model = tf.sequential();\n *  model.add(tf.layers.repeatVector({n: 4, inputShape: [2]}));\n *  const x = tf.tensor2d([[10, 20]]);\n *  // Use the model to do inference on a data point the model hasn't seen\n *  model.predict(x).print();\n *  // output shape is now [batch, 2, 4]\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function repeatVector(args: RepeatVectorLayerArgs) {\n  return new RepeatVector(args);\n}\n\n/**\n * Reshapes an input to a certain shape.\n *\n * ```js\n * const input = tf.input({shape: [4, 3]});\n * const reshapeLayer = tf.layers.reshape({targetShape: [2, 6]});\n * // Inspect the inferred output shape of the Reshape layer, which\n * // equals `[null, 2, 6]`. (The 1st dimension is the undermined batch size.)\n * console.log(JSON.stringify(reshapeLayer.apply(input).shape));\n * ```\n *\n * Input shape:\n *   Arbitrary, although all dimensions in the input shape must be fixed.\n *   Use the configuration `inputShape` when using this layer as the\n *   first layer in a model.\n *\n *\n * Output shape:\n *   [batchSize, targetShape[0], targetShape[1], ...,\n *    targetShape[targetShape.length - 1]].\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function reshape(args: ReshapeLayerArgs) {\n  return new Reshape(args);\n}\n\n/**\n * Permutes the dimensions of the input according to a given pattern.\n *\n * Useful for, e.g., connecting RNNs and convnets together.\n *\n * Example:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.permute({\n *   dims: [2, 1],\n *   inputShape: [10, 64]\n * }));\n * console.log(model.outputShape);\n * // Now model's output shape is [null, 64, 10], where null is the\n * // unpermuted sample (batch) dimension.\n * ```\n *\n * Input shape:\n *   Arbitrary. Use the configuration field `inputShape` when using this\n *   layer as the first layer in a model.\n *\n * Output shape:\n *   Same rank as the input shape, but with the dimensions re-ordered (i.e.,\n *   permuted) according to the `dims` configuration of this layer.\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function permute(args: PermuteLayerArgs) {\n  return new Permute(args);\n}\n\n/**\n * Maps positive integers (indices) into dense vectors of fixed size.\n * E.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n *\n * **Input shape:** 2D tensor with shape: `[batchSize, sequenceLength]`.\n *\n * **Output shape:** 3D tensor with shape: `[batchSize, sequenceLength,\n * outputDim]`.\n *\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\n */\nexport function embedding(args: EmbeddingLayerArgs) {\n  return new Embedding(args);\n}\n\n// Merge Layers.\n\n/**\n * Layer that performs element-wise addition on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). The inputs are specified as an\n * `Array` when the `apply` method of the `Add` layer instance is called. For\n * example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const addLayer = tf.layers.add();\n * const sum = addLayer.apply([input1, input2]);\n * console.log(JSON.stringify(sum.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function add(args?: LayerArgs) {\n  return new Add(args);\n}\n\n/**\n * Layer that performs element-wise averaging on an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const averageLayer = tf.layers.average();\n * const average = averageLayer.apply([input1, input2]);\n * console.log(JSON.stringify(average.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function average(args?: LayerArgs) {\n  return new Average(args);\n}\n\n/**\n * Layer that concatenates an `Array` of inputs.\n *\n * It takes a list of tensors, all of the same shape except for the\n * concatenation axis, and returns a single tensor, the concatenation\n * of all inputs. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 3]});\n * const concatLayer = tf.layers.concatenate();\n * const output = concatLayer.apply([input1, input2]);\n * console.log(JSON.stringify(output.shape));\n * // You get [null, 2, 5], with the first dimension as the undetermined batch\n * // dimension. The last dimension (5) is the result of concatenating the\n * // last dimensions of the inputs (2 and 3).\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function concatenate(args?: ConcatenateLayerArgs) {\n  return new Concatenate(args);\n}\n\n/**\n * Layer that computes the element-wise maximum of an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const maxLayer = tf.layers.maximum();\n * const max = maxLayer.apply([input1, input2]);\n * console.log(JSON.stringify(max.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function maximum(args?: LayerArgs) {\n  return new Maximum(args);\n}\n\n/**\n * Layer that computes the element-wise minimum of an `Array` of inputs.\n *\n * It takes as input a list of tensors, all of the same shape, and returns a\n * single tensor (also of the same shape). For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const minLayer = tf.layers.minimum();\n * const min = minLayer.apply([input1, input2]);\n * console.log(JSON.stringify(min.shape));\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function minimum(args?: LayerArgs) {\n  return new Minimum(args);\n}\n\n/**\n * Layer that multiplies (element-wise) an `Array` of inputs.\n *\n * It takes as input an Array of tensors, all of the same\n * shape, and returns a single tensor (also of the same shape).\n * For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const input3 = tf.input({shape: [2, 2]});\n * const multiplyLayer = tf.layers.multiply();\n * const product = multiplyLayer.apply([input1, input2, input3]);\n * console.log(product.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function multiply(args?: LayerArgs) {\n  return new Multiply(args);\n}\n\n/**\n * Layer that computes a dot product between samples in two tensors.\n *\n * E.g., if applied to a list of two tensors `a` and `b` both of shape\n * `[batchSize, n]`, the output will be a tensor of shape `[batchSize, 1]`,\n * where each entry at index `[i, 0]` will be the dot product between\n * `a[i, :]` and `b[i, :]`.\n *\n * Example:\n *\n * ```js\n * const dotLayer = tf.layers.dot({axes: -1});\n * const x1 = tf.tensor2d([[10, 20], [30, 40]]);\n * const x2 = tf.tensor2d([[-1, -2], [-3, -4]]);\n *\n * // Invoke the layer's apply() method in eager (imperative) mode.\n * const y = dotLayer.apply([x1, x2]);\n * y.print();\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\n */\nexport function dot(args: DotLayerArgs) {\n  return new Dot(args);\n}\n\n// Normalization Layers.\n\n/**\n * Batch normalization layer (Ioffe and Szegedy, 2014).\n *\n * Normalize the activations of the previous layer at each batch,\n * i.e. applies a transformation that maintains the mean activation\n * close to 0 and the activation standard deviation close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape` (Array of integers, does\n *   not include the sample axis) when calling the constructor of this class,\n *   if this layer is used as a first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Batch Normalization: Accelerating Deep Network Training by Reducing\n * Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n *\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nexport function batchNormalization(args?: BatchNormalizationLayerArgs) {\n  return new BatchNormalization(args);\n}\n\n/**\n * Layer-normalization layer (Ba et al., 2016).\n *\n * Normalizes the activations of the previous layer for each given example in a\n * batch independently, instead of across a batch like in `batchNormalization`.\n * In other words, this layer applies a transformation that maintains the mean\n * activation within each example close to 0 and activation variance close to 1.\n *\n * Input shape:\n *   Arbitrary. Use the argument `inputShape` when using this layer as the first\n *   layer in a model.\n *\n * Output shape:\n *   Same as input.\n *\n * References:\n *   - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n *\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\n */\nexport function layerNormalization(args?: LayerNormalizationLayerArgs) {\n  return new LayerNormalization(args);\n}\n\n// Padding Layers.\n\n/**\n * Zero-padding layer for 2D input (e.g., image).\n *\n * This layer can add rows and columns of zeros\n * at the top, bottom, left and right side of an image tensor.\n *\n * Input shape:\n *   4D tensor with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, rows, cols, channels]`\n *   - If `data_format` is `\"channels_first\"`:\n *     `[batch, channels, rows, cols]`.\n *\n * Output shape:\n *   4D with shape:\n *   - If `dataFormat` is `\"channelsLast\"`:\n *     `[batch, paddedRows, paddedCols, channels]`\n *    - If `dataFormat` is `\"channelsFirst\"`:\n *     `[batch, channels, paddedRows, paddedCols]`.\n *\n * @doc {heading: 'Layers', subheading: 'Padding', namespace: 'layers'}\n */\nexport function zeroPadding2d(args?: ZeroPadding2DLayerArgs) {\n  return new ZeroPadding2D(args);\n}\n\n// Pooling Layers.\n\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape: `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n *\n * `tf.avgPool1d` is an alias.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function averagePooling1d(args: Pooling1DLayerArgs) {\n  return new AveragePooling1D(args);\n}\nexport function avgPool1d(args: Pooling1DLayerArgs) {\n  return averagePooling1d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling1d(args: Pooling1DLayerArgs) {\n  return averagePooling1d(args);\n}\n\n/**\n * Average pooling operation for spatial data.\n *\n * Input shape:\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, rows, cols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *  - If `dataFormat === CHANNEL_LAST`:\n *      4D tensor with shape:\n *      `[batchSize, pooledRows, pooledCols, channels]`\n *  - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *      `[batchSize, channels, pooledRows, pooledCols]`\n *\n * `tf.avgPool2d` is an alias.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function averagePooling2d(args: Pooling2DLayerArgs) {\n  return new AveragePooling2D(args);\n}\nexport function avgPool2d(args: Pooling2DLayerArgs) {\n  return averagePooling2d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling2d(args: Pooling2DLayerArgs) {\n  return averagePooling2d(args);\n}\n\n/**\n * Average pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      4D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function averagePooling3d(args: Pooling3DLayerArgs) {\n  return new AveragePooling3D(args);\n}\nexport function avgPool3d(args: Pooling3DLayerArgs) {\n  return averagePooling3d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling3d(args: Pooling3DLayerArgs) {\n  return averagePooling3d(args);\n}\n\n/**\n * Global average pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape: 2D tensor with shape: `[batchSize, features]`.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function globalAveragePooling1d(args?: LayerArgs) {\n  return new GlobalAveragePooling1D(args);\n}\n\n/**\n * Global average pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function globalAveragePooling2d(args: GlobalPooling2DLayerArgs) {\n  return new GlobalAveragePooling2D(args);\n}\n\n/**\n * Global max pooling operation for temporal data.\n *\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\n *\n * Output Shape: 2D tensor with shape: `[batchSize, features]`.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function globalMaxPooling1d(args?: LayerArgs) {\n  return new GlobalMaxPooling1D(args);\n}\n\n/**\n * Global max pooling operation for spatial data.\n *\n * Input shape:\n *   - If `dataFormat` is `CHANNEL_LAST`:\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\n *   - If `dataFormat` is `CHANNEL_FIRST`:\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\n *\n * Output shape:\n *   2D tensor with shape: `[batchSize, channels]`.\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function globalMaxPooling2d(args: GlobalPooling2DLayerArgs) {\n  return new GlobalMaxPooling2D(args);\n}\n\n/**\n * Max pooling operation for temporal data.\n *\n * Input shape:  `[batchSize, inLength, channels]`\n *\n * Output shape: `[batchSize, pooledLength, channels]`\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function maxPooling1d(args: Pooling1DLayerArgs) {\n  return new MaxPooling1D(args);\n}\n\n/**\n * Max pooling operation for spatial data.\n *\n * Input shape\n *   - If `dataFormat === CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, rows, cols, channels]`\n *   - If `dataFormat === CHANNEL_FIRST`:\n *      4D tensor with shape:\n *       `[batchSize, channels, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=CHANNEL_LAST`:\n *       4D tensor with shape:\n *       `[batchSize, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=CHANNEL_FIRST`:\n *       4D tensor with shape:\n *       `[batchSize, channels, pooledRows, pooledCols]`\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function maxPooling2d(args: Pooling2DLayerArgs) {\n  return new MaxPooling2D(args);\n}\n\n/**\n * Max pooling operation for 3D data.\n *\n * Input shape\n *   - If `dataFormat === channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, depths, rows, cols, channels]`\n *   - If `dataFormat === channelsFirst`:\n *      5D tensor with shape:\n *       `[batchSize, channels, depths, rows, cols]`\n *\n * Output shape\n *   - If `dataFormat=channelsLast`:\n *       5D tensor with shape:\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\n *   - If `dataFormat=channelsFirst`:\n *       5D tensor with shape:\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\n *\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\n */\nexport function maxPooling3d(args: Pooling3DLayerArgs) {\n  return new MaxPooling3D(args);\n}\n\n// Recurrent Layers.\n\n/**\n * Gated Recurrent Unit - Cho et al. 2014.\n *\n * This is an `RNN` layer consisting of one `GRUCell`. However, unlike\n * the underlying `GRUCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.gru({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `GRUCell`'s number of units.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function gru(args: GRULayerArgs) {\n  return new GRU(args);\n}\n\n/**\n * Cell class for `GRU`.\n *\n * `GRUCell` is distinct from the `RNN` subclass `GRU` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `GRU` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.gruCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `GRUCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.gruCell({units: 4}),\n *   tf.layers.gruCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `gruCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `GRUCell`, use the\n * `tf.layers.gru`.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function gruCell(args: GRUCellLayerArgs) {\n  return new GRUCell(args);\n}\n\n/**\n * Long-Short Term Memory layer - Hochreiter 1997.\n *\n * This is an `RNN` layer consisting of one `LSTMCell`. However, unlike\n * the underlying `LSTMCell`, the `apply` method of `LSTM` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const lstm = tf.layers.lstm({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = lstm.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `LSTMCell`'s number of units.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function lstm(args: LSTMLayerArgs) {\n  return new LSTM(args);\n}\n\n/**\n * Cell class for `LSTM`.\n *\n * `LSTMCell` is distinct from the `RNN` subclass `LSTM` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `LSTM` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.lstmCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `LSTMCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.lstmCell({units: 4}),\n *   tf.layers.lstmCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `lstmCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `LSTMCell`, use the\n * `tf.layers.lstm`.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function lstmCell(args: LSTMCellLayerArgs) {\n  return new LSTMCell(args);\n}\n\n/**\n * Fully-connected RNN where the output is to be fed back to input.\n *\n * This is an `RNN` layer consisting of one `SimpleRNNCell`. However, unlike\n * the underlying `SimpleRNNCell`, the `apply` method of `SimpleRNN` operates\n * on a sequence of inputs. The shape of the input (not including the first,\n * batch dimension) needs to be at least 2-D, with the first dimension being\n * time steps. For example:\n *\n * ```js\n * const rnn = tf.layers.simpleRNN({units: 8, returnSequences: true});\n *\n * // Create an input with 10 time steps.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the `SimpleRNNCell`'s number of units.\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function simpleRNN(args: SimpleRNNLayerArgs) {\n  return new SimpleRNN(args);\n}\n\n/**\n * Cell class for `SimpleRNN`.\n *\n * `SimpleRNNCell` is distinct from the `RNN` subclass `SimpleRNN` in that its\n * `apply` method takes the input data of only a single time step and returns\n * the cell's output at the time step, while `SimpleRNN` takes the input data\n * over a number of time steps. For example:\n *\n * ```js\n * const cell = tf.layers.simpleRNNCell({units: 2});\n * const input = tf.input({shape: [10]});\n * const output = cell.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10]: This is the cell's output at a single time step. The 1st\n * // dimension is the unknown batch size.\n * ```\n *\n * Instance(s) of `SimpleRNNCell` can be used to construct `RNN` layers. The\n * most typical use of this workflow is to combine a number of cells into a\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\n * RNN. For example:\n *\n * ```js\n * const cells = [\n *   tf.layers.simpleRNNCell({units: 4}),\n *   tf.layers.simpleRNNCell({units: 8}),\n * ];\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\n *\n * // Create an input with 10 time steps and a length-20 vector at each step.\n * const input = tf.input({shape: [10, 20]});\n * const output = rnn.apply(input);\n *\n * console.log(JSON.stringify(output.shape));\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\n * // 3rd dimension is the last `SimpleRNNCell`'s number of units.\n * ```\n *\n * To create an `RNN` consisting of only *one* `SimpleRNNCell`, use the\n * `tf.layers.simpleRNN`.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function simpleRNNCell(args: SimpleRNNCellLayerArgs) {\n  return new SimpleRNNCell(args);\n}\n\n/**\n * Convolutional LSTM layer - Xingjian Shi 2015.\n *\n * This is a `ConvRNN2D` layer consisting of one `ConvLSTM2DCell`. However,\n * unlike the underlying `ConvLSTM2DCell`, the `apply` method of `ConvLSTM2D`\n * operates on a sequence of inputs. The shape of the input (not including the\n * first, batch dimension) needs to be 4-D, with the first dimension being time\n * steps. For example:\n *\n * ```js\n * const filters = 3;\n * const kernelSize = 3;\n *\n * const batchSize = 4;\n * const sequenceLength = 2;\n * const size = 5;\n * const channels = 3;\n *\n * const inputShape = [batchSize, sequenceLength, size, size, channels];\n * const input = tf.ones(inputShape);\n *\n * const layer = tf.layers.convLstm2d({filters, kernelSize});\n *\n * const output = layer.apply(input);\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function convLstm2d(args: ConvLSTM2DArgs) {\n  return new ConvLSTM2D(args);\n}\n\n/**\n * Cell class for `ConvLSTM2D`.\n *\n * `ConvLSTM2DCell` is distinct from the `ConvRNN2D` subclass `ConvLSTM2D` in\n * that its `call` method takes the input data of only a single time step and\n * returns the cell's output at the time step, while `ConvLSTM2D` takes the\n * input data over a number of time steps. For example:\n *\n * ```js\n * const filters = 3;\n * const kernelSize = 3;\n *\n * const sequenceLength = 1;\n * const size = 5;\n * const channels = 3;\n *\n * const inputShape = [sequenceLength, size, size, channels];\n * const input = tf.ones(inputShape);\n *\n * const cell = tf.layers.convLstm2dCell({filters, kernelSize});\n *\n * cell.build(input.shape);\n *\n * const outputSize = size - kernelSize + 1;\n * const outShape = [sequenceLength, outputSize, outputSize, filters];\n *\n * const initialH = tf.zeros(outShape);\n * const initialC = tf.zeros(outShape);\n *\n * const [o, h, c] = cell.call([input, initialH, initialC], {});\n * ```\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function convLstm2dCell(args: ConvLSTM2DCellArgs) {\n  return new ConvLSTM2DCell(args);\n}\n\n/**\n * Base class for recurrent layers.\n *\n * Input shape:\n *   3D tensor with shape `[batchSize, timeSteps, inputDim]`.\n *\n * Output shape:\n *   - if `returnState`, an Array of tensors (i.e., `tf.Tensor`s). The first\n *     tensor is the output. The remaining tensors are the states at the\n *     last time step, each with shape `[batchSize, units]`.\n *   - if `returnSequences`, the output will have shape\n *     `[batchSize, timeSteps, units]`.\n *   - else, the output will have shape `[batchSize, units]`.\n *\n * Masking:\n *   This layer supports masking for input data with a variable number\n *   of timesteps. To introduce masks to your data,\n *   use an embedding layer with the `mask_zero` parameter\n *   set to `True`.\n *\n * Notes on using statefulness in RNNs:\n *   You can set RNN layers to be 'stateful', which means that the states\n *   computed for the samples in one batch will be reused as initial states\n *   for the samples in the next batch. This assumes a one-to-one mapping\n *   between samples in different successive batches.\n *\n *   To enable statefulness:\n *     - specify `stateful: true` in the layer constructor.\n *     - specify a fixed batch size for your model, by passing\n *       if sequential model:\n *         `batchInputShape=[...]` to the first layer in your model.\n *       else for functional model with 1 or more Input layers:\n *         `batchShape=[...]` to all the first layers in your model.\n *       This is the expected shape of your inputs *including the batch size*.\n *       It should be a tuple of integers, e.g. `(32, 10, 100)`.\n *     - specify `shuffle=False` when calling fit().\n *\n *   To reset the states of your model, call `.resetStates()` on either\n *   a specific layer, or on your entire model.\n *\n * Note on specifying the initial state of RNNs\n *   You can specify the initial state of RNN layers symbolically by\n *   calling them with the option `initialState`. The value of\n *   `initialState` should be a tensor or list of tensors representing\n *   the initial state of the RNN layer.\n *\n *   You can specify the initial state of RNN layers numerically by\n *   calling `resetStates` with the keyword argument `states`. The value of\n *   `states` should be a numpy array or list of numpy arrays representing\n *   the initial state of the RNN layer.\n *\n * Note on passing external constants to RNNs\n *   You can pass \"external\" constants to the cell using the `constants`\n *   keyword argument of `RNN.call` method. This requires that the `cell.call`\n *   method accepts the same keyword argument `constants`. Such constants\n *   can be used to condition the cell transformation on additional static\n *   inputs (not changing over time), a.k.a. an attention mechanism.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function rnn(args: RNNLayerArgs) {\n  return new RNN(args);\n}\n\n/**\n * Wrapper allowing a stack of RNN cells to behave as a single cell.\n *\n * Used to implement efficient stacked RNNs.\n *\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\n */\nexport function stackedRNNCells(args: StackedRNNCellsArgs){\n  return new StackedRNNCells(args);\n}\n\n// Wrapper Layers.\n\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nexport function bidirectional(args: BidirectionalLayerArgs) {\n  return new Bidirectional(args);\n}\n\n/**\n * This wrapper applies a layer to every temporal slice of an input.\n *\n * The input should be at least 3D,  and the dimension of the index `1` will be\n * considered to be the temporal dimension.\n *\n * Consider a batch of 32 samples, where each sample is a sequence of 10 vectors\n * of 16 dimensions. The batch input shape of the layer is then `[32,  10,\n * 16]`, and the `inputShape`, not including the sample dimension, is\n * `[10, 16]`.\n *\n * You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10\n * timesteps, independently:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.dense({units: 8}),\n *   inputShape: [10, 16],\n * }));\n *\n * // Now model.outputShape = [null, 10, 8].\n * // The output will then have shape `[32, 10, 8]`.\n *\n * // In subsequent layers, there is no need for `inputShape`:\n * model.add(tf.layers.timeDistributed({layer: tf.layers.dense({units: 32})}));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * // Now model.outputShape = [null, 10, 32].\n * ```\n *\n * The output will then have shape `[32, 10, 32]`.\n *\n * `TimeDistributed` can be used with arbitrary layers, not just `Dense`, for\n * instance a `Conv2D` layer.\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.timeDistributed({\n *   layer: tf.layers.conv2d({filters: 64, kernelSize: [3, 3]}),\n *   inputShape: [10, 299, 299, 3],\n * }));\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'}\n */\nexport function timeDistributed(args: WrapperLayerArgs) {\n  return new TimeDistributed(args);\n}\n\n// Aliases for pooling.\nexport const globalMaxPool1d = globalMaxPooling1d;\nexport const globalMaxPool2d = globalMaxPooling2d;\nexport const maxPool1d = maxPooling1d;\nexport const maxPool2d = maxPooling2d;\n\nexport {Layer, RNN, RNNCell, input /* alias for tf.input */};\n\n/**\n * Apply additive zero-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * This is useful to mitigate overfitting\n * (you could see it as a form of random data augmentation).\n * Gaussian Noise (GS) is a natural choice as corruption process\n * for real valued inputs.\n *\n * # Arguments\n * stddev: float, standard deviation of the noise distribution.\n *\n * # Input shape\n * Arbitrary. Use the keyword argument `input_shape`\n * (tuple of integers, does not include the samples axis)\n * when using this layer as the first layer in a model.\n *\n * # Output shape\n * Same shape as input.\n *\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n */\nexport function gaussianNoise(args: GaussianNoiseArgs) {\n  return new GaussianNoise(args);\n}\n\n/**\n * Apply multiplicative 1-centered Gaussian noise.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n *      http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n *\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n */\nexport function gaussianDropout(args: GaussianDropoutArgs) {\n  return new GaussianDropout(args);\n}\n\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n *\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\n */\nexport function alphaDropout(args: AlphaDropoutArgs) {\n  return new AlphaDropout(args);\n}\n\n/**\n * Masks a sequence by using a mask value to skip timesteps.\n *\n * If all features for a given sample timestep are equal to `mask_value`,\n * then the sample timestep will be masked (skipped) in all downstream layers\n * (as long as they support masking).\n *\n * If any downstream layer does not support masking yet receives such\n * an input mask, an exception will be raised.\n *\n * Arguments:\n *   - `maskValue`: Either None or mask value to skip.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * @doc {heading: 'Layers', subheading: 'Mask', namespace: 'layers'}\n */\nexport function masking(args?: MaskingArgs) {\n  return new Masking(args);\n}\n\n/**\n * A preprocessing layer which rescales input values to a new range.\n *\n * This layer rescales every value of an input (often an image) by multiplying\n * by `scale` and adding `offset`.\n *\n * For instance:\n * 1. To rescale an input in the ``[0, 255]`` range\n * to be in the `[0, 1]` range, you would pass `scale=1/255`.\n * 2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]`\n * range, you would pass `scale=1./127.5, offset=-1`.\n * The rescaling is applied both during training and inference. Inputs can be\n * of integer or floating point dtype, and by default the layer will output\n * floats.\n *\n * Arguments:\n *   - `scale`: Float, the scale to apply to the inputs.\n *   - `offset`: Float, the offset to apply to the inputs.\n *\n * Input shape:\n *   Arbitrary.\n *\n * Output shape:\n *   Same as input.\n *\n * @doc {heading: 'Layers', subheading: 'Rescaling', namespace: 'layers'}\n */\nexport function rescaling(args?: RescalingArgs) {\n  return new Rescaling(args);\n}\n\n/**\n *  A preprocessing layer which center crops images.\n *\n *   This layers crops the central portion of the images to a target size. If an\n *   image is smaller than the target size, it will be resized and cropped so as\n *   to return the largest possible window in the image that matches the target\n *   aspect ratio.\n *\n *   Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n *   of integer or floating point dtype.\n *\n *   If the input height/width is even and the target height/width is odd (or\n *   inversely), the input image is left-padded by 1 pixel.\n *\n *   Arguments:\n *     `height`: Integer, the height of the output shape.\n *     `width`: Integer, the width of the output shape.\n *\n *   Input shape:\n *     3D (unbatched) or 4D (batched) tensor with shape:\n *     `(..., height, width, channels)`, in `channelsLast` format.\n *\n *   Output shape:\n *     3D (unbatched) or 4D (batched) tensor with shape:\n *     `(..., targetHeight, targetWidth, channels)`.\n *\n *\n *  @doc {heading: 'Layers', subheading: 'CenterCrop', namespace: 'layers'}\n */\nexport function centerCrop(args?: CenterCropArgs) {\n   return new CenterCrop(args);\n  }\n\n/**\n * A preprocessing layer which resizes images.\n * This layer resizes an image input to a target height and width. The input\n * should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"`\n * format.  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,\n * 255]`) and of interger or floating point dtype. By default, the layer will\n * output floats.\n *\n * Arguments:\n *   - `height`: number, the height for the output tensor.\n *   - `width`: number, the width for the output tensor.\n *   - `interpolation`: string, the method for image resizing interpolation.\n *   - `cropToAspectRatio`: boolean, whether to keep image aspect ratio.\n *\n * Input shape:\n *   Arbitrary.\n *\n * Output shape:\n *   height, width, num channels.\n *\n * @doc {heading: 'Layers', subheading: 'Resizing', namespace: 'layers'}\n */\nexport function resizing(args?: ResizingArgs) {\n  return new Resizing(args);\n}\n\n/**\n * A preprocessing layer which encodes integer features.\n *\n * This layer provides options for condensing data into a categorical encoding\n * when the total number of tokens are known in advance. It accepts integer\n * values as inputs, and it outputs a dense representation of those\n * inputs.\n *\n * Arguments:\n *\n * numTokens: The total number of tokens the layer should support. All\n *  inputs to the layer must integers in the range `0 <= value <\n *  numTokens`, or an error will be thrown.\n *\n * outputMode: Specification for the output of the layer.\n *  Defaults to `multiHot`. Values can be `oneHot`, `multiHot` or\n *  `count`, configuring the layer as follows:\n *\n *    oneHot: Encodes each individual element in the input into an\n *      array of `numTokens` size, containing a 1 at the element index. If\n *      the last dimension is size 1, will encode on that dimension. If the\n *      last dimension is not size 1, will append a new dimension for the\n *      encoded output.\n *\n *    multiHot: Encodes each sample in the input into a single array\n *     of `numTokens` size, containing a 1 for each vocabulary term\n *     present in the sample. Treats the last dimension as the sample\n *     dimension, if input shape is `(..., sampleLength)`, output shape\n *     will be `(..., numTokens)`.\n *\n *    count: Like `multiHot`, but the int array contains a count of\n *     the number of times the token at that index appeared in the sample.\n *\n *  For all output modes, currently only output up to rank 2 is supported.\n *   Call arguments:\n *    inputs: A 1D or 2D tensor of integer inputs.\n *    countWeights: A tensor in the same shape as `inputs` indicating the\n *    weight for each sample value when summing up in `count` mode. Not used\n *    in `multiHot` or `oneHot` modes.\n *\n *\n * @doc {heading: 'Layers', subheading: 'CategoryEncoding', namespace: 'layers'}\n */\nexport function categoryEncoding(args: CategoryEncodingArgs) {\n  return new CategoryEncoding(args);\n}\n\n /**\n  * A preprocessing layer which randomly varies image width during training.\n  *\n  * This layer will randomly adjusts the width of a batch of images of a batch\n  * of images by a random factor.\n  *\n  * The input should be a 3D (unbatched) or 4D (batched) tensor in\n  * the `\"channels_last\"` image data format. Input pixel values can be of any\n  * range (e.g. `[0., 1.)` or `[0, 255]`) and of integer or floating point\n  * dtype. By default, the layer will output floats. By default, this layer is\n  * inactive during inference. For an overview and full list of preprocessing\n  * layers, see the preprocessing [guide]\n  * (https://www.tensorflow.org/guide/keras/preprocessing_layers).\n  *\n  * Arguments:\n  *\n  * factor:\n  *   A positive float (fraction of original width), or a tuple of size 2\n  *   representing lower and upper bound for resizing vertically.\n  *   When represented as a single float, this value is used for both the upper\n  *   and lower bound. For instance, `factor=(0.2, 0.3)` results in an output\n  *   with width changed by a random amount in the range `[20%, 30%]`.\n  *   `factor=(-0.2, 0.3)` results in an output with width changed by a random\n  *   amount in the range `[-20%, +30%]`. `factor=0.2` results in an output\n  *   with width changed by a random amount in the range `[-20%, +20%]`.\n  * interpolation:\n  *   String, the interpolation method.\n  *   Defaults to `bilinear`.\n  *   Supports `\"bilinear\"`, `\"nearest\"`.\n  *   The tf methods `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`, `\"lanczos5\"`,\n  *   `\"gaussian\"`, `\"mitchellcubic\"` are unimplemented in tfjs.\n  * seed:\n  *   Integer. Used to create a random seed.\n  *\n  * Input shape:\n  *     3D (unbatched) or 4D (batched) tensor with shape:\n  *     `(..., height, width, channels)`, in `\"channels_last\"` format.\n  * Output shape:\n  *     3D (unbatched) or 4D (batched) tensor with shape:\n  *     `(..., height, random_width, channels)`.\n  *\n  *\n  * @doc {heading: 'Layers', subheading: 'RandomWidth', namespace: 'layers'}\n  */\n  export function randomWidth(args: RandomWidthArgs) {\n    return new RandomWidth(args);\n  }\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAEH,OAAO,EAAC,UAAU,EAAiB,MAAM,sBAAsB,CAAC;AAChE,OAAO,EAAC,KAAK,EAAY,MAAM,mBAAmB,CAAC;AACnD,OAAO,EAAC,KAAK,EAAC,MAAM,WAAW,CAAC;AAChC,OAAO,EAAC,GAAG,EAAgB,SAAS,EAAsB,KAAK,EAAkB,IAAI,EAAiB,OAAO,EAAoB,eAAe,EAA2B,MAAM,+BAA+B,CAAC;AACjN,OAAO,EAAC,MAAM,EAAE,MAAM,EAAE,eAAe,EAAE,MAAM,EAAiB,UAAU,EAAuB,eAAe,EAA0B,YAAY,EAAyB,eAAe,EAAC,MAAM,wBAAwB,CAAC;AAC9N,OAAO,EAAC,eAAe,EAA2B,MAAM,kCAAkC,CAAC;AAC3F,OAAO,EAAC,UAAU,EAAkB,cAAc,EAAqB,MAAM,kCAAkC,CAAC;AAChH,OAAO,EAAC,UAAU,EAAuB,KAAK,EAAkB,OAAO,EAAoB,OAAO,EAAoB,OAAO,EAAe,OAAO,EAAoB,YAAY,EAAyB,OAAO,EAAoB,gBAAgB,EAA8B,MAAM,eAAe,CAAC;AAC3S,OAAO,EAAC,SAAS,EAAqB,MAAM,qBAAqB,CAAC;AAClE,OAAO,EAAC,GAAG,EAAE,OAAO,EAAE,WAAW,EAAwB,GAAG,EAAgB,OAAO,EAAE,OAAO,EAAE,QAAQ,EAAC,MAAM,gBAAgB,CAAC;AAC9H,OAAO,EAAC,YAAY,EAAoB,eAAe,EAAuB,aAAa,EAAoB,MAAM,gBAAgB,CAAC;AACtI,OAAO,EAAC,kBAAkB,EAA+B,kBAAkB,EAA8B,MAAM,wBAAwB,CAAC;AACxI,OAAO,EAAC,aAAa,EAAyB,MAAM,kBAAkB,CAAC;AACvE,OAAO,EAAC,gBAAgB,EAAE,gBAAgB,EAAE,gBAAgB,EAAE,sBAAsB,EAAE,sBAAsB,EAAE,kBAAkB,EAAE,kBAAkB,EAA4B,YAAY,EAAE,YAAY,EAAE,YAAY,EAA6D,MAAM,kBAAkB,CAAC;AAC9S,OAAO,EAAC,GAAG,EAAE,OAAO,EAAkC,IAAI,EAAE,QAAQ,EAAoC,GAAG,EAAE,OAAO,EAAgB,SAAS,EAAE,aAAa,EAA8C,eAAe,EAAsB,MAAM,oBAAoB,CAAC;AAC1Q,OAAO,EAAC,aAAa,EAA0B,eAAe,EAAmB,MAAM,mBAAmB,CAAC;AAC3G,OAAO,EAAC,SAAS,EAAgB,MAAM,4CAA4C,CAAC;AACpF,OAAO,EAAC,UAAU,EAAiB,MAAM,oCAAoC,CAAC;AAC9E,OAAO,EAAC,gBAAgB,EAAuB,MAAM,0CAA0C,CAAC;AAChG,OAAO,EAAC,QAAQ,EAAe,MAAM,uCAAuC,CAAC;AAC7E,OAAO,EAAC,WAAW,EAAkB,MAAM,qCAAqC,CAAC;;;;;;;;;;;;;;;;;;;;;;AAyC3E,SAAU,UAAU,CAAC,IAAoB;IAC7C,OAAO,uLAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AA4BK,SAAU,GAAG,CAAC,IAAmB;IACrC,OAAO,gMAAI,MAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAmBK,SAAU,IAAI,CAAC,IAAoB;IACvC,OAAO,gMAAI,OAAI,CAAC,IAAI,CAAC,CAAC;AACxB,CAAC;AAsBK,SAAU,SAAS,CAAC,IAAyB;IACjD,OAAO,gMAAI,YAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAuBK,SAAU,KAAK,CAAC,IAAqB;IACzC,OAAO,gMAAI,QAAK,CAAC,IAAI,CAAC,CAAC;AACzB,CAAC;AAkBK,SAAU,OAAO,CAAC,IAAuB;IAC7C,OAAO,gMAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AA0BK,SAAU,eAAe,CAAC,IAA+B;IAC7D,OAAO,gMAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAwBK,SAAU,MAAM,CAAC,IAAmB;IACxC,OAAO,yLAAI,SAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAoBK,SAAU,MAAM,CAAC,IAAmB;IACxC,OAAO,yLAAI,SAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAqCK,SAAU,eAAe,CAAC,IAAmB;IACjD,OAAO,yLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAoBK,SAAU,MAAM,CAAC,IAAmB;IACxC,OAAO,yLAAI,SAAM,CAAC,IAAI,CAAC,CAAC;AAC1B,CAAC;AAEK,SAAU,eAAe,CAAC,IAAmB;IACjD,OAAO,yLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AA+BK,SAAU,eAAe,CAAC,IAA4B;IAC1D,OAAO,yLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAiCK,SAAU,UAAU,CAAC,IAAyB;IAClD,OAAO,yLAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AA0BK,SAAU,YAAY,CAAC,IAA2B;IACtD,OAAO,yLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAcK,SAAU,eAAe,CAAC,IAA8B;IAC5D,OAAO,mMAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAmCK,SAAU,UAAU,CAAC,IAAyB;IAClD,OAAO,gLAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AAmCK,SAAU,KAAK,CAAC,IAAoB;IACxC,OAAO,gLAAI,QAAK,CAAC,IAAI,CAAC,CAAC;AACzB,CAAC;AAYK,SAAU,OAAO,CAAC,IAAsB;IAC5C,OAAO,gLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAkCK,SAAU,gBAAgB,CAAC,IAAiC;IAChE,OAAO,gLAAI,mBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AAqBK,SAAU,OAAO,CAAC,IAAuB;IAC7C,OAAO,gLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAgBK,SAAU,YAAY,CAAC,IAA2B;IACtD,OAAO,gLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAyBK,SAAU,OAAO,CAAC,IAAsB;IAC5C,OAAO,gLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AA8BK,SAAU,OAAO,CAAC,IAAsB;IAC5C,OAAO,gLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAaK,SAAU,SAAS,CAAC,IAAwB;IAChD,OAAO,sLAAI,YAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAwBK,SAAU,GAAG,CAAC,IAAgB;IAClC,OAAO,iLAAI,MAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAoBK,SAAU,OAAO,CAAC,IAAgB;IACtC,OAAO,iLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAsBK,SAAU,WAAW,CAAC,IAA2B;IACrD,OAAO,iLAAI,cAAW,CAAC,IAAI,CAAC,CAAC;AAC/B,CAAC;AAoBK,SAAU,OAAO,CAAC,IAAgB;IACtC,OAAO,iLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAoBK,SAAU,OAAO,CAAC,IAAgB;IACtC,OAAO,iLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAqBK,SAAU,QAAQ,CAAC,IAAgB;IACvC,OAAO,iLAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AAwBK,SAAU,GAAG,CAAC,IAAkB;IACpC,OAAO,iLAAI,MAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AAyBK,SAAU,kBAAkB,CAAC,IAAkC;IACnE,OAAO,yLAAI,qBAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAsBK,SAAU,kBAAkB,CAAC,IAAkC;IACnE,OAAO,yLAAI,qBAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AA0BK,SAAU,aAAa,CAAC,IAA6B;IACzD,OAAO,mLAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAeK,SAAU,gBAAgB,CAAC,IAAwB;IACvD,OAAO,mLAAI,mBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AACK,SAAU,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAGK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAyBK,SAAU,gBAAgB,CAAC,IAAwB;IACvD,OAAO,mLAAI,mBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AACK,SAAU,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAGK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAuBK,SAAU,gBAAgB,CAAC,IAAwB;IACvD,OAAO,mLAAI,mBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AACK,SAAU,SAAS,CAAC,IAAwB;IAChD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAGK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,gBAAgB,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAWK,SAAU,sBAAsB,CAAC,IAAgB;IACrD,OAAO,mLAAI,yBAAsB,CAAC,IAAI,CAAC,CAAC;AAC1C,CAAC;AAgBK,SAAU,sBAAsB,CAAC,IAA8B;IACnE,OAAO,mLAAI,yBAAsB,CAAC,IAAI,CAAC,CAAC;AAC1C,CAAC;AAWK,SAAU,kBAAkB,CAAC,IAAgB;IACjD,OAAO,mLAAI,qBAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAgBK,SAAU,kBAAkB,CAAC,IAA8B;IAC/D,OAAO,mLAAI,qBAAkB,CAAC,IAAI,CAAC,CAAC;AACtC,CAAC;AAWK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,mLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAuBK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,mLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAuBK,SAAU,YAAY,CAAC,IAAwB;IACnD,OAAO,mLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AA2BK,SAAU,GAAG,CAAC,IAAkB;IACpC,OAAO,qLAAI,MAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AA+CK,SAAU,OAAO,CAAC,IAAsB;IAC5C,OAAO,qLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AAyBK,SAAU,IAAI,CAAC,IAAmB;IACtC,OAAO,qLAAI,OAAI,CAAC,IAAI,CAAC,CAAC;AACxB,CAAC;AA+CK,SAAU,QAAQ,CAAC,IAAuB;IAC9C,OAAO,qLAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AA0BK,SAAU,SAAS,CAAC,IAAwB;IAChD,OAAO,qLAAI,YAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AA+CK,SAAU,aAAa,CAAC,IAA4B;IACxD,OAAO,qLAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AA6BK,SAAU,UAAU,CAAC,IAAoB;IAC7C,OAAO,mMAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC9B,CAAC;AAmCK,SAAU,cAAc,CAAC,IAAwB;IACrD,OAAO,mMAAI,iBAAc,CAAC,IAAI,CAAC,CAAC;AAClC,CAAC;AA8DK,SAAU,GAAG,CAAC,IAAkB;IACpC,OAAO,qLAAI,MAAG,CAAC,IAAI,CAAC,CAAC;AACvB,CAAC;AASK,SAAU,eAAe,CAAC,IAAyB;IACvD,OAAO,qLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAKK,SAAU,aAAa,CAAC,IAA4B;IACxD,OAAO,oLAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAgDK,SAAU,eAAe,CAAC,IAAsB;IACpD,OAAO,oLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAGM,MAAM,eAAe,GAAG,kBAAkB,CAAC;AAC3C,MAAM,eAAe,GAAG,kBAAkB,CAAC;AAC3C,MAAM,SAAS,GAAG,YAAY,CAAC;AAC/B,MAAM,SAAS,GAAG,YAAY,CAAC;;AA2BhC,SAAU,aAAa,CAAC,IAAuB;IACnD,OAAO,iLAAI,gBAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AA0BK,SAAU,eAAe,CAAC,IAAyB;IACvD,OAAO,iLAAI,kBAAe,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AAiCK,SAAU,YAAY,CAAC,IAAsB;IACjD,OAAO,iLAAI,eAAY,CAAC,IAAI,CAAC,CAAC;AAChC,CAAC;AAyBK,SAAU,OAAO,CAAC,IAAkB;IACxC,OAAO,gLAAI,UAAO,CAAC,IAAI,CAAC,CAAC;AAC3B,CAAC;AA6BK,SAAU,SAAS,CAAC,IAAoB;IAC5C,OAAO,gNAAI,YAAS,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AA+BK,SAAU,UAAU,CAAC,IAAqB;IAC7C,OAAO,wMAAI,aAAU,CAAC,IAAI,CAAC,CAAC;AAC7B,CAAC;AAwBG,SAAU,QAAQ,CAAC,IAAmB;IAC1C,OAAO,2MAAI,WAAQ,CAAC,IAAI,CAAC,CAAC;AAC5B,CAAC;AA6CK,SAAU,gBAAgB,CAAC,IAA0B;IACzD,OAAO,8MAAI,mBAAgB,CAAC,IAAI,CAAC,CAAC;AACpC,CAAC;AA8CO,SAAU,WAAW,CAAC,IAAqB;IAC/C,OAAO,yMAAI,cAAW,CAAC,IAAI,CAAC,CAAC;AAC/B,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4094, "column": 0}, "map": {"version":3,"file":"exports_metrics.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_metrics.js/__/__/__/__/__/tfjs-layers/src/exports_metrics.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport {Tensor} from '@tensorflow/tfjs-core';\n\nimport * as losses from './losses';\nimport * as metrics from './metrics';\n\n/**\n * Binary accuracy metric function.\n *\n * `yTrue` and `yPred` can have 0-1 values. Example:\n * ```js\n * const x = tf.tensor2d([[1, 1, 1, 1], [0, 0, 0, 0]], [2, 4]);\n * const y = tf.tensor2d([[1, 0, 1, 0], [0, 0, 0, 1]], [2, 4]);\n * const accuracy = tf.metrics.binaryAccuracy(x, y);\n * accuracy.print();\n * ```\n *\n * `yTrue` and `yPred` can also have floating-number values between 0 and 1, in\n * which case the values will be thresholded at 0.5 to yield 0-1 values (i.e.,\n * a value >= 0.5 and <= 1.0 is interpreted as 1).\n *\n * Example:\n * ```js\n * const x = tf.tensor1d([1, 1, 1, 1, 0, 0, 0, 0]);\n * const y = tf.tensor1d([0.2, 0.4, 0.6, 0.8, 0.2, 0.3, 0.4, 0.7]);\n * const accuracy = tf.metrics.binaryAccuracy(x, y);\n * accuracy.print();\n * ```\n *\n * @param yTrue Binary Tensor of truth.\n * @param yPred Binary Tensor of prediction.\n * @return Accuracy Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function binaryAccuracy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.binaryAccuracy(yTrue, yPred);\n}\n\n/**\n * Binary crossentropy metric function.\n *\n * Example:\n * ```js\n * const x = tf.tensor2d([[0], [1], [1], [1]]);\n * const y = tf.tensor2d([[0], [0], [0.5], [1]]);\n * const crossentropy = tf.metrics.binaryCrossentropy(x, y);\n * crossentropy.print();\n * ```\n *\n * @param yTrue Binary Tensor of truth.\n * @param yPred Binary Tensor of prediction, probabilities for the `1` case.\n * @return Accuracy Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function binaryCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.binaryCrossentropy(yTrue, yPred);\n}\n\n/**\n * Sparse categorical accuracy metric function.\n *\n * Example:\n * ```js\n *\n * const yTrue = tf.tensor1d([1, 1, 2, 2, 0]);\n * const yPred = tf.tensor2d(\n *      [[0, 1, 0], [1, 0, 0], [0, 0.4, 0.6], [0, 0.6, 0.4], [0.7, 0.3, 0]]);\n * const crossentropy = tf.metrics.sparseCategoricalAccuracy(yTrue, yPred);\n * crossentropy.print();\n * ```\n *\n * @param yTrue True labels: indices.\n * @param yPred Predicted probabilities or logits.\n * @returns Accuracy tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function sparseCategoricalAccuracy(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.sparseCategoricalAccuracy(yTrue, yPred);\n}\n\n/**\n * Categorical accuracy metric function.\n *\n * Example:\n * ```js\n * const x = tf.tensor2d([[0, 0, 0, 1], [0, 0, 0, 1]]);\n * const y = tf.tensor2d([[0.1, 0.8, 0.05, 0.05], [0.1, 0.05, 0.05, 0.8]]);\n * const accuracy = tf.metrics.categoricalAccuracy(x, y);\n * accuracy.print();\n * ```\n *\n * @param yTrue Binary Tensor of truth: one-hot encoding of categories.\n * @param yPred Binary Tensor of prediction: probabilities or logits for the\n *   same categories as in `yTrue`.\n * @return Accuracy Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function categoricalAccuracy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.categoricalAccuracy(yTrue, yPred);\n}\n\n/**\n * Categorical crossentropy between an output tensor and a target tensor.\n *\n * @param target A tensor of the same shape as `output`.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function categoricalCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.categoricalCrossentropy(yTrue, yPred);\n}\n\n/**\n * Computes the precision of the predictions with respect to the labels.\n *\n * Example:\n * ```js\n * const x = tf.tensor2d(\n *    [\n *      [0, 0, 0, 1],\n *      [0, 1, 0, 0],\n *      [0, 0, 0, 1],\n *      [1, 0, 0, 0],\n *      [0, 0, 1, 0]\n *    ]\n * );\n *\n * const y = tf.tensor2d(\n *    [\n *      [0, 0, 1, 0],\n *      [0, 1, 0, 0],\n *      [0, 0, 0, 1],\n *      [0, 1, 0, 0],\n *      [0, 1, 0, 0]\n *    ]\n * );\n *\n * const precision = tf.metrics.precision(x, y);\n * precision.print();\n * ```\n *\n * @param yTrue The ground truth values. Expected to contain only 0-1 values.\n * @param yPred The predicted values. Expected to contain only 0-1 values.\n * @return Precision Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function precision(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.precision(yTrue, yPred);\n}\n\n/**\n * Computes the recall of the predictions with respect to the labels.\n *\n * Example:\n * ```js\n * const x = tf.tensor2d(\n *    [\n *      [0, 0, 0, 1],\n *      [0, 1, 0, 0],\n *      [0, 0, 0, 1],\n *      [1, 0, 0, 0],\n *      [0, 0, 1, 0]\n *    ]\n * );\n *\n * const y = tf.tensor2d(\n *    [\n *      [0, 0, 1, 0],\n *      [0, 1, 0, 0],\n *      [0, 0, 0, 1],\n *      [0, 1, 0, 0],\n *      [0, 1, 0, 0]\n *    ]\n * );\n *\n * const recall = tf.metrics.recall(x, y);\n * recall.print();\n * ```\n *\n * @param yTrue The ground truth values. Expected to contain only 0-1 values.\n * @param yPred The predicted values. Expected to contain only 0-1 values.\n * @return Recall Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function recall(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.recall(yTrue, yPred);\n}\n\n/**\n * Loss or metric function: Cosine proximity.\n *\n * Mathematically, cosine proximity is defined as:\n *   `-sum(l2Normalize(yTrue) * l2Normalize(yPred))`,\n * wherein `l2Normalize()` normalizes the L2 norm of the input to 1 and `*`\n * represents element-wise multiplication.\n *\n * ```js\n * const yTrue = tf.tensor2d([[1, 0], [1, 0]]);\n * const yPred = tf.tensor2d([[1 / Math.sqrt(2), 1 / Math.sqrt(2)], [0, 1]]);\n * const proximity = tf.metrics.cosineProximity(yTrue, yPred);\n * proximity.print();\n * ```\n *\n * @param yTrue Truth Tensor.\n * @param yPred Prediction Tensor.\n * @return Cosine proximity Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function cosineProximity(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.cosineProximity(yTrue, yPred);\n}\n\n/**\n * Loss or metric function: Mean absolute error.\n *\n * Mathematically, mean absolute error is defined as:\n *   `mean(abs(yPred - yTrue))`,\n * wherein the `mean` is applied over feature dimensions.\n *\n * ```js\n * const yTrue = tf.tensor2d([[0, 1], [0, 0], [2, 3]]);\n * const yPred = tf.tensor2d([[0, 1], [0, 1], [-2, -3]]);\n * const mse = tf.metrics.meanAbsoluteError(yTrue, yPred);\n * mse.print();\n * ```\n *\n * @param yTrue Truth Tensor.\n * @param yPred Prediction Tensor.\n * @return Mean absolute error Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function meanAbsoluteError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanAbsoluteError(yTrue, yPred);\n}\n\n/**\n * Loss or metric function: Mean absolute percentage error.\n *\n * ```js\n * const yTrue = tf.tensor2d([[0, 1], [10, 20]]);\n * const yPred = tf.tensor2d([[0, 1], [11, 24]]);\n * const mse = tf.metrics.meanAbsolutePercentageError(yTrue, yPred);\n * mse.print();\n * ```\n *\n * Aliases: `tf.metrics.MAPE`, `tf.metrics.mape`.\n *\n * @param yTrue Truth Tensor.\n * @param yPred Prediction Tensor.\n * @return Mean absolute percentage error Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function meanAbsolutePercentageError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanAbsolutePercentageError(yTrue, yPred);\n}\n\nexport function MAPE(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanAbsolutePercentageError(yTrue, yPred);\n}\n\nexport function mape(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanAbsolutePercentageError(yTrue, yPred);\n}\n\n/**\n * Loss or metric function: Mean squared error.\n *\n * ```js\n * const yTrue = tf.tensor2d([[0, 1], [3, 4]]);\n * const yPred = tf.tensor2d([[0, 1], [-3, -4]]);\n * const mse = tf.metrics.meanSquaredError(yTrue, yPred);\n * mse.print();\n * ```\n *\n * Aliases: `tf.metrics.MSE`, `tf.metrics.mse`.\n *\n * @param yTrue Truth Tensor.\n * @param yPred Prediction Tensor.\n * @return Mean squared error Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function meanSquaredError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanSquaredError(yTrue, yPred);\n}\n\nexport function MSE(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanSquaredError(yTrue, yPred);\n}\n\nexport function mse(yTrue: Tensor, yPred: Tensor): Tensor {\n  return losses.meanSquaredError(yTrue, yPred);\n}\n\n/**\n * Computes R2 score.\n *\n * ```js\n * const yTrue = tf.tensor2d([[0, 1], [3, 4]]);\n * const yPred = tf.tensor2d([[0, 1], [-3, -4]]);\n * const r2Score = tf.metrics.r2Score(yTrue, yPred);\n * r2Score.print();\n * ```\n * @param yTrue Truth Tensor.\n * @param yPred Prediction Tensor.\n * @return R2 score Tensor.\n *\n * @doc {heading: 'Metrics', namespace: 'metrics'}\n */\nexport function r2Score(yTrue: Tensor, yPred: Tensor): Tensor {\n  return metrics.r2Score(yTrue, yPred);\n}\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAWA,OAAO,KAAK,MAAM,MAAM,UAAU,CAAC;AACnC,OAAO,KAAK,OAAO,MAAM,WAAW,CAAC;;;AA+B/B,SAAU,cAAc,CAAC,KAAa,EAAE,KAAa;IACzD,gLAAO,OAAO,CAAC,SAAA,AAAc,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC9C,CAAC;AAmBK,SAAU,kBAAkB,CAAC,KAAa,EAAE,KAAa;IAC7D,OAAO,OAAO,CAAC,sLAAA,AAAkB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAClD,CAAC;AAqBK,SAAU,yBAAyB,CACrC,KAAa,EAAE,KAAa;IAC9B,WAAO,OAAO,CAAC,yLAAA,AAAyB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACzD,CAAC;AAoBK,SAAU,mBAAmB,CAAC,KAAa,EAAE,KAAa;IAC9D,gLAAO,OAAO,CAAC,cAAA,AAAmB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACnD,CAAC;AAaK,SAAU,uBAAuB,CAAC,KAAa,EAAE,KAAa;IAClE,gLAAO,OAAO,CAAC,kBAAA,AAAuB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACvD,CAAC;AAqCK,SAAU,SAAS,CAAC,KAAa,EAAE,KAAa;IACpD,gLAAO,OAAO,CAAC,IAAA,AAAS,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACzC,CAAC;AAqCK,SAAU,MAAM,CAAC,KAAa,EAAE,KAAa;IACjD,gLAAO,OAAO,CAAC,CAAA,AAAM,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACtC,CAAC;AAuBK,SAAU,eAAe,CAAC,KAAa,EAAE,KAAa;IAC1D,+KAAO,MAAM,CAAC,WAAA,AAAe,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC9C,CAAC;AAsBK,SAAU,iBAAiB,CAAC,KAAa,EAAE,KAAa;IAC5D,+KAAO,MAAM,CAAC,aAAA,AAAiB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAChD,CAAC;AAoBK,SAAU,2BAA2B,CACvC,KAAa,EAAE,KAAa;IAC9B,+KAAO,MAAM,CAAC,uBAAA,AAA2B,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC1D,CAAC;AAEK,SAAU,IAAI,CAAC,KAAa,EAAE,KAAa;IAC/C,QAAO,MAAM,CAAC,8LAAA,AAA2B,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC1D,CAAC;AAEK,SAAU,IAAI,CAAC,KAAa,EAAE,KAAa;IAC/C,OAAO,MAAM,CAAC,+LAAA,AAA2B,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC1D,CAAC;AAoBK,SAAU,gBAAgB,CAAC,KAAa,EAAE,KAAa;IAC3D,QAAO,MAAM,CAAC,mLAAA,AAAgB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC/C,CAAC;AAEK,SAAU,GAAG,CAAC,KAAa,EAAE,KAAa;IAC9C,+KAAO,MAAM,CAAC,YAAA,AAAgB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC/C,CAAC;AAEK,SAAU,GAAG,CAAC,KAAa,EAAE,KAAa;IAC9C,+KAAO,MAAM,CAAC,YAAA,AAAgB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AAC/C,CAAC;AAiBK,SAAU,OAAO,CAAC,KAAa,EAAE,KAAa;IAClD,gLAAO,OAAO,CAAC,EAAA,AAAO,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACvC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4170, "column": 0}, "map": {"version":3,"file":"exports_models.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_models.js/__/__/__/__/__/tfjs-layers/src/exports_models.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nexport {modelFromJSON} from './models';\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;AAEH,OAAO,EAAC,aAAa,EAAC,MAAM,UAAU,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4196, "column": 0}, "map": {"version":3,"file":"exports_regularizers.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/exports_regularizers.js/__/__/__/__/__/tfjs-layers/src/exports_regularizers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as regularizers from './regularizers';\n// tslint:disable-next-line:max-line-length\nimport {L1Args, L1L2, L1L2Args, L2Args, Regularizer} from './regularizers';\n\n/**\n * Regularizer for L1 and L2 regularization.\n *\n * Adds a term to the loss to penalize large weights:\n * loss += sum(l1 * abs(x)) + sum(l2 * x^2)\n *\n * @doc {heading: 'Regularizers', namespace: 'regularizers'}\n */\nexport function l1l2(config?: L1L2Args): Regularizer {\n  return new L1L2(config);\n}\n\n/**\n * Regularizer for L1 regularization.\n *\n * Adds a term to the loss to penalize large weights:\n * loss += sum(l1 * abs(x))\n * @param args l1 config.\n *\n * @doc {heading: 'Regularizers', namespace: 'regularizers'}\n */\nexport function l1(config?: L1Args): Regularizer {\n  return regularizers.l1(config);\n}\n\n/**\n * Regularizer for L2 regularization.\n *\n * Adds a term to the loss to penalize large weights:\n * loss += sum(l2 * x^2)\n * @param args l2 config.\n *\n * @doc {heading: 'Regularizers', namespace: 'regularizers'}\n */\nexport function l2(config?: L2Args): Regularizer {\n  return regularizers.l2(config);\n}\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;;;;;AACH,OAAO,KAAK,YAAY,MAAM,gBAAgB,CAAC;;;AAYzC,SAAU,IAAI,CAAC,MAAiB;IACpC,OAAO,8KAAI,OAAI,CAAC,MAAM,CAAC,CAAC;AAC1B,CAAC;AAWK,SAAU,EAAE,CAAC,MAAe;IAChC,qLAAO,KAAa,AAAE,EAAC,KAAJ,CAAC,AAAS,CAAC,CAAC;AACjC,CAAC;AAWK,SAAU,EAAE,CAAC,MAAe;IAChC,qLAAO,KAAa,AAAE,EAAC,KAAJ,CAAC,AAAS,CAAC,CAAC;AACjC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4227, "column": 0}, "map": {"version":3,"file":"callbacks.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/callbacks.js/__/__/__/__/__/tfjs-layers/src/callbacks.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/callbacks.py */\n\nimport {BaseCallback} from './base_callbacks';\nimport {Container} from './engine/container';\nimport {LayersModel} from './engine/training';\nimport {NotImplementedError} from './errors';\nimport {Logs, resolveScalarsInLogs} from './logs';\n\nexport abstract class Callback extends BaseCallback {\n  /** Instance of `keras.models.Model`. Reference of the model being trained. */\n  model: LayersModel = null;\n\n  override setModel(model: Container): void {\n    if (!(model instanceof LayersModel)) {\n      throw new Error('model must be a LayersModel, not some other Container');\n    }\n    this.model = model;\n  }\n}\n\nexport interface EarlyStoppingCallbackArgs {\n  /**\n   * Quantity to be monitored.\n   *\n   * Defaults to 'val_loss'.\n   */\n  monitor?: string;\n\n  /**\n   * Minimum change in the monitored quantity to qualify as improvement,\n   * i.e., an absolute change of less than `minDelta` will count as no\n   * improvement.\n   *\n   * Defaults to 0.\n   */\n  minDelta?: number;\n\n  /**\n   * Number of epochs with no improvement after which training will be stopped.\n   *\n   * Defaults to 0.\n   */\n  patience?: number;\n\n  /** Verbosity mode. */\n  verbose?: number;\n\n  /**\n   * Mode: one of 'min', 'max', and 'auto'.\n   * - In 'min' mode, training will be stopped when the quantity monitored has\n   *   stopped decreasing.\n   * - In 'max' mode, training will be stopped when the quantity monitored has\n   *   stopped increasing.\n   * - In 'auto' mode, the direction is inferred automatically from the name of\n   *   the monitored quantity.\n   *\n   * Defaults to 'auto'.\n   */\n  mode?: 'auto'|'min'|'max';\n\n  /**\n   * Baseline value of the monitored quantity.\n   *\n   * If specified, training will be stopped if the model doesn't show\n   * improvement over the baseline.\n   */\n  baseline?: number;\n\n  /**\n   * Whether to restore model weights from the epoch with the best value\n   * of the monitored quantity. If `False`, the model weights obtained at the\n   * last step of training are used.\n   *\n   * **`True` is not supported yet.**\n   */\n  restoreBestWeights?: boolean;\n}\n\nfunction less(currVal: number, prevVal: number) {\n  return currVal < prevVal;\n}\n\nfunction greater(currVal: number, prevVal: number) {\n  return currVal > prevVal;\n}\n\n/**\n * A Callback that stops training when a monitored quantity has stopped\n * improving.\n */\nexport class EarlyStopping extends Callback {\n  protected readonly monitor: string;\n  protected readonly minDelta: number;\n  protected readonly patience: number;\n  protected readonly baseline: number;\n  protected readonly verbose: number;\n  protected readonly mode: 'auto'|'min'|'max';\n\n  protected monitorFunc: (currVal: number, prevVal: number) => boolean;\n\n  private wait: number;\n  private stoppedEpoch: number;\n  private best: number;\n\n  constructor(args?: EarlyStoppingCallbackArgs) {\n    super();\n    if (args == null) {\n      args = {};\n    }\n    if (args.restoreBestWeights) {\n      throw new NotImplementedError(\n          'restoreBestWeights = True is not implemented in EarlyStopping yet.');\n    }\n\n    this.monitor = args.monitor || 'val_loss';\n    this.minDelta = Math.abs(args.minDelta || 0);\n    this.patience = args.patience || 0;\n    this.verbose = args.verbose || 0;\n    this.mode = args.mode || 'auto';\n    this.baseline = args.baseline;\n\n    if (['auto', 'min', 'max'].indexOf(this.mode) === -1) {\n      console.warn(\n          `EarlyStopping mode '${this.mode}' is invalid. ` +\n          `Falling back to mode 'auto'.`);\n      this.mode = 'auto';\n    }\n\n    if (this.mode === 'min') {\n      this.monitorFunc = less;\n    } else if (this.mode === 'max') {\n      this.monitorFunc = greater;\n    } else {\n      // For mode === 'auto'.\n      if (this.monitor.indexOf('acc') !== -1) {\n        this.monitorFunc = greater;\n      } else {\n        this.monitorFunc = less;\n      }\n    }\n\n    if (this.monitorFunc === less) {\n      this.minDelta *= -1;\n    }\n  }\n\n  override async onTrainBegin(logs?: Logs) {\n    this.wait = 0;\n    this.stoppedEpoch = 0;\n    if (this.baseline != null) {\n      this.best = this.baseline;\n    } else {\n      this.best = this.monitorFunc === less ? Infinity : -Infinity;\n    }\n  }\n\n  override async onEpochEnd(epoch: number, logs?: Logs) {\n    await resolveScalarsInLogs(logs);\n    const current = this.getMonitorValue(logs);\n    if (current == null) {\n      return;\n    }\n\n    if (this.monitorFunc(current - this.minDelta, this.best)) {\n      this.best = current;\n      this.wait = 0;\n      // TODO(cais): Logic for restoreBestWeights.\n    } else {\n      this.wait++;\n      if (this.wait >= this.patience) {\n        this.stoppedEpoch = epoch;\n        this.model.stopTraining = true;\n      }\n      // TODO(cais): Logic for restoreBestWeights.\n    }\n  }\n\n  override async onTrainEnd(logs?: Logs) {\n    if (this.stoppedEpoch > 0 && this.verbose) {\n      console.log(`Epoch ${this.stoppedEpoch}: early stopping.`);\n    }\n  }\n\n  private getMonitorValue(logs: Logs) {\n    if (logs == null) {\n      logs = {};\n    }\n    const monitorValue = logs[this.monitor];\n    if (monitorValue == null) {\n      console.warn(\n          `Metric for EarlyStopping ${this.monitor} is not available. ` +\n          `Available metrics are: ${Object.keys(logs)}`);\n    }\n    return monitorValue;\n  }\n}\n\n/**\n * Factory function for a Callback that stops training when a monitored\n * quantity has stopped improving.\n *\n * Early stopping is a type of regularization, and protects model against\n * overfitting.\n *\n * The following example based on fake data illustrates how this callback\n * can be used during `tf.LayersModel.fit()`:\n *\n * ```js\n * const model = tf.sequential();\n * model.add(tf.layers.dense({\n *   units: 3,\n *   activation: 'softmax',\n *   kernelInitializer: 'ones',\n *   inputShape: [2]\n * }));\n * const xs = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const ys = tf.tensor2d([[1, 0, 0], [0, 1, 0]], [2, 3]);\n * const xsVal = tf.tensor2d([4, 3, 2, 1], [2, 2]);\n * const ysVal = tf.tensor2d([[0, 0, 1], [0, 1, 0]], [2, 3]);\n * model.compile(\n *     {loss: 'categoricalCrossentropy', optimizer: 'sgd', metrics: ['acc']});\n *\n * // Without the EarlyStopping callback, the val_acc value would be:\n * //   0.5, 0.5, 0.5, 0.5, ...\n * // With val_acc being monitored, training should stop after the 2nd epoch.\n * const history = await model.fit(xs, ys, {\n *   epochs: 10,\n *   validationData: [xsVal, ysVal],\n *   callbacks: tf.callbacks.earlyStopping({monitor: 'val_acc'})\n * });\n *\n * // Expect to see a length-2 array.\n * console.log(history.history.val_acc);\n * ```\n *\n * @doc {\n *   heading: 'Callbacks',\n *   namespace: 'callbacks'\n * }\n */\nexport function earlyStopping(args?: EarlyStoppingCallbackArgs) {\n  return new EarlyStopping(args);\n}\n\nexport const callbacks = {earlyStopping};\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG,CAEH,uCAAA,EAAyC;;;;;;AAEzC,OAAO,EAAC,YAAY,EAAC,MAAM,kBAAkB,CAAC;AAE9C,OAAO,EAAC,WAAW,EAAC,MAAM,mBAAmB,CAAC;AAC9C,OAAO,EAAC,mBAAmB,EAAC,MAAM,UAAU,CAAC;AAC7C,OAAO,EAAO,oBAAoB,EAAC,MAAM,QAAQ,CAAC;;;;;AAE5C,MAAgB,QAAS,qLAAQ,eAAY;IAAnD,aAAA;;QACE,4EAAA,EAA8E,CAC9E,IAAA,CAAA,KAAK,GAAgB,IAAI,CAAC;IAQ5B,CAAC;IANU,QAAQ,CAAC,KAAgB,EAAA;QAChC,IAAI,CAAC,CAAC,KAAK,4LAAY,cAAW,CAAC,EAAE;YACnC,MAAM,IAAI,KAAK,CAAC,uDAAuD,CAAC,CAAC;SAC1E;QACD,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;IACrB,CAAC;CACF;AA4DD,SAAS,IAAI,CAAC,OAAe,EAAE,OAAe;IAC5C,OAAO,OAAO,GAAG,OAAO,CAAC;AAC3B,CAAC;AAED,SAAS,OAAO,CAAC,OAAe,EAAE,OAAe;IAC/C,OAAO,OAAO,GAAG,OAAO,CAAC;AAC3B,CAAC;AAMK,MAAO,aAAc,SAAQ,QAAQ;IAczC,YAAY,IAAgC,CAAA;QAC1C,KAAK,EAAE,CAAC;QACR,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,IAAI,IAAI,CAAC,kBAAkB,EAAE;YAC3B,MAAM,wKAAI,sBAAmB,CACzB,oEAAoE,CAAC,CAAC;SAC3E;QAED,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,UAAU,CAAC;QAC1C,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,QAAQ,IAAI,CAAC,CAAC,CAAC;QAC7C,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,IAAI,CAAC,CAAC;QACnC,IAAI,CAAC,OAAO,GAAG,IAAI,CAAC,OAAO,IAAI,CAAC,CAAC;QACjC,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,IAAI,IAAI,MAAM,CAAC;QAChC,IAAI,CAAC,QAAQ,GAAG,IAAI,CAAC,QAAQ,CAAC;QAE9B,IAAI;YAAC,MAAM;YAAE,KAAK;YAAE,KAAK;SAAC,CAAC,OAAO,CAAC,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;YACpD,OAAO,CAAC,IAAI,CACR,CAAA,oBAAA,EAAuB,IAAI,CAAC,IAAI,CAAA,cAAA,CAAgB,GAChD,CAAA,4BAAA,CAA8B,CAAC,CAAC;YACpC,IAAI,CAAC,IAAI,GAAG,MAAM,CAAC;SACpB;QAED,IAAI,IAAI,CAAC,IAAI,KAAK,KAAK,EAAE;YACvB,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC;SACzB,MAAM,IAAI,IAAI,CAAC,IAAI,KAAK,KAAK,EAAE;YAC9B,IAAI,CAAC,WAAW,GAAG,OAAO,CAAC;SAC5B,MAAM;YACL,uBAAuB;YACvB,IAAI,IAAI,CAAC,OAAO,CAAC,OAAO,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE;gBACtC,IAAI,CAAC,WAAW,GAAG,OAAO,CAAC;aAC5B,MAAM;gBACL,IAAI,CAAC,WAAW,GAAG,IAAI,CAAC;aACzB;SACF;QAED,IAAI,IAAI,CAAC,WAAW,KAAK,IAAI,EAAE;YAC7B,IAAI,CAAC,QAAQ,IAAI,CAAC,CAAC,CAAC;SACrB;IACH,CAAC;IAEQ,KAAK,CAAC,YAAY,CAAC,IAAW,EAAA;QACrC,IAAI,CAAC,IAAI,GAAG,CAAC,CAAC;QACd,IAAI,CAAC,YAAY,GAAG,CAAC,CAAC;QACtB,IAAI,IAAI,CAAC,QAAQ,IAAI,IAAI,EAAE;YACzB,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,QAAQ,CAAC;SAC3B,MAAM;YACL,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,WAAW,KAAK,IAAI,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC;SAC9D;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,KAAa,EAAE,IAAW,EAAA;QAClD,MAAM,6LAAA,AAAoB,EAAC,IAAI,CAAC,CAAC;QACjC,MAAM,OAAO,GAAG,IAAI,CAAC,eAAe,CAAC,IAAI,CAAC,CAAC;QAC3C,IAAI,OAAO,IAAI,IAAI,EAAE;YACnB,OAAO;SACR;QAED,IAAI,IAAI,CAAC,WAAW,CAAC,OAAO,GAAG,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,IAAI,CAAC,EAAE;YACxD,IAAI,CAAC,IAAI,GAAG,OAAO,CAAC;YACpB,IAAI,CAAC,IAAI,GAAG,CAAC,CAAC;QACd,4CAA4C;SAC7C,MAAM;YACL,IAAI,CAAC,IAAI,EAAE,CAAC;YACZ,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,CAAC,QAAQ,EAAE;gBAC9B,IAAI,CAAC,YAAY,GAAG,KAAK,CAAC;gBAC1B,IAAI,CAAC,KAAK,CAAC,YAAY,GAAG,IAAI,CAAC;aAChC;QACD,4CAA4C;SAC7C;IACH,CAAC;IAEQ,KAAK,CAAC,UAAU,CAAC,IAAW,EAAA;QACnC,IAAI,IAAI,CAAC,YAAY,GAAG,CAAC,IAAI,IAAI,CAAC,OAAO,EAAE;YACzC,OAAO,CAAC,GAAG,CAAC,CAAA,MAAA,EAAS,IAAI,CAAC,YAAY,CAAA,iBAAA,CAAmB,CAAC,CAAC;SAC5D;IACH,CAAC;IAEO,eAAe,CAAC,IAAU,EAAA;QAChC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,IAAI,GAAG,CAAA,CAAE,CAAC;SACX;QACD,MAAM,YAAY,GAAG,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;QACxC,IAAI,YAAY,IAAI,IAAI,EAAE;YACxB,OAAO,CAAC,IAAI,CACR,CAAA,yBAAA,EAA4B,IAAI,CAAC,OAAO,CAAA,mBAAA,CAAqB,GAC7D,CAAA,uBAAA,EAA0B,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;SACpD;QACD,OAAO,YAAY,CAAC;IACtB,CAAC;CACF;AA6CK,SAAU,aAAa,CAAC,IAAgC;IAC5D,OAAO,IAAI,aAAa,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC;AAEM,MAAM,SAAS,GAAG;IAAC,aAAa;AAAA,CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4362, "column": 0}, "map": {"version":3,"file":"index.js","sourceRoot":"","sources":["file:///project/sandbox/user-workspace/node_modules/%40tensorflow/tfjs-layers/dist/index.js/__/__/__/__/__/tfjs-layers/src/index.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport './flags_layers';\nimport '@tensorflow/tfjs-core';\n// tslint:disable-next-line: no-imports-from-dist\nimport '@tensorflow/tfjs-core/dist/register_all_gradients';\n\n// This file lists all exports of TensorFlow.js Layers\n\nimport * as constraints from './exports_constraints';\nimport * as initializers from './exports_initializers';\nimport * as layers from './exports_layers';\nimport * as metrics from './exports_metrics';\nimport * as models from './exports_models';\nimport * as regularizers from './exports_regularizers';\n\nexport {CallbackList, CustomCallback, CustomCallbackArgs, History} from './base_callbacks';\nexport {Callback, callbacks, EarlyStopping, EarlyStoppingCallbackArgs} from './callbacks';\nexport {InputSpec, SymbolicTensor} from './engine/topology';\nexport {LayersModel, ModelCompileArgs, ModelEvaluateArgs} from './engine/training';\nexport {ModelFitDatasetArgs} from './engine/training_dataset';\nexport {ModelFitArgs} from './engine/training_tensors';\nexport {ClassWeight, ClassWeightMap} from './engine/training_utils';\nexport {input, loadLayersModel, model, registerCallbackConstructor, sequential} from './exports';\nexport {Shape} from './keras_format/common';\nexport {GRUCellLayerArgs, GRULayerArgs, LSTMCellLayerArgs, LSTMLayerArgs, RNN, RNNLayerArgs, SimpleRNNCellLayerArgs, SimpleRNNLayerArgs} from './layers/recurrent';\nexport {Logs} from './logs';\nexport {ModelAndWeightsConfig, Sequential, SequentialArgs} from './models';\nexport {LayerVariable} from './variables';\nexport {version as version_layers} from './version';\nexport {constraints, initializers, layers, metrics, models, regularizers};\n"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;AAEH,OAAO,gBAAgB,CAAC;AACxB,OAAO,uBAAuB,CAAC;AAC/B,iDAAiD;AACjD,OAAO,mDAAmD,CAAC;AAE3D,sDAAsD;AAEtD,OAAO,KAAK,WAAW,MAAM,uBAAuB,CAAC;AACrD,OAAO,KAAK,YAAY,MAAM,wBAAwB,CAAC;AACvD,OAAO,KAAK,MAAM,MAAM,kBAAkB,CAAC;AAC3C,OAAO,KAAK,OAAO,MAAM,mBAAmB,CAAC;AAC7C,OAAO,KAAK,MAAM,MAAM,kBAAkB,CAAC;AAC3C,OAAO,KAAK,YAAY,MAAM,wBAAwB,CAAC;AAEvD,OAAO,EAAC,YAAY,EAAE,cAAc,EAAsB,OAAO,EAAC,MAAM,kBAAkB,CAAC;AAC3F,OAAO,EAAC,QAAQ,EAAE,SAAS,EAAE,aAAa,EAA4B,MAAM,aAAa,CAAC;AAC1F,OAAO,EAAC,SAAS,EAAE,cAAc,EAAC,MAAM,mBAAmB,CAAC;AAC5D,OAAO,EAAC,WAAW,EAAsC,MAAM,mBAAmB,CAAC;AAInF,OAAO,EAAC,KAAK,EAAE,eAAe,EAAE,KAAK,EAAE,2BAA2B,EAAE,UAAU,EAAC,MAAM,WAAW,CAAC;AAEjG,OAAO,EAAmE,GAAG,EAA2D,MAAM,oBAAoB,CAAC;AAEnK,OAAO,EAAwB,UAAU,EAAiB,MAAM,UAAU,CAAC;AAC3E,OAAO,EAAC,aAAa,EAAC,MAAM,aAAa,CAAC;AAC1C,OAAO,EAAC,OAAO,IAAI,cAAc,EAAC,MAAM,WAAW,CAAC","ignoreList":[0],"debugId":null}}]
}